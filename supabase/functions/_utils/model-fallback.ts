// ============================================================================
// MODEL FALLBACK
// ============================================================================
// Implementa fallback autom√°tico entre m√∫ltiplos provedores LLM
// Por: SYNCADS
// ============================================================================

export interface MessageRole {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface ModelConfig {
  provider: string;
  model: string;
  apiKey: string;
  maxTokens: number;
  endpoint?: string;
}

export interface FallbackResult {
  success: boolean;
  model: string;
  provider: string;
  response: string;
  error?: string;
  tokensUsed?: number;
}

/**
 * Lista de modelos em ordem de prioridade
 */
export const MODEL_PRIORITY: ModelConfig[] = [
  // 1. OpenAI GPT-4 (Primary)
  {
    provider: 'OPENAI',
    model: 'gpt-4-turbo-preview',
    apiKey: Deno.env.get('OPENAI_API_KEY') || '',
    maxTokens: 128000,
  },
  // 2. Anthropic Claude 3 (Fallback 1)
  {
    provider: 'ANTHROPIC',
    model: 'claude-3-opus-20240229',
    apiKey: Deno.env.get('ANTHROPIC_API_KEY') || '',
    maxTokens: 200000,
  },
  // 3. Groq Mixtral (Fallback 2)
  {
    provider: 'GROQ',
    model: 'mixtral-8x7b-32768',
    apiKey: Deno.env.get('GROQ_API_KEY') || '',
    maxTokens: 32768,
  },
];

/**
 * Chama IA com fallback autom√°tico
 */
export async function callWithFallback(
  systemPrompt: string,
  conversationHistory: MessageRole[],
  userMessage: string,
  temperature: number = 0.7
): Promise<FallbackResult> {
  // Preparar mensagens
  const messages = [
    { role: 'system', content: systemPrompt },
    ...conversationHistory,
    { role: 'user', content: userMessage },
  ];

  // Tentar cada modelo em sequ√™ncia
  for (const config of MODEL_PRIORITY) {
    // Verificar se API key est√° dispon√≠vel
    if (!config.apiKey) {
      console.log(`‚ö†Ô∏è ${config.provider} API key n√£o configurada, pulando...`);
      continue;
    }

    try {
      console.log(`üì§ Tentando ${config.provider} ${config.model}...`);

      const result = await callModel(config, messages, temperature);

      console.log(`‚úÖ Sucesso com ${config.provider} ${config.model}`);
      
      return {
        success: true,
        model: config.model,
        provider: config.provider,
        response: result.response,
        tokensUsed: result.tokensUsed,
      };
    } catch (error: any) {
      console.log(`‚ùå ${config.provider} falhou: ${error.message}`);

      // Se foi √∫ltimo modelo, retornar erro
      if (config === MODEL_PRIORITY[MODEL_PRIORITY.length - 1]) {
        return {
          success: false,
          model: 'none',
          provider: 'none',
          response: '',
          error: `Todos modelos falharam. √öltimo erro: ${error.message}`,
        };
      }

      // Continuar para pr√≥ximo modelo
      console.log(`üîÑ Tentando pr√≥ximo modelo...`);
    }
  }

  // Se chegou aqui, nenhum modelo tinha API key
  return {
    success: false,
    model: 'none',
    provider: 'none',
    response: '',
    error: 'Nenhum modelo configurado com API key v√°lida',
  };
}

/**
 * Chama um modelo espec√≠fico
 */
async function callModel(
  config: ModelConfig,
  messages: MessageRole[],
  temperature: number
): Promise<{ response: string; tokensUsed?: number }> {
  const startTime = Date.now();

  if (config.provider === 'OPENAI') {
    return await callOpenAI(config, messages, temperature);
  } else if (config.provider === 'ANTHROPIC') {
    return await callAnthropic(config, messages, temperature);
  } else if (config.provider === 'GROQ') {
    return await callGroq(config, messages, temperature);
  } else {
    throw new Error(`Provider ${config.provider} n√£o implementado`);
  }
}

/**
 * Chama OpenAI
 */
async function callOpenAI(
  config: ModelConfig,
  messages: MessageRole[],
  temperature: number
): Promise<{ response: string; tokensUsed?: number }> {
  const url = 'https://api.openai.com/v1/chat/completions';
  
  const response = await fetch(url, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${config.apiKey}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: config.model,
      messages: messages.map(m => ({
        role: m.role,
        content: m.content,
      })),
      temperature,
    }),
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`OpenAI error: ${error}`);
  }

  const data = await response.json();

  return {
    response: data.choices[0].message.content,
    tokensUsed: data.usage?.total_tokens,
  };
}

/**
 * Chama Anthropic
 */
async function callAnthropic(
  config: ModelConfig,
  messages: MessageRole[],
  temperature: number
): Promise<{ response: string; tokensUsed?: number }> {
  const url = 'https://api.anthropic.com/v1/messages';
  
  const response = await fetch(url, {
    method: 'POST',
    headers: {
      'x-api-key': config.apiKey,
      'anthropic-version': '2023-06-01',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: config.model,
      messages: messages.slice(1), // Anthropic n√£o aceita system role expl√≠cito
      system: messages[0].content, // Primeira mensagem como system
      max_tokens: 4096,
    }),
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Anthropic error: ${error}`);
  }

  const data = await response.json();

  return {
    response: data.content[0].text,
    tokensUsed: data.usage?.input_tokens && data.usage?.output_tokens
      ? data.usage.input_tokens + data.usage.output_tokens
      : undefined,
  };
}

/**
 * Chama Groq
 */
async function callGroq(
  config: ModelConfig,
  messages: MessageRole[],
  temperature: number
): Promise<{ response: string; tokensUsed?: number }> {
  const url = 'https://api.groq.com/openai/v1/chat/completions';
  
  const response = await fetch(url, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${config.apiKey}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: config.model,
      messages: messages.map(m => ({
        role: m.role,
        content: m.content,
      })),
      temperature,
    }),
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Groq error: ${error}`);
  }

  const data = await response.json();

  return {
    response: data.choices[0].message.content,
    tokensUsed: data.usage?.total_tokens,
  };
}

