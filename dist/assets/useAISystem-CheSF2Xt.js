import{i as e}from"./index-i3b3sNGH.js";import{r as t}from"./vendor-react-X41Bjl18.js";const a=e("outline","sparkles","Sparkles",[["path",{d:"M16 18a2 2 0 0 1 2 2a2 2 0 0 1 2 -2a2 2 0 0 1 -2 -2a2 2 0 0 1 -2 2zm0 -12a2 2 0 0 1 2 2a2 2 0 0 1 2 -2a2 2 0 0 1 -2 -2a2 2 0 0 1 -2 2zm-7 12a6 6 0 0 1 6 -6a6 6 0 0 1 -6 -6a6 6 0 0 1 -6 6a6 6 0 0 1 6 6z",key:"svg-0"}]]);var n,s={exports:{}},r="object"==typeof Reflect?Reflect:null,i=r&&"function"==typeof r.apply?r.apply:function(e,t,a){return Function.prototype.apply.call(e,t,a)};n=r&&"function"==typeof r.ownKeys?r.ownKeys:Object.getOwnPropertySymbols?function(e){return Object.getOwnPropertyNames(e).concat(Object.getOwnPropertySymbols(e))}:function(e){return Object.getOwnPropertyNames(e)};var o=Number.isNaN||function(e){return e!=e};function c(){c.init.call(this)}s.exports=c,s.exports.once=function(e,t){return new Promise(function(a,n){function s(a){e.removeListener(t,r),n(a)}function r(){"function"==typeof e.removeListener&&e.removeListener("error",s),a([].slice.call(arguments))}_(e,t,r,{once:!0}),"error"!==t&&function(e,t,a){"function"==typeof e.on&&_(e,"error",t,a)}(e,s,{once:!0})})},c.EventEmitter=c,c.prototype._events=void 0,c.prototype._eventsCount=0,c.prototype._maxListeners=void 0;var l=10;function d(e){if("function"!=typeof e)throw new TypeError('The "listener" argument must be of type Function. Received type '+typeof e)}function p(e){return void 0===e._maxListeners?c.defaultMaxListeners:e._maxListeners}function u(e,t,a,n){var s,r,i;if(d(a),void 0===(r=e._events)?(r=e._events=Object.create(null),e._eventsCount=0):(void 0!==r.newListener&&(e.emit("newListener",t,a.listener?a.listener:a),r=e._events),i=r[t]),void 0===i)i=r[t]=a,++e._eventsCount;else if("function"==typeof i?i=r[t]=n?[a,i]:[i,a]:n?i.unshift(a):i.push(a),(s=p(e))>0&&i.length>s&&!i.warned){i.warned=!0;var o=new Error("Possible EventEmitter memory leak detected. "+i.length+" "+String(t)+" listeners added. Use emitter.setMaxListeners() to increase limit");o.name="MaxListenersExceededWarning",o.emitter=e,o.type=t,o.count=i.length,console&&console.warn}return e}function m(){if(!this.fired)return this.target.removeListener(this.type,this.wrapFn),this.fired=!0,0===arguments.length?this.listener.call(this.target):this.listener.apply(this.target,arguments)}function g(e,t,a){var n={fired:!1,wrapFn:void 0,target:e,type:t,listener:a},s=m.bind(n);return s.listener=a,n.wrapFn=s,s}function f(e,t,a){var n=e._events;if(void 0===n)return[];var s=n[t];return void 0===s?[]:"function"==typeof s?a?[s.listener||s]:[s]:a?function(e){for(var t=new Array(e.length),a=0;a<t.length;++a)t[a]=e[a].listener||e[a];return t}(s):y(s,s.length)}function h(e){var t=this._events;if(void 0!==t){var a=t[e];if("function"==typeof a)return 1;if(void 0!==a)return a.length}return 0}function y(e,t){for(var a=new Array(t),n=0;n<t;++n)a[n]=e[n];return a}function _(e,t,a,n){if("function"==typeof e.on)n.once?e.once(t,a):e.on(t,a);else{if("function"!=typeof e.addEventListener)throw new TypeError('The "emitter" argument must be of type EventEmitter. Received type '+typeof e);e.addEventListener(t,function s(r){n.once&&e.removeEventListener(t,s),a(r)})}}Object.defineProperty(c,"defaultMaxListeners",{enumerable:!0,get:function(){return l},set:function(e){if("number"!=typeof e||e<0||o(e))throw new RangeError('The value of "defaultMaxListeners" is out of range. It must be a non-negative number. Received '+e+".");l=e}}),c.init=function(){void 0!==this._events&&this._events!==Object.getPrototypeOf(this)._events||(this._events=Object.create(null),this._eventsCount=0),this._maxListeners=this._maxListeners||void 0},c.prototype.setMaxListeners=function(e){if("number"!=typeof e||e<0||o(e))throw new RangeError('The value of "n" is out of range. It must be a non-negative number. Received '+e+".");return this._maxListeners=e,this},c.prototype.getMaxListeners=function(){return p(this)},c.prototype.emit=function(e){for(var t=[],a=1;a<arguments.length;a++)t.push(arguments[a]);var n="error"===e,s=this._events;if(void 0!==s)n=n&&void 0===s.error;else if(!n)return!1;if(n){var r;if(t.length>0&&(r=t[0]),r instanceof Error)throw r;var o=new Error("Unhandled error."+(r?" ("+r.message+")":""));throw o.context=r,o}var c=s[e];if(void 0===c)return!1;if("function"==typeof c)i(c,this,t);else{var l=c.length,d=y(c,l);for(a=0;a<l;++a)i(d[a],this,t)}return!0},c.prototype.addListener=function(e,t){return u(this,e,t,!1)},c.prototype.on=c.prototype.addListener,c.prototype.prependListener=function(e,t){return u(this,e,t,!0)},c.prototype.once=function(e,t){return d(t),this.on(e,g(this,e,t)),this},c.prototype.prependOnceListener=function(e,t){return d(t),this.prependListener(e,g(this,e,t)),this},c.prototype.removeListener=function(e,t){var a,n,s,r,i;if(d(t),void 0===(n=this._events))return this;if(void 0===(a=n[e]))return this;if(a===t||a.listener===t)0===--this._eventsCount?this._events=Object.create(null):(delete n[e],n.removeListener&&this.emit("removeListener",e,a.listener||t));else if("function"!=typeof a){for(s=-1,r=a.length-1;r>=0;r--)if(a[r]===t||a[r].listener===t){i=a[r].listener,s=r;break}if(s<0)return this;0===s?a.shift():function(e,t){for(;t+1<e.length;t++)e[t]=e[t+1];e.pop()}(a,s),1===a.length&&(n[e]=a[0]),void 0!==n.removeListener&&this.emit("removeListener",e,i||t)}return this},c.prototype.off=c.prototype.removeListener,c.prototype.removeAllListeners=function(e){var t,a,n;if(void 0===(a=this._events))return this;if(void 0===a.removeListener)return 0===arguments.length?(this._events=Object.create(null),this._eventsCount=0):void 0!==a[e]&&(0===--this._eventsCount?this._events=Object.create(null):delete a[e]),this;if(0===arguments.length){var s,r=Object.keys(a);for(n=0;n<r.length;++n)"removeListener"!==(s=r[n])&&this.removeAllListeners(s);return this.removeAllListeners("removeListener"),this._events=Object.create(null),this._eventsCount=0,this}if("function"==typeof(t=a[e]))this.removeListener(e,t);else if(void 0!==t)for(n=t.length-1;n>=0;n--)this.removeListener(e,t[n]);return this},c.prototype.listeners=function(e){return f(this,e,!0)},c.prototype.rawListeners=function(e){return f(this,e,!1)},c.listenerCount=function(e,t){return"function"==typeof e.listenerCount?e.listenerCount(t):h.call(e,t)},c.prototype.listenerCount=h,c.prototype.eventNames=function(){return this._eventsCount>0?n(this._events):[]};var v=s.exports,b=(e=>(e.BROWSER_AUTOMATION="BROWSER_AUTOMATION",e.PYTHON_EXECUTION="PYTHON_EXECUTION",e.INTERNAL_TOOLS="INTERNAL_TOOLS",e.MULTIMODAL_PIPELINE="MULTIMODAL_PIPELINE",e.HYBRID="HYBRID",e))(b||{}),x=(e=>(e.PENDING="PENDING",e.IN_PROGRESS="IN_PROGRESS",e.SUCCESS="SUCCESS",e.FAILED="FAILED",e.RETRYING="RETRYING",e.FALLBACK="FALLBACK",e))(x||{});class E extends v.EventEmitter{moduleRegistry=new Map;memoryContext;isProcessing=!1;queue=[];config;constructor(e){super(),this.config={maxRetries:e?.maxRetries||3,fallbackEnabled:e?.fallbackEnabled??!0,learningEnabled:e?.learningEnabled??!0,parallelExecution:e?.parallelExecution??!1,timeout:e?.timeout||3e5,debugMode:e?.debugMode??!1},this.memoryContext=this.initializeMemory(),this.initializeCoreModules()}initializeMemory(){return{sessionId:this.generateSessionId(),history:[],decisions:[],learnings:{},errorPatterns:[]}}initializeCoreModules(){this.registerModule({name:"browser-controller",type:"BROWSER_AUTOMATION",capabilities:["web-scraping","form-filling","dom-manipulation","visual-testing"],priority:10,reliability:.95,avgExecutionTime:5e3,successRate:.95}),this.registerModule({name:"python-executor",type:"PYTHON_EXECUTION",capabilities:["data-processing","ml-inference","api-calls","file-processing"],priority:9,reliability:.98,avgExecutionTime:3e3,successRate:.98}),this.registerModule({name:"internal-tools",type:"INTERNAL_TOOLS",capabilities:["database-ops","cache-ops","queue-ops","auth-ops"],priority:10,reliability:.99,avgExecutionTime:1e3,successRate:.99}),this.emit("core:initialized",{modules:this.moduleRegistry.size})}registerModule(e){this.moduleRegistry.set(e.name,e),this.log(`Módulo registrado: ${e.name} [${e.type}]`)}getModule(e){return this.moduleRegistry.get(e)}listModules(e){const t=Array.from(this.moduleRegistry.values());return e?t.filter(t=>t.type===e):t}async analyzeRequest(e){this.log(`Analisando requisição: ${e.input}`);const t=await this.performDeepAnalysis(e),a=this.makeDecision(t);return this.memoryContext.decisions.push(a),this.emit("decision:made",a),a}async performDeepAnalysis(e){const t=this.extractKeywords(e.input);return{keywords:t,intent:this.detectIntent(e.input),complexity:this.estimateComplexity(e.input),capabilities:this.matchCapabilities(t),context:e.context}}extractKeywords(e){const t=[];return/abrir|navegar|clicar|preencher|scrape|extrair|elemento|página|dom/i.test(e)&&t.push("browser-automation"),/processar|calcular|analisar|machine learning|ml|ia|dados|csv|json|api externa/i.test(e)&&t.push("python-execution"),/salvar|buscar|banco|database|tabela|query|sql|supabase/i.test(e)&&t.push("database"),/imagem|foto|gerar|editar|filtro|resize|upload/i.test(e)&&t.push("image-processing"),/arquivo|file|pdf|excel|documento|importar|exportar/i.test(e)&&t.push("file-processing"),t}detectIntent(e){return/criar|gerar|novo/i.test(e)?"CREATE":/buscar|listar|encontrar|pesquisar/i.test(e)?"READ":/atualizar|editar|modificar/i.test(e)?"UPDATE":/deletar|remover|excluir/i.test(e)?"DELETE":/automatizar|executar|rodar/i.test(e)?"AUTOMATE":"UNKNOWN"}estimateComplexity(e){let t=1;return e.length>100&&(t+=1),/e |então |depois |seguida/i.test(e)&&(t+=2),/se |caso |quando/i.test(e)&&(t+=1),/todos |cada |para cada/i.test(e)&&(t+=2),Math.min(t,10)}matchCapabilities(e){const t=[];for(const[a,n]of this.moduleRegistry)for(const s of e)if(n.capabilities.some(e=>e.includes(s)||s.includes(e))){t.push(a);break}return t}makeDecision(e){const{keywords:t,intent:a,complexity:n,capabilities:s}=e;let r="INTERNAL_TOOLS",i=.5,o="",c=[],l=s;return t.includes("browser-automation")?(r="BROWSER_AUTOMATION",i=.9,o="Requisição envolve automação de navegador",c=["PYTHON_EXECUTION","INTERNAL_TOOLS"]):t.includes("python-execution")||t.includes("image-processing")?(r="PYTHON_EXECUTION",i=.85,o="Requisição requer processamento Python",c=["INTERNAL_TOOLS"]):t.length>2&&(r="HYBRID",i=.8,o="Requisição requer múltiplas ferramentas",c=["PYTHON_EXECUTION","BROWSER_AUTOMATION"]),{taskType:r,confidence:i,reasoning:o,fallbackOptions:c,estimatedTime:3e3*n,requiredModules:l}}createExecutionPlan(e,t){const a=[];let n=0;if("HYBRID"===t.taskType)for(const r of t.requiredModules)a.push(this.createStep(++n,r,e));else{const s=t.requiredModules[0]||"internal-tools";a.push(this.createStep(++n,s,e))}const s=this.createFallbackStrategies(t);return{steps:a,totalSteps:a.length,estimatedDuration:t.estimatedTime,fallbackStrategies:s}}createStep(e,t,a){return{id:`step-${e}-${Date.now()}`,order:e,action:`execute-${t}`,taskType:this.moduleRegistry.get(t)?.type||"INTERNAL_TOOLS",module:t,parameters:{input:a.input,context:a.context},expectedOutput:null,timeout:this.config.timeout,retryCount:0,maxRetries:this.config.maxRetries}}createFallbackStrategies(e){return e.fallbackOptions.map((e,t)=>({triggerCondition:"execution_failed",alternativeTaskType:e,alternativeModule:this.findModuleByType(e),priority:t+1}))}findModuleByType(e){for(const[t,a]of this.moduleRegistry)if(a.type===e)return t;return"internal-tools"}async execute(e){this.memoryContext.currentTask=e,this.emit("execution:started",e);try{const t=await this.analyzeRequest(e),a=this.createExecutionPlan(e,t),n=[];for(const e of a.steps){const t=await this.executeStep(e,a.fallbackStrategies);if(n.push(t),"FAILED"===t.status&&!this.config.fallbackEnabled)break}return this.memoryContext.history.push(...n),this.emit("execution:completed",n),n}catch(t){throw this.log(`Erro na execução: ${t}`,"error"),this.emit("execution:error",t),t}}async executeStep(e,t){const a=Date.now();let n=e,s=!1;for(;n.retryCount<=n.maxRetries;)try{this.emit("step:executing",n);const e=await this.executeModule(n);return{stepId:n.id,status:"SUCCESS",output:e,executionTime:Date.now()-a,retriesUsed:n.retryCount,fallbackUsed:s}}catch(r){if(n.retryCount++,this.log(`Erro no step ${n.id}: ${r.message}`,"warn"),n.retryCount<=n.maxRetries){this.emit("step:retrying",{step:n,attempt:n.retryCount}),await this.sleep(1e3*n.retryCount);continue}if(this.config.fallbackEnabled&&t.length>0){const e=t.shift();if(e){this.log(`Ativando fallback: ${e.alternativeModule}`),n=this.createStep(n.order,e.alternativeModule,{input:n.parameters.input}),s=!0;continue}}return this.recordError(r.message),{stepId:n.id,status:"FAILED",error:r.message,executionTime:Date.now()-a,retriesUsed:n.retryCount,fallbackUsed:s}}throw new Error("Máximo de tentativas excedido")}async executeModule(e){if(!this.moduleRegistry.get(e.module))throw new Error(`Módulo não encontrado: ${e.module}`);return new Promise((t,a)=>{const n=setTimeout(()=>{a(new Error("Timeout na execução do módulo"))},e.timeout);this.emit("module:execute",{module:e.module,type:e.taskType,parameters:e.parameters,onSuccess:e=>{clearTimeout(n),t(e)},onError:e=>{clearTimeout(n),a(e)}})})}recordError(e){const t=this.memoryContext.errorPatterns.find(t=>t.error===e);t?(t.frequency++,t.lastOccurrence=Date.now()):this.memoryContext.errorPatterns.push({error:e,frequency:1,lastOccurrence:Date.now()}),this.config.learningEnabled&&this.learnFromError(e)}learnFromError(e){this.log(`Aprendendo com erro: ${e}`)}enqueue(e){this.queue.push(e),this.emit("queue:added",e),this.processQueue()}async processQueue(){if(!this.isProcessing&&0!==this.queue.length){for(this.isProcessing=!0;this.queue.length>0;){const e=this.queue.shift();await this.execute(e)}this.isProcessing=!1}}generateSessionId(){return`session-${Date.now()}-${Math.random().toString(36).substr(2,9)}`}sleep(e){return new Promise(t=>setTimeout(t,e))}log(e,t="info"){this.config.debugMode,this.emit("log",{level:t,message:e,timestamp:Date.now()})}getMemory(){return{...this.memoryContext}}getStats(){const e=this.memoryContext.history,t=e.filter(e=>"SUCCESS"===e.status).length,a=e.filter(e=>"FAILED"===e.status).length;return{totalExecutions:e.length,successRate:e.length>0?t/e.length:0,failureRate:e.length>0?a/e.length:0,avgExecutionTime:e.reduce((e,t)=>e+t.executionTime,0)/e.length||0,topErrors:this.memoryContext.errorPatterns.sort((e,t)=>t.frequency-e.frequency).slice(0,5)}}reset(){this.memoryContext=this.initializeMemory(),this.queue=[],this.isProcessing=!1,this.emit("core:reset")}}var w=(e=>(e.DATA_PROCESSING="DATA_PROCESSING",e.WEB_SCRAPING="WEB_SCRAPING",e.IMAGE_PROCESSING="IMAGE_PROCESSING",e.VIDEO_PROCESSING="VIDEO_PROCESSING",e.AUDIO_PROCESSING="AUDIO_PROCESSING",e.MACHINE_LEARNING="MACHINE_LEARNING",e.DEEP_LEARNING="DEEP_LEARNING",e.NLP="NLP",e.COMPUTER_VISION="COMPUTER_VISION",e.DATA_VISUALIZATION="DATA_VISUALIZATION",e.WEB_FRAMEWORK="WEB_FRAMEWORK",e.API_CLIENT="API_CLIENT",e.DATABASE="DATABASE",e.FILE_PROCESSING="FILE_PROCESSING",e.SECURITY="SECURITY",e.TESTING="TESTING",e.AUTOMATION="AUTOMATION",e.NETWORKING="NETWORKING",e.CLOUD_SERVICES="CLOUD_SERVICES",e.BLOCKCHAIN="BLOCKCHAIN",e.IOT="IOT",e.GAME_DEVELOPMENT="GAME_DEVELOPMENT",e.SCIENTIFIC_COMPUTING="SCIENTIFIC_COMPUTING",e.GEOSPATIAL="GEOSPATIAL",e.TIME_SERIES="TIME_SERIES",e.OPTIMIZATION="OPTIMIZATION",e.ROBOTICS="ROBOTICS",e.UTILITIES="UTILITIES",e.OTHER="OTHER",e))(w||{}),T=(e=>(e.BASIC="BASIC",e.INTERMEDIATE="INTERMEDIATE",e.ADVANCED="ADVANCED",e.EXPERT="EXPERT",e))(T||{}),S=(e=>(e.PYTHON="PYTHON",e.BROWSER="BROWSER",e.NODE="NODE",e.HYBRID="HYBRID",e))(S||{});class P{modules=new Map;categoryIndex=new Map;tagIndex=new Map;packageIndex=new Map;constructor(){this.initializeIndexes()}initializeIndexes(){Object.values(w).forEach(e=>{this.categoryIndex.set(e,new Set)})}register(e){this.validateModule(e),this.modules.set(e.id,e),this.updateIndexes(e)}registerBatch(e){e.forEach(e=>this.register(e))}validateModule(e){if(!e.id||!e.name||!e.packageName)throw new Error("Módulo inválido: campos obrigatórios faltando");if(this.modules.has(e.id))throw new Error(`Módulo já registrado: ${e.id}`);this.packageIndex.has(e.packageName)}updateIndexes(e){this.categoryIndex.get(e.category)?.add(e.id),e.tags.forEach(t=>{this.tagIndex.has(t)||this.tagIndex.set(t,new Set),this.tagIndex.get(t)?.add(e.id)}),this.packageIndex.set(e.packageName,e.id)}get(e){return this.modules.get(e)}getByPackage(e){const t=this.packageIndex.get(e);return t?this.modules.get(t):void 0}getByCategory(e){const t=this.categoryIndex.get(e)||new Set;return Array.from(t).map(e=>this.modules.get(e)).filter(Boolean)}getByTag(e){const t=this.tagIndex.get(e)||new Set;return Array.from(t).map(e=>this.modules.get(e)).filter(Boolean)}search(e){let t=Array.from(this.modules.values());if(e.query){const a=e.query.toLowerCase();t=t.filter(e=>e.name.toLowerCase().includes(a)||e.description.toLowerCase().includes(a)||e.packageName.toLowerCase().includes(a)||e.tags.some(e=>e.toLowerCase().includes(a)))}return e.category&&(t=t.filter(t=>t.category===e.category)),e.complexity&&(t=t.filter(t=>t.complexity===e.complexity)),e.environment&&(t=t.filter(t=>t.environment===e.environment)),e.tags&&e.tags.length>0&&(t=t.filter(t=>e.tags.some(e=>t.tags.includes(e)))),void 0!==e.minReliability&&(t=t.filter(t=>t.reliability>=e.minReliability)),void 0!==e.maxExecutionTime&&(t=t.filter(t=>t.avgExecutionTime<=e.maxExecutionTime)),void 0!==e.requiresGPU&&(t=t.filter(t=>t.gpuSupport===e.requiresGPU)),t}findBestMatch(e){const t=this.extractKeywords(e),a=this.scoreModules(t);return a.length>0?a[0].module:null}extractKeywords(e){const t=[],a=e.toLowerCase().split(/\s+/);for(let n=0;n<a.length;n++)t.push(a[n]),n<a.length-1&&t.push(`${a[n]} ${a[n+1]}`),n<a.length-2&&t.push(`${a[n]} ${a[n+1]} ${a[n+2]}`);return t}scoreModules(e){const t=[];for(const a of this.modules.values()){let n=0;e.forEach(e=>{a.name.toLowerCase().includes(e)&&(n+=10),a.description.toLowerCase().includes(e)&&(n+=5),a.purpose.toLowerCase().includes(e)&&(n+=3),a.tags.some(t=>t.toLowerCase().includes(e))&&(n+=2)}),n*=a.reliability,n*=a.successRate,"EXPERT"===a.complexity&&(n*=.8),"ADVANCED"===a.complexity&&(n*=.9),"deprecated"===a.status&&(n*=.3),"experimental"===a.status&&(n*=.7),t.push({module:a,score:n})}return t.sort((e,t)=>t.score-e.score)}recommendAlternatives(e,t=5){const a=this.modules.get(e);if(!a)return[];return this.getByCategory(a.category).filter(t=>t.id!==e).map(e=>({module:e,similarity:this.calculateSimilarity(a,e)})).sort((e,t)=>t.similarity-e.similarity).slice(0,t).map(e=>e.module)}calculateSimilarity(e,t){let a=0;e.category===t.category&&(a+=30);a+=5*e.tags.filter(e=>t.tags.includes(e)).length;return a+=3*e.subcategories.filter(e=>t.subcategories.includes(e)).length,e.complexity===t.complexity&&(a+=10),e.environment===t.environment&&(a+=10),a}getFallbackChain(e){const t=this.modules.get(e);if(!t)return[];const a=[];return t.fallbackModules.forEach(e=>{const t=this.modules.get(e);t&&a.push(t)}),t.alternativeModules.forEach(e=>{const t=this.modules.get(e);t&&!a.includes(t)&&a.push(t)}),a}getStats(){const e=Array.from(this.modules.values()),t={},a={},n={};e.forEach(e=>{t[e.category]=(t[e.category]||0)+1,a[e.complexity]=(a[e.complexity]||0)+1,n[e.environment]=(n[e.environment]||0)+1});const s=e.reduce((e,t)=>e+t.reliability,0)/e.length,r=e.reduce((e,t)=>e+t.successRate,0)/e.length,i=[...e].sort((e,t)=>t.popularity-e.popularity).slice(0,10),o=[...e].sort((e,t)=>t.reliability-e.reliability).slice(0,10),c=[...e].sort((e,t)=>t.lastUpdated-e.lastUpdated).slice(0,10);return{totalModules:e.length,byCategory:t,byComplexity:a,byEnvironment:n,avgReliability:s,avgSuccessRate:r,mostPopular:i,mostReliable:o,recentlyUpdated:c}}export(){return Array.from(this.modules.values())}exportByCategory(e){return this.getByCategory(e)}exportJSON(){return JSON.stringify(this.export(),null,2)}update(e,t){const a=this.modules.get(e);if(!a)throw new Error(`Módulo não encontrado: ${e}`);const n={...a,...t,lastUpdated:Date.now()};this.modules.set(e,n),(t.category||t.tags)&&this.updateIndexes(n)}remove(e){const t=this.modules.get(e);t&&(this.categoryIndex.get(t.category)?.delete(e),t.tags.forEach(t=>{this.tagIndex.get(t)?.delete(e)}),this.packageIndex.delete(t.packageName),this.modules.delete(e))}clear(){this.modules.clear(),this.categoryIndex.clear(),this.tagIndex.clear(),this.packageIndex.clear(),this.initializeIndexes()}list(e){let t=Array.from(this.modules.values());return e?.status&&(t=t.filter(t=>t.status===e.status)),e?.category&&(t=t.filter(t=>t.category===e.category)),t.sort((e,t)=>e.name.localeCompare(t.name))}count(){return this.modules.size}has(e){return this.modules.has(e)}}let C=null;var A=(e=>(e.NAVIGATE="NAVIGATE",e.CLICK="CLICK",e.TYPE="TYPE",e.SELECT="SELECT",e.SCROLL="SCROLL",e.WAIT="WAIT",e.EXTRACT="EXTRACT",e.SCREENSHOT="SCREENSHOT",e.EXECUTE_SCRIPT="EXECUTE_SCRIPT",e.SUBMIT_FORM="SUBMIT_FORM",e.HOVER="HOVER",e.DRAG_DROP="DRAG_DROP",e.UPLOAD_FILE="UPLOAD_FILE",e.SWITCH_TAB="SWITCH_TAB",e.SWITCH_FRAME="SWITCH_FRAME",e.BACK="BACK",e.FORWARD="FORWARD",e.REFRESH="REFRESH",e))(A||{}),R=(e=>(e.CSS="CSS",e.XPATH="XPATH",e.TEXT="TEXT",e.ID="ID",e.CLASS="CLASS",e.NAME="NAME",e.TAG="TAG",e.ATTRIBUTE="ATTRIBUTE",e.VISUAL="VISUAL",e))(R||{}),I=(e=>(e.PENDING="PENDING",e.RUNNING="RUNNING",e.SUCCESS="SUCCESS",e.FAILED="FAILED",e.ELEMENT_NOT_FOUND="ELEMENT_NOT_FOUND",e.TIMEOUT="TIMEOUT",e.RETRYING="RETRYING",e))(I||{});class k extends v.EventEmitter{connection;commandQueue=[];isExecuting=!1;config;messageHandlers=new Map;constructor(e){super(),this.config={extensionId:e?.extensionId||"default-extension-id",defaultTimeout:e?.defaultTimeout||3e4,maxRetries:e?.maxRetries||3,retryDelay:e?.retryDelay||1e3,screenshotOnError:e?.screenshotOnError??!0,debugMode:e?.debugMode??!1,waitForExtension:e?.waitForExtension??!0},this.connection={connected:!1},this.initializeConnection()}initializeConnection(){"undefined"!=typeof window?(window.addEventListener("message",this.handleExtensionMessage.bind(this)),this.config.waitForExtension&&this.waitForExtension()):this.log("Ambiente não é navegador, conexão simulada","warn")}async waitForExtension(e=1e4){const t=Date.now();for(;!this.connection.connected&&Date.now()-t<e;)await this.pingExtension(),await this.sleep(500);if(!this.connection.connected)throw this.log("Extensão não respondeu dentro do timeout","error"),new Error("Extensão não conectada")}async pingExtension(){try{const e=await this.sendToExtension({type:"PING"});e?.pong&&(this.connection.connected=!0,this.connection.lastPing=Date.now(),this.emit("extension:connected"),this.log("Extensão conectada com sucesso"))}catch(e){}}handleExtensionMessage(e){if("syncads-extension"!==e.data?.source)return;const{messageId:t,type:a,payload:n,error:s}=e.data;if("PONG"===a&&(this.connection.connected=!0,this.connection.lastPing=Date.now()),t&&this.messageHandlers.has(t)){this.messageHandlers.get(t)(s?{error:s}:n),this.messageHandlers.delete(t)}this.emit("extension:message",e.data)}sendToExtension(e){return new Promise((t,a)=>{const n=this.generateMessageId(),s=setTimeout(()=>{this.messageHandlers.delete(n),a(new Error("Timeout ao comunicar com extensão"))},this.config.defaultTimeout);this.messageHandlers.set(n,e=>{clearTimeout(s),e.error?a(new Error(e.error)):t(e)}),window.postMessage({source:"syncads-core",messageId:n,...e},"*")})}async navigate(e,t){const a={id:this.generateCommandId(),action:"NAVIGATE",value:e,options:t,timeout:this.config.defaultTimeout};return this.executeCommand(a)}async click(e,t){const a={id:this.generateCommandId(),action:"CLICK",selector:e,options:t,timeout:this.config.defaultTimeout,retries:this.config.maxRetries};return this.executeCommand(a)}async type(e,t,a){const n={id:this.generateCommandId(),action:"TYPE",selector:e,value:t,options:{...a,clearBefore:a?.clearBefore??!0},timeout:this.config.defaultTimeout,retries:this.config.maxRetries};return this.executeCommand(n)}async select(e,t,a){const n={id:this.generateCommandId(),action:"SELECT",selector:e,value:t,options:a,timeout:this.config.defaultTimeout,retries:this.config.maxRetries};return this.executeCommand(n)}async extract(e,t){const a={id:this.generateCommandId(),action:"EXTRACT",selector:e,options:{attributes:t},timeout:this.config.defaultTimeout};return this.executeCommand(a)}async screenshot(e=!1){const t={id:this.generateCommandId(),action:"SCREENSHOT",value:{fullPage:e},timeout:this.config.defaultTimeout};return this.executeCommand(t)}async executeScript(e,t){const a={id:this.generateCommandId(),action:"EXECUTE_SCRIPT",value:{script:e,args:t},timeout:this.config.defaultTimeout};return this.executeCommand(a)}async wait(e){const t={id:this.generateCommandId(),action:"WAIT",value:e,timeout:e+1e3};return this.executeCommand(t)}async fillForm(e){const t=[];for(const[n,s]of Object.entries(e.fields))try{let e;switch(s.type){case"select":e=await this.select(s.selector,s.value);break;case"checkbox":case"radio":e=await this.click(s.selector);break;case"file":e=await this.uploadFile(s.selector,s.value);break;default:e=await this.type(s.selector,s.value)}t.push(e),this.emit("form:field-filled",{fieldName:n,result:e})}catch(a){this.log(`Erro ao preencher campo ${n}: ${a.message}`,"error"),t.push({commandId:this.generateCommandId(),status:"FAILED",error:a.message,executionTime:0,retriesUsed:0})}if(e.submitButton){const a=await this.click(e.submitButton,{waitAfter:e.waitAfterSubmit});t.push(a)}return t}async scrape(e){const t=[];let a=1;const n=e.maxPages||10;for(;a<=n;)try{const s={};for(const[t,a]of Object.entries(e.selectors)){const n=await this.extract(a,"json"===e.output?["textContent","href","src"]:void 0);"SUCCESS"===n.status&&(s[t]=n.output)}if(t.push(s),this.emit("scrape:page-completed",{page:a,data:s}),!e.pagination||a>=n)break;if("SUCCESS"!==(await this.click(e.pagination.nextButtonSelector,{waitAfter:e.pagination.delay||2e3})).status)break;a++}catch(s){this.log(`Erro no scraping página ${a}: ${s.message}`,"error");break}return this.emit("scrape:completed",{totalPages:a,totalItems:t.length}),t}async uploadFile(e,t){const a={id:this.generateCommandId(),action:"UPLOAD_FILE",selector:e,value:t,timeout:this.config.defaultTimeout};return this.executeCommand(a)}async hover(e){const t={id:this.generateCommandId(),action:"HOVER",selector:e,timeout:this.config.defaultTimeout};return this.executeCommand(t)}async executePlan(e){this.log(`Executando plano: ${e.name}`),this.emit("plan:started",e);const t=[];let a=0;for(;a<e.steps.length;){const s=e.steps[a];try{if(e.conditions){const n=e.conditions.find(e=>e.step===a);if(n){if(!(await this.evaluateCondition(n,t))){if("skip"===n.action){a++;continue}if("abort"===n.action)break;if("branch"===n.action&&void 0!==n.branchTo){a=n.branchTo;continue}}}}const n=await this.executeCommand(s);if(t.push(n),this.emit("plan:step-completed",{step:a,result:n}),"FAILED"===n.status&&!e.errorHandling?.continueOnError){if(this.log(`Plano falhou no passo ${a}`,"error"),e.errorHandling?.fallbackPlan)return this.log("Executando plano de fallback"),this.executePlan(e.errorHandling.fallbackPlan);break}a++}catch(n){if(this.log(`Erro no passo ${a}: ${n.message}`,"error"),t.push({commandId:s.id,status:"FAILED",error:n.message,executionTime:0,retriesUsed:0}),!e.errorHandling?.continueOnError)break;a++}}return this.emit("plan:completed",{plan:e,results:t}),t}async evaluateCondition(e,t){return!0}createPlan(e,t){return new N(e,t)}async executeCommand(e){const t=Date.now();let a=0;const n=e.retries??this.config.maxRetries;for(this.emit("command:executing",e);a<=n;)try{e.options?.waitBefore&&await this.sleep(e.options.waitBefore);const n=await this.sendToExtension({type:"EXECUTE_COMMAND",payload:e});if(e.options?.waitAfter&&await this.sleep(e.options.waitAfter),e.options?.validateSuccess&&!e.options.validateSuccess(n))throw new Error("Validação de sucesso falhou");const s={commandId:e.id,status:"SUCCESS",output:n.data,executionTime:Date.now()-t,retriesUsed:a,elementFound:n.elementFound};return this.emit("command:success",s),s}catch(s){if(a++,this.log(`Erro no comando ${e.action}: ${s.message} (tentativa ${a}/${n})`,"warn"),a<=n){if(e.fallbackSelectors&&e.fallbackSelectors.length>0){const t=e.fallbackSelectors.shift();if(t){e.selector=t,this.log(`Tentando selector alternativo: ${t.value}`);continue}}await this.sleep(this.config.retryDelay*a);continue}const r=this.config.screenshotOnError?await this.captureErrorScreenshot():void 0,i={commandId:e.id,status:"FAILED",error:s.message,screenshot:r,executionTime:Date.now()-t,retriesUsed:a};return this.emit("command:failed",i),i}throw new Error("Máximo de tentativas excedido")}async captureErrorScreenshot(){try{return(await this.screenshot(!1)).output}catch{return}}buildSelector(e,t,a){return{type:e,value:t,waitForVisible:a?.waitForVisible??!0,waitForClickable:a?.waitForClickable??!1,index:a?.index??0,fallbacks:a?.fallbacks}}async getCurrentUrl(){return(await this.sendToExtension({type:"GET_CURRENT_URL"})).url}async getPageTitle(){return(await this.sendToExtension({type:"GET_PAGE_TITLE"})).title}async getDOM(){return(await this.sendToExtension({type:"GET_DOM"})).dom}isConnected(){return this.connection.connected}getConnection(){return{...this.connection}}generateCommandId(){return`cmd-${Date.now()}-${Math.random().toString(36).substr(2,9)}`}generateMessageId(){return`msg-${Date.now()}-${Math.random().toString(36).substr(2,9)}`}sleep(e){return new Promise(t=>setTimeout(t,e))}log(e,t="info"){this.config.debugMode,this.emit("log",{level:t,message:e,timestamp:Date.now()})}}class N{plan;constructor(e,t){this.plan={id:`plan-${Date.now()}`,name:e,description:t,steps:[],variables:{},conditions:[]}}navigate(e){return this.plan.steps.push({id:`step-${this.plan.steps.length}`,action:"NAVIGATE",value:e,timeout:3e4}),this}click(e,t){return this.plan.steps.push({id:`step-${this.plan.steps.length}`,action:"CLICK",selector:e,options:t,timeout:3e4}),this}type(e,t,a){return this.plan.steps.push({id:`step-${this.plan.steps.length}`,action:"TYPE",selector:e,value:t,options:a,timeout:3e4}),this}extract(e,t){return this.plan.steps.push({id:`step-${this.plan.steps.length}`,action:"EXTRACT",selector:e,options:{attributes:t},timeout:3e4}),this}wait(e){return this.plan.steps.push({id:`step-${this.plan.steps.length}`,action:"WAIT",value:e,timeout:e+1e3}),this}screenshot(e=!1){return this.plan.steps.push({id:`step-${this.plan.steps.length}`,action:"SCREENSHOT",value:{fullPage:e},timeout:1e4}),this}setVariable(e,t){return this.plan.variables[e]=t,this}addCondition(e){return this.plan.conditions.push(e),this}setErrorHandling(e){return this.plan.errorHandling=e,this}build(){return this.plan}}const U={id:"pandas-001",name:"Pandas",packageName:"pandas",version:"2.2.0",category:w.DATA_PROCESSING,subcategories:["data-analysis","data-manipulation","csv-processing","excel-processing","time-series","statistical-analysis"],description:"Biblioteca poderosa de análise e manipulação de dados em Python, fornecendo estruturas de dados flexíveis e eficientes.",purpose:"Processar, analisar, transformar e manipular dados estruturados (tabelas, séries temporais, etc.)",useCases:["Leitura e escrita de arquivos CSV, Excel, JSON, SQL","Limpeza e preparação de dados","Análise exploratória de dados","Transformação e agregação de dados","Manipulação de séries temporais","Join e merge de datasets","Pivoteamento e reshaping de dados","Cálculos estatísticos e agregações","Filtragem e seleção de dados","Tratamento de valores missing"],complexity:T.INTERMEDIATE,environment:S.PYTHON,dependencies:["numpy","python-dateutil","pytz"],installCommand:"pip install pandas numpy",promptSystem:{systemPrompt:"Você é um especialista em Pandas, a biblioteca Python para análise de dados.\nAo trabalhar com Pandas, você SEMPRE deve:\n- Usar métodos eficientes e vetorizados (evitar loops)\n- Verificar tipos de dados com df.dtypes\n- Tratar valores NaN apropriadamente\n- Usar inplace=False por padrão para imutabilidade\n- Aplicar copy() quando necessário para evitar SettingWithCopyWarning\n- Usar loc/iloc para indexação explícita\n- Considerar memory_usage() para datasets grandes\n- Documentar transformações importantes",instructions:["1. SEMPRE importe pandas como: import pandas as pd","2. Verifique se o arquivo existe antes de ler","3. Use try-except para operações de I/O","4. Especifique dtypes ao ler dados grandes para economizar memória","5. Use chunksize para arquivos muito grandes","6. Sempre verifique df.head(), df.info() e df.describe() após carregar dados","7. Use pd.to_datetime() para converter strings em datas","8. Prefira query() ou loc[] para filtragens complexas","9. Use groupby().agg() para agregações múltiplas","10. Sempre valide o output com assert ou print de verificação"],bestPractices:["Use method chaining para operações sequenciais: df.pipe().pipe().pipe()","Prefira vectorização sobre iteração: use apply(), map(), ou operações nativas","Configure display.max_columns e display.max_rows para debugging","Use categorical dtype para colunas com poucos valores únicos","Aplique astype() para otimizar tipos de dados","Use merge() com indicador=True para debug de joins","Prefira concat() com keys para rastreabilidade","Use pd.options para configurar comportamento global","Aplique reset_index() após operações que modificam índice","Use copy() explicitamente quando criar views modificáveis"],commonPitfalls:["EVITE: df[df.col == valor] sem usar copy() - causa SettingWithCopyWarning","EVITE: Loops com iterrows() - extremamente lento","EVITE: Múltiplos append() em loop - use list + pd.concat()","EVITE: inplace=True sem necessidade - dificulta debug","EVITE: Comparações com NaN usando == - use isna() ou notna()","EVITE: Modificar DataFrame durante iteração","EVITE: Assumir ordem de colunas - sempre use nomes explícitos","EVITE: Ignorar warnings de dtype - podem causar bugs silenciosos","EVITE: Usar string indexing sem verificar existência da coluna","EVITE: apply() com funções pesadas - considere numba ou cython"],errorHandling:["FileNotFoundError: Verificar se arquivo existe com os.path.exists()",'KeyError: Usar .get() ou verificar coluna com "col" in df.columns',"ValueError: Validar formato dos dados antes de operações","MemoryError: Usar chunksize ou dask para dados muito grandes","SettingWithCopyWarning: Sempre usar .copy() ou .loc[] apropriadamente","ParserError: Especificar encoding, sep, e error_bad_lines corretamente","DtypeWarning: Especificar dtype explicitamente ao ler arquivos","IndexError: Verificar tamanho do DataFrame antes de acessar índices","TypeError: Verificar tipos com isinstance() antes de operações","PerformanceWarning: Otimizar queries ou usar indices apropriados"],optimizationTips:["Use category dtype para colunas com baixa cardinalidade","Especifique usecols ao ler apenas colunas necessárias","Use nrows para testar código com subset dos dados","Configure chunksize para processar dados em lotes",'Use query() com engine="numexpr" para queries complexas',"Aplique eval() para expressões aritméticas complexas","Use pd.read_csv() com dtype dict para economia de memória","Configure low_memory=False para evitar mixed types","Use sparse arrays para dados esparsos","Considere parquet ou feather para I/O mais rápido que CSV"]},whenToUse:[{condition:"Processar arquivos CSV, Excel, ou JSON",reasoning:"Pandas tem readers otimizados e robustos para estes formatos",confidence:.95,priority:1},{condition:"Análise exploratória de dados estruturados",reasoning:"Métodos integrados como describe(), info(), value_counts() são perfeitos",confidence:.98,priority:1},{condition:"Operações de agregação e groupby",reasoning:"GroupBy do Pandas é extremamente poderoso e flexível",confidence:.97,priority:1},{condition:"Limpeza e transformação de dados",reasoning:"Ferramentas completas para fillna(), dropna(), replace(), etc",confidence:.96,priority:1},{condition:"Trabalhar com séries temporais",reasoning:"Suporte nativo a datetime, resampling, rolling windows",confidence:.94,priority:2},{condition:"Merge, join, concatenação de datasets",reasoning:"Operações SQL-like com sintaxe simples",confidence:.95,priority:2},{condition:"Pivoteamento e reshaping de dados",reasoning:"pivot_table(), melt(), stack(), unstack() muito poderosos",confidence:.93,priority:2},{condition:"Dados tabulares com até ~10M linhas",reasoning:"Performance adequada para a maioria dos casos",confidence:.9,priority:3}],whenNotToUse:[{condition:"Dados com bilhões de linhas",reasoning:"Pandas carrega tudo em memória - use Dask, PySpark, ou Polars",confidence:.95,priority:1},{condition:"Operações que requerem streaming de dados",reasoning:"Pandas não é otimizado para streaming - considere outras soluções",confidence:.9,priority:2},{condition:"Processamento distribuído necessário",reasoning:"Pandas é single-threaded - use Dask ou PySpark",confidence:.93,priority:1},{condition:"Apenas operações matemáticas em arrays",reasoning:"NumPy puro é mais eficiente",confidence:.85,priority:3},{condition:"Necessita de performance máxima",reasoning:"Polars é mais rápido, use quando performance é crítica",confidence:.88,priority:2},{condition:"Dados não estruturados ou aninhados",reasoning:"Use outras bibliotecas especializadas",confidence:.87,priority:3}],mainFunctions:[{name:"pd.read_csv",description:"Lê arquivo CSV e retorna DataFrame",signature:'pd.read_csv(filepath, sep=",", header="infer", dtype=None, parse_dates=False, **kwargs)',parameters:[{name:"filepath",type:"str",required:!0,description:"Caminho do arquivo CSV"},{name:"sep",type:"str",required:!1,default:",",description:"Delimitador do CSV"},{name:"header",type:"int|list",required:!1,default:"infer",description:"Linha com nomes das colunas"},{name:"dtype",type:"dict",required:!1,description:"Tipos de dados por coluna"},{name:"parse_dates",type:"bool|list",required:!1,default:!1,description:"Colunas para converter em datetime"},{name:"encoding",type:"str",required:!1,default:"utf-8",description:"Encoding do arquivo"}],returnType:"pd.DataFrame",example:'df = pd.read_csv("data.csv", parse_dates=["date"], dtype={"id": int})',promptTemplate:"Para ler o arquivo {filepath}, use pd.read_csv() com os parâmetros apropriados"},{name:"df.groupby",description:"Agrupa DataFrame por uma ou mais colunas",signature:"df.groupby(by, axis=0, level=None, as_index=True, sort=True)",parameters:[{name:"by",type:"str|list",required:!0,description:"Colunas para agrupar"},{name:"as_index",type:"bool",required:!1,default:!0,description:"Usar chaves de agrupamento como índice"}],returnType:"pd.DataFrameGroupBy",example:'df.groupby("category").agg({"sales": "sum", "quantity": "mean"})',promptTemplate:"Para agrupar por {by} e calcular agregações, use groupby().agg()"},{name:"df.merge",description:"Combina dois DataFrames usando join SQL-style",signature:'df.merge(right, how="inner", on=None, left_on=None, right_on=None)',parameters:[{name:"right",type:"pd.DataFrame",required:!0,description:"DataFrame da direita"},{name:"how",type:"str",required:!1,default:"inner",description:"Tipo de join (inner, left, right, outer)"},{name:"on",type:"str|list",required:!1,description:"Colunas para join"}],returnType:"pd.DataFrame",example:'merged = df1.merge(df2, on="id", how="left", indicator=True)',promptTemplate:"Para combinar {left} e {right} por {on}, use merge() com how={how}"},{name:"df.fillna",description:"Preenche valores NaN/missing",signature:"df.fillna(value=None, method=None, axis=None, inplace=False)",parameters:[{name:"value",type:"scalar|dict|Series",required:!1,description:"Valor para preencher NaN"},{name:"method",type:"str",required:!1,description:"Método: ffill, bfill"},{name:"inplace",type:"bool",required:!1,default:!1,description:"Modificar in-place"}],returnType:"pd.DataFrame",example:'df.fillna({"age": df["age"].mean(), "name": "Unknown"})',promptTemplate:"Para preencher valores missing em {columns}, use fillna() com estratégia apropriada"},{name:"df.pivot_table",description:"Cria tabela pivoteada",signature:'df.pivot_table(values, index, columns, aggfunc="mean", fill_value=None)',parameters:[{name:"values",type:"str|list",required:!0,description:"Colunas para agregar"},{name:"index",type:"str|list",required:!0,description:"Colunas para linhas"},{name:"columns",type:"str|list",required:!0,description:"Colunas para pivotear"},{name:"aggfunc",type:"function|str",required:!1,default:"mean",description:"Função de agregação"}],returnType:"pd.DataFrame",example:'pivot = df.pivot_table(values="sales", index="date", columns="product", aggfunc="sum")',promptTemplate:"Para pivotar {values} por {index} e {columns}, use pivot_table()"}],examples:[{title:"Leitura e Análise Básica de CSV",description:"Carregar arquivo CSV e fazer análise exploratória inicial",input:{filepath:"vendas.csv",action:"analisar dados"},code:'import pandas as pd\nimport os\n\n# Verificar se arquivo existe\nif not os.path.exists(\'vendas.csv\'):\n    raise FileNotFoundError("Arquivo vendas.csv não encontrado")\n\n# Ler CSV\ndf = pd.read_csv(\n    \'vendas.csv\',\n    parse_dates=[\'data_venda\'],\n    dtype={\'produto_id\': int, \'quantidade\': int}\n)\n\n# Análise exploratória\nprint("Informações do DataFrame:")\nprint(df.info())\nprint("\\nPrimeiras linhas:")\nprint(df.head())\nprint("\\nEstatísticas descritivas:")\nprint(df.describe())\nprint("\\nValores únicos por coluna:")\nprint(df.nunique())\nprint("\\nValores missing:")\nprint(df.isna().sum())',output:{rows:1e3,columns:5,missing_values:0},explanation:"Este código demonstra as melhores práticas para ler um CSV e fazer análise inicial completa",useCase:"Análise exploratória de dados de vendas"},{title:"Agregação e GroupBy Avançado",description:"Agrupar dados e calcular múltiplas agregações",input:{data:"vendas_df",group_by:"categoria",metrics:["soma","média","contagem"]},code:"# Múltiplas agregações em uma operação\nresultado = df.groupby('categoria').agg({\n    'valor_venda': ['sum', 'mean', 'count'],\n    'quantidade': ['sum', 'mean'],\n    'desconto': 'mean'\n}).round(2)\n\n# Renomear colunas\nresultado.columns = ['_'.join(col).strip() for col in resultado.columns.values]\nresultado = resultado.reset_index()\n\n# Adicionar percentuais\nresultado['percentual_vendas'] = (\n    resultado['valor_venda_sum'] / resultado['valor_venda_sum'].sum() * 100\n).round(2)\n\nprint(resultado)",output:{grouped_rows:10,aggregations:6},explanation:"Demonstra como fazer agregações múltiplas e calcular métricas derivadas",useCase:"Relatório de vendas por categoria"},{title:"Limpeza de Dados",description:"Limpar e preparar dados para análise",input:{data:"raw_data",issues:["missing","duplicates","outliers"]},code:"# Remover duplicatas\ndf_clean = df.drop_duplicates(subset=['id'], keep='first')\n\n# Tratar valores missing\ndf_clean['idade'].fillna(df_clean['idade'].median(), inplace=True)\ndf_clean['categoria'].fillna('Desconhecido', inplace=True)\n\n# Remover outliers usando IQR\nQ1 = df_clean['valor'].quantile(0.25)\nQ3 = df_clean['valor'].quantile(0.75)\nIQR = Q3 - Q1\ndf_clean = df_clean[\n    (df_clean['valor'] >= Q1 - 1.5 * IQR) &\n    (df_clean['valor'] <= Q3 + 1.5 * IQR)\n]\n\n# Converter tipos\ndf_clean['data'] = pd.to_datetime(df_clean['data'], errors='coerce')\ndf_clean['categoria'] = df_clean['categoria'].astype('category')\n\n# Validação final\nassert df_clean.duplicated().sum() == 0, \"Ainda existem duplicatas\"\nprint(f\"Dados limpos: {len(df_clean)} linhas\")",output:{clean_rows:950,removed_duplicates:30,removed_outliers:20},explanation:"Pipeline completo de limpeza de dados com validações",useCase:"Preparação de dados para machine learning"},{title:"Join de Múltiplos DataFrames",description:"Combinar dados de diferentes fontes",input:{tables:["clientes","pedidos","produtos"]},code:"# Merge com indicador para debug\ndf_merged = (\n    pedidos\n    .merge(clientes, on='cliente_id', how='left', suffixes=('_pedido', '_cliente'))\n    .merge(produtos, on='produto_id', how='left')\n)\n\n# Verificar resultados do merge\nprint(\"Pedidos sem cliente:\", df_merged['cliente_id'].isna().sum())\nprint(\"Pedidos sem produto:\", df_merged['produto_id'].isna().sum())\n\n# Criar features derivadas\ndf_merged['ticket_medio'] = df_merged['valor_total'] / df_merged['quantidade']\ndf_merged['dias_desde_cadastro'] = (\n    pd.to_datetime('today') - df_merged['data_cadastro']\n).dt.days\n\nprint(df_merged.head())",output:{merged_rows:5e3,columns:25},explanation:"Demonstra joins complexos com validações e feature engineering",useCase:"Análise de comportamento de compra de clientes"},{title:"Análise de Série Temporal",description:"Processar e analisar dados temporais",input:{data:"vendas_diarias",analysis:"tendência e sazonalidade"},code:"# Configurar índice temporal\ndf['data'] = pd.to_datetime(df['data'])\ndf = df.set_index('data').sort_index()\n\n# Resample para diferentes períodos\nvendas_mensais = df['valor'].resample('M').sum()\nvendas_semanais = df['valor'].resample('W').sum()\n\n# Rolling windows\ndf['media_movel_7d'] = df['valor'].rolling(window=7).mean()\ndf['media_movel_30d'] = df['valor'].rolling(window=30).mean()\n\n# Crescimento percentual\ndf['crescimento_dia'] = df['valor'].pct_change() * 100\ndf['crescimento_mes'] = df['valor'].pct_change(periods=30) * 100\n\n# Estatísticas por dia da semana\npor_dia_semana = df.groupby(df.index.dayofweek)['valor'].agg(['mean', 'sum', 'count'])\npor_dia_semana.index = ['Seg', 'Ter', 'Qua', 'Qui', 'Sex', 'Sab', 'Dom']\n\nprint(por_dia_semana)",output:{time_series_metrics:"calculated",seasonality:"detected"},explanation:"Análise completa de série temporal com resampling e rolling windows",useCase:"Previsão de vendas e análise de tendências"}],inputFormat:{type:"multiple",description:"Pandas aceita diversos formatos de entrada",examples:[{format:"CSV",code:'pd.read_csv("file.csv")'},{format:"Excel",code:'pd.read_excel("file.xlsx")'},{format:"JSON",code:'pd.read_json("file.json")'},{format:"SQL",code:'pd.read_sql("SELECT * FROM table", conn)'},{format:"Parquet",code:'pd.read_parquet("file.parquet")'},{format:"Dictionary",code:'pd.DataFrame({"col": [1, 2, 3]})'}],validation:"Sempre verificar se arquivo existe antes de ler"},outputFormat:{type:"pd.DataFrame or pd.Series",description:"Retorna DataFrame ou Series dependendo da operação",examples:[{operation:"read",output:"pd.DataFrame"},{operation:"column selection",output:"pd.Series"},{operation:"aggregation",output:"pd.DataFrame or scalar"}],validation:"Sempre verificar tipo com type() e shape com .shape"},fallbackModules:["polars-001","dask-001","numpy-001"],alternativeModules:["modin-001","vaex-001"],avgExecutionTime:500,memoryUsage:"2-3x do tamanho dos dados em disco",cpuIntensive:!1,gpuSupport:!1,reliability:.98,successRate:.97,popularity:100,lastUpdated:Date.now(),status:"active",documentation:"https://pandas.pydata.org/docs/",repository:"https://github.com/pandas-dev/pandas",license:"BSD-3-Clause",tags:["data-analysis","data-processing","csv","excel","dataframe","time-series","statistics","etl","data-cleaning","data-transformation"]},q={id:"numpy-002",name:"NumPy",packageName:"numpy",version:"1.26.0",category:w.SCIENTIFIC_COMPUTING,subcategories:["numerical-computing","array-operations","linear-algebra","mathematical-operations","random-numbers","fourier-transform","statistics"],description:"Biblioteca fundamental para computação científica em Python, fornecendo suporte para arrays multidimensionais e operações matemáticas de alto desempenho.",purpose:"Realizar operações numéricas eficientes em arrays, álgebra linear, transformadas matemáticas e computação científica",useCases:["Operações matemáticas vetorizadas em arrays","Álgebra linear (matrizes, vetores, sistemas lineares)","Geração de números aleatórios","Transformadas de Fourier","Operações estatísticas básicas","Broadcasting e manipulação de arrays multidimensionais","Indexação avançada e slicing","Operações elemento por elemento (element-wise)","Reshaping e transposição de arrays","Integração com outras bibliotecas científicas"],complexity:T.INTERMEDIATE,environment:S.PYTHON,dependencies:[],installCommand:"pip install numpy",promptSystem:{systemPrompt:"Você é um especialista em NumPy, a biblioteca Python para computação numérica.\nAo trabalhar com NumPy, você SEMPRE deve:\n- Usar operações vetorizadas ao invés de loops Python\n- Especificar dtype apropriado para economizar memória\n- Usar broadcasting quando possível\n- Evitar cópias desnecessárias de arrays\n- Usar views ao invés de cópias quando apropriado\n- Considerar ordem de memória (C-contiguous vs F-contiguous)\n- Usar funções universais (ufuncs) para operações elemento por elemento\n- Documentar shape esperado dos arrays",instructions:["1. SEMPRE importe numpy como: import numpy as np","2. Especifique dtype ao criar arrays: np.array([1, 2, 3], dtype=np.float32)","3. Use np.zeros(), np.ones(), np.empty() para pré-alocar arrays","4. Prefira operações vetorizadas: arr + 5 ao invés de [x + 5 for x in arr]","5. Use broadcasting para operações em arrays de diferentes shapes","6. Verifique shape dos arrays com arr.shape antes de operações","7. Use np.newaxis ou reshape para adicionar dimensões","8. Prefira arr.copy() explícito quando precisar de cópia","9. Use np.where() para operações condicionais vetorizadas","10. Sempre valide dimensões antes de operações de álgebra linear"],bestPractices:["Use dtype apropriado: int8/16/32 para inteiros, float32/64 para decimais","Pré-aloque arrays quando souber o tamanho final","Use np.concatenate() ou np.vstack()/np.hstack() ao invés de append em loop","Aproveite broadcasting para evitar loops explícitos","Use views (slicing) ao invés de cópias quando possível","Prefira np.einsum() para operações tensoriais complexas","Use np.clip() para limitar valores em arrays","Configure np.seterr() para controlar warnings de overflow","Use np.memmap() para arrays maiores que a RAM","Aproveite axis parameter em funções de agregação"],commonPitfalls:["EVITE: Modificar array durante iteração","EVITE: Usar listas Python para cálculos numéricos - use arrays","EVITE: Loops Python - use operações vetorizadas","EVITE: Comparações com == para floats - use np.isclose()","EVITE: Criar arrays com append em loop - pré-aloque","EVITE: Usar dtype object - especifique tipo numérico","EVITE: Assumir que operações retornam cópias - podem ser views","EVITE: Divisão por zero sem tratamento","EVITE: Broadcasting acidental que muda dimensões inesperadamente","EVITE: Usar tolist() desnecessariamente - mantém como array"],errorHandling:["ValueError: Verificar shapes compatíveis antes de operações","MemoryError: Usar dtype menor ou np.memmap() para arrays grandes","IndexError: Validar índices com array.shape","TypeError: Garantir tipos compatíveis com operação","RuntimeWarning (divide by zero): Usar np.errstate() ou np.seterr()","LinAlgError: Verificar se matriz é singular ou mal-condicionada"],userPromptTemplate:"Tarefa: {task_description}\n\nEntrada:\n{input_description}\n\nParâmetros:\n{parameters}\n\nUse NumPy para realizar operações numéricas eficientes:\n1. Importe numpy como np\n2. Crie/manipule arrays apropriadamente\n3. Use operações vetorizadas\n4. Retorne resultados estruturados",examples:[{input:"Criar array e realizar operações matemáticas básicas",output:'import numpy as np\n\ndef basic_array_operations():\n    """Demonstra operações básicas com arrays NumPy"""\n    # Criar arrays\n    arr1 = np.array([1, 2, 3, 4, 5])\n    arr2 = np.array([10, 20, 30, 40, 50])\n\n    # Operações vetorizadas (muito mais rápido que loops)\n    soma = arr1 + arr2\n    produto = arr1 * arr2\n    potencia = arr1 ** 2\n\n    # Operações estatísticas\n    media = np.mean(arr1)\n    desvio = np.std(arr1)\n\n    return {\n        "soma": soma.tolist(),\n        "produto": produto.tolist(),\n        "potencia": potencia.tolist(),\n        "estatisticas": {\n            "media": float(media),\n            "desvio_padrao": float(desvio),\n            "minimo": int(arr1.min()),\n            "maximo": int(arr1.max())\n        }\n    }'},{input:"Álgebra linear: multiplicação de matrizes",output:'import numpy as np\n\ndef matrix_operations():\n    """Operações de álgebra linear com NumPy"""\n    # Criar matrizes\n    A = np.array([[1, 2], [3, 4]], dtype=np.float64)\n    B = np.array([[5, 6], [7, 8]], dtype=np.float64)\n\n    # Multiplicação de matrizes (@ ou np.dot)\n    produto_matricial = A @ B\n\n    # Transposta\n    A_t = A.T\n\n    # Determinante\n    det_A = np.linalg.det(A)\n\n    # Inversa (se não singular)\n    try:\n        A_inv = np.linalg.inv(A)\n        inversivel = True\n    except np.linalg.LinAlgError:\n        A_inv = None\n        inversivel = False\n\n    # Autovalores e autovetores\n    eigenvalues, eigenvectors = np.linalg.eig(A)\n\n    return {\n        "produto_matricial": produto_matricial.tolist(),\n        "determinante": float(det_A),\n        "inversivel": inversivel,\n        "autovalores": eigenvalues.tolist(),\n        "transposta": A_t.tolist()\n    }'},{input:"Broadcasting para operações eficientes",output:'import numpy as np\n\ndef broadcasting_example():\n    """Demonstra broadcasting para operações eficientes"""\n    # Array 2D (3x4)\n    matriz = np.array([[1, 2, 3, 4],\n                       [5, 6, 7, 8],\n                       [9, 10, 11, 12]])\n\n    # Vetor 1D (4,)\n    vetor = np.array([1, 0, 1, 0])\n\n    # Broadcasting: vetor é automaticamente expandido\n    resultado = matriz + vetor  # (3,4) + (4,) = (3,4)\n\n    # Normalizar cada linha (subtrair média da linha)\n    medias_linhas = matriz.mean(axis=1, keepdims=True)  # (3,1)\n    matriz_normalizada = matriz - medias_linhas  # Broadcasting\n\n    # Normalizar cada coluna\n    medias_colunas = matriz.mean(axis=0)  # (4,)\n    matriz_norm_col = matriz - medias_colunas\n\n    return {\n        "matriz_original": matriz.tolist(),\n        "resultado_broadcast": resultado.tolist(),\n        "matriz_normalizada_linhas": matriz_normalizada.tolist(),\n        "matriz_normalizada_colunas": matriz_norm_col.tolist()\n    }'},{input:"Indexação avançada e slicing",output:'import numpy as np\n\ndef advanced_indexing():\n    """Demonstra indexação avançada e slicing"""\n    # Array 2D\n    arr = np.arange(20).reshape(4, 5)\n\n    # Slicing básico (retorna view, não cópia)\n    subarray = arr[1:3, 2:4]\n\n    # Indexação booleana (retorna cópia)\n    maiores_que_10 = arr[arr > 10]\n\n    # Fancy indexing (retorna cópia)\n    linhas = [0, 2]\n    colunas = [1, 3]\n    elementos_especificos = arr[linhas, colunas]\n\n    # np.where para indexação condicional\n    indices = np.where(arr > 10)\n\n    # Modificar elementos condicionalmente\n    arr_modificado = arr.copy()\n    arr_modificado[arr_modificado < 5] = 0\n\n    return {\n        "array_original": arr.tolist(),\n        "subarray": subarray.tolist(),\n        "maiores_que_10": maiores_que_10.tolist(),\n        "elementos_especificos": elementos_especificos.tolist(),\n        "indices_maiores_10": {\n            "linhas": indices[0].tolist(),\n            "colunas": indices[1].tolist()\n        },\n        "array_modificado": arr_modificado.tolist()\n    }'},{input:"Geração de números aleatórios",output:'import numpy as np\n\ndef random_operations():\n    """Geração de números aleatórios com NumPy"""\n    # Fixar seed para reprodutibilidade\n    np.random.seed(42)\n\n    # Números uniformes [0, 1)\n    uniforme = np.random.random(5)\n\n    # Inteiros aleatórios\n    inteiros = np.random.randint(0, 100, size=10)\n\n    # Distribuição normal (média=0, std=1)\n    normal = np.random.randn(1000)\n\n    # Escolher aleatoriamente de um array\n    opcoes = np.array([\'A\', \'B\', \'C\', \'D\'])\n    escolhas = np.random.choice(opcoes, size=10, replace=True)\n\n    # Embaralhar array\n    arr = np.arange(10)\n    np.random.shuffle(arr)\n\n    # Amostragem sem reposição\n    amostra = np.random.choice(np.arange(100), size=10, replace=False)\n\n    return {\n        "uniforme": uniforme.tolist(),\n        "inteiros": inteiros.tolist(),\n        "normal_stats": {\n            "media": float(normal.mean()),\n            "std": float(normal.std()),\n            "min": float(normal.min()),\n            "max": float(normal.max())\n        },\n        "escolhas": escolhas.tolist(),\n        "array_embaralhado": arr.tolist(),\n        "amostra": amostra.tolist()\n    }'}],outputFormat:{type:"object",required:["success"],properties:{success:{type:"boolean",description:"Indica se a operação foi bem-sucedida"},result:{type:"any",description:"Resultado da operação (array, matriz, etc)"},shape:{type:"array",description:"Dimensões do resultado"},dtype:{type:"string",description:"Tipo de dados do array"},statistics:{type:"object",description:"Estatísticas descritivas (se aplicável)"},error:{type:"string",description:"Mensagem de erro se success=false"}}}},tags:["numpy","numerical","scientific","array","matrix","linear-algebra","math","statistics","vectorization"],keywords:["numpy","np","array","matrix","matriz","linear algebra","algebra linear","mathematical","matematica","numerical","numerico","vectorization","vetorizacao","broadcasting","statistics","estatistica","random","aleatorio"],performance:{speed:9,memory:8,cpuIntensive:!0,gpuAccelerated:!1,scalability:9},scoring:{baseScore:.95,rules:[{condition:'keywords include ["array", "matrix", "numerical"]',adjustment:.05,description:"Fundamental para operações numéricas"},{condition:'keywords include ["linear algebra", "algebra"]',adjustment:.03,description:"Excelente para álgebra linear"},{condition:'keywords include ["vectorization", "broadcast"]',adjustment:.02,description:"Operações vetorizadas eficientes"},{condition:'keywords include ["statistics", "math", "mathematical"]',adjustment:.02,description:"Operações matemáticas e estatísticas"},{condition:'keywords include ["random", "aleatorio"]',adjustment:.02,description:"Geração de números aleatórios"},{condition:'keywords include ["dataframe", "pandas"]',adjustment:-.2,description:"Para DataFrames, use Pandas"},{condition:'keywords include ["plot", "graph", "visualization"]',adjustment:-.4,description:"Para visualização, use Matplotlib/Seaborn"},{condition:'keywords include ["deep learning", "neural network"]',adjustment:-.3,description:"Para deep learning, use TensorFlow/PyTorch"}]},config:{maxRetries:2,timeout:3e4,cacheable:!0,requiresAuth:!1,rateLimit:null},alternatives:[{name:"Pandas",when:"Trabalhar com dados tabulares (DataFrames)",reason:"Pandas é construído sobre NumPy mas oferece estruturas de dados de alto nível"},{name:"SciPy",when:"Computação científica avançada, otimização, integração",reason:"SciPy estende NumPy com funcionalidades científicas especializadas"},{name:"TensorFlow/PyTorch",when:"Deep learning, GPU acceleration, redes neurais",reason:"Frameworks modernos com suporte a GPU e diferenciação automática"},{name:"CuPy",when:"Operações NumPy com GPU (CUDA)",reason:"CuPy é NumPy para GPU"}],documentation:{official:"https://numpy.org/doc/",examples:"https://numpy.org/doc/stable/user/quickstart.html",apiReference:"https://numpy.org/doc/stable/reference/"},commonIssues:[{issue:"ValueError: operands could not be broadcast together",solution:"Verificar shapes dos arrays e usar reshape ou newaxis",code:"# Erro: shapes incompatíveis\n# arr1.shape = (3, 4), arr2.shape = (3,)\narr2_reshaped = arr2[:, np.newaxis]  # (3, 1)\nresult = arr1 + arr2_reshaped  # Broadcasting correto"},{issue:"MemoryError: array is too large",solution:"Usar dtype menor ou np.memmap para arrays grandes",code:"# Usar dtype menor\narr = np.zeros(shape, dtype=np.float32)  # ao invés de float64\n\n# Ou usar memmap para arrays gigantes\narr = np.memmap('temp.dat', dtype='float32', mode='w+', shape=shape)"},{issue:"RuntimeWarning: divide by zero",solution:"Usar np.errstate ou np.seterr para controlar warnings",code:"# Suprimir warnings temporariamente\nwith np.errstate(divide='ignore', invalid='ignore'):\n    result = arr1 / arr2\n\n# Ou tratar explicitamente\nresult = np.divide(arr1, arr2, out=np.zeros_like(arr1), where=arr2!=0)"},{issue:"Modificação acidental de array original (view vs copy)",solution:"Usar .copy() explicitamente quando necessário",code:"# View (modifica original)\nview = arr[1:5]\nview[0] = 999  # Modifica arr também!\n\n# Copy (não modifica original)\ncopia = arr[1:5].copy()\ncopia[0] = 999  # arr permanece inalterado"}],bestPractices:["Sempre use operações vetorizadas ao invés de loops Python","Especifique dtype apropriado ao criar arrays para economizar memória","Use broadcasting para operações em arrays de shapes diferentes","Pré-aloque arrays com np.zeros/ones/empty quando souber o tamanho","Verifique shapes com .shape antes de operações de álgebra linear","Use views (slicing) quando possível ao invés de criar cópias","Configure np.seterr() para controlar warnings de operações numéricas","Use axis parameter em funções de agregação para controlar dimensões"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},O={id:"pillow-003",name:"Pillow (PIL)",packageName:"Pillow",version:"10.2.0",category:w.IMAGE_PROCESSING,subcategories:["image-manipulation","resize","crop","filters","format-conversion","thumbnails","watermark","color-manipulation"],description:"Biblioteca Python mais popular para processamento de imagens, oferecendo API simples e intuitiva para operações de imagem como resize, crop, rotate, filtros e conversão de formato.",purpose:"Processar, manipular e transformar imagens de forma simples e eficiente",useCases:["Redimensionar imagens (resize/scale)","Cortar imagens (crop)","Converter formatos de imagem (JPEG, PNG, WebP, GIF, etc)","Rotacionar e espelhar imagens","Criar thumbnails mantendo aspect ratio","Aplicar filtros básicos (blur, sharpen, edge enhance)","Adicionar texto e watermarks","Manipular cores e brilho","Otimizar tamanho de arquivos","Processar imagens em batch"],complexity:T.BASIC,environment:S.PYTHON,dependencies:[],installCommand:"pip install Pillow",promptSystem:{systemPrompt:"Você é um especialista em Pillow (PIL), a biblioteca Python de processamento de imagens.\n\nAo trabalhar com Pillow, você SEMPRE deve:\n- Usar Image.Resampling.LANCZOS para melhor qualidade em resize\n- Adicionar optimize=True ao salvar imagens\n- Verificar o modo da imagem antes de salvar em JPEG (não aceita transparência)\n- Usar thumbnail() para preservar aspect ratio automaticamente\n- Fechar imagens após processamento ou usar context manager\n- Especificar quality ao salvar (85-95 para produção)\n- Converter RGBA para RGB quando necessário para JPEG\n\nREGRAS DE USO:\n1. SEMPRE use try-except para operações de arquivo\n2. SEMPRE valide formatos de entrada antes de processar\n3. SEMPRE otimize imagens ao salvar (optimize=True)\n4. NUNCA carregue imagens muito grandes (>100MB) sem processar em chunks\n5. SEMPRE especifique o método de resampling em resize\n6. SEMPRE verifique transparência antes de salvar em JPEG\n7. Use quality=95 para arquivos importantes, 85 para web\n\nQUANDO USAR PILLOW:\n✅ Operações básicas de imagem (resize, crop, rotate)\n✅ Conversão de formato de imagem\n✅ Criar thumbnails\n✅ Adicionar watermarks ou texto\n✅ Aplicar filtros simples\n✅ Otimização de tamanho de arquivo\n✅ Imagens de até 50-100MB\n\nQUANDO NÃO USAR PILLOW:\n❌ Computer vision avançada (use OpenCV)\n❌ Detecção de objetos/faces (use OpenCV ou modelos ML)\n❌ Processamento de vídeo (use OpenCV ou moviepy)\n❌ Imagens gigantes >100MB (use pyvips)\n❌ Performance crítica em batch (use OpenCV)\n❌ GPU acceleration necessária\n\nESTRUTURA DE RESPOSTA:\nSempre retorne um dicionário com:\n- original_size: dimensões originais\n- new_size: novas dimensões (se aplicável)\n- output_path: caminho do arquivo salvo\n- format: formato final\n- size_bytes: tamanho em bytes\n- mode: modo de cor (RGB, RGBA, L, etc)",userPromptTemplate:"Tarefa: {task_description}\n\nEntrada:\n{input_description}\n\nParâmetros:\n{parameters}\n\nUse Pillow (PIL) para processar a imagem seguindo as melhores práticas:\n1. Abra a imagem com Image.open()\n2. Execute a operação solicitada\n3. Otimize e salve o resultado\n4. Retorne informações detalhadas",examples:[{input:"Redimensionar imagem para 800x600",output:'from PIL import Image\n\ndef resize_image(input_path: str, output_path: str, width: int = 800, height: int = 600):\n    """Redimensiona imagem com alta qualidade"""\n    try:\n        # Abrir imagem\n        with Image.open(input_path) as img:\n            original_size = img.size\n\n            # Redimensionar com melhor qualidade\n            img_resized = img.resize((width, height), Image.Resampling.LANCZOS)\n\n            # Salvar otimizado\n            img_resized.save(output_path, quality=95, optimize=True)\n\n            return {\n                "success": True,\n                "original_size": original_size,\n                "new_size": (width, height),\n                "output_path": output_path,\n                "format": img.format\n            }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Criar thumbnail mantendo proporção",output:'from PIL import Image\n\ndef create_thumbnail(input_path: str, output_path: str, max_size: tuple = (200, 200)):\n    """Cria thumbnail mantendo aspect ratio"""\n    try:\n        with Image.open(input_path) as img:\n            original_size = img.size\n\n            # Thumbnail preserva proporção automaticamente\n            img.thumbnail(max_size, Image.Resampling.LANCZOS)\n\n            # Salvar otimizado\n            img.save(output_path, quality=85, optimize=True)\n\n            return {\n                "success": True,\n                "original_size": original_size,\n                "thumbnail_size": img.size,\n                "output_path": output_path\n            }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Converter PNG com transparência para JPEG",output:'from PIL import Image\n\ndef convert_png_to_jpeg(input_path: str, output_path: str):\n    """Converte PNG para JPEG tratando transparência"""\n    try:\n        with Image.open(input_path) as img:\n            # Verificar se tem transparência\n            if img.mode in ("RGBA", "LA", "P"):\n                # Criar fundo branco\n                background = Image.new("RGB", img.size, (255, 255, 255))\n\n                # Colar imagem sobre fundo\n                if img.mode == "P":\n                    img = img.convert("RGBA")\n\n                background.paste(img, mask=img.split()[-1] if img.mode in ("RGBA", "LA") else None)\n                img = background\n            elif img.mode != "RGB":\n                img = img.convert("RGB")\n\n            # Salvar como JPEG\n            img.save(output_path, "JPEG", quality=95, optimize=True)\n\n            return {\n                "success": True,\n                "output_path": output_path,\n                "format": "JPEG"\n            }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Adicionar watermark de texto",output:'from PIL import Image, ImageDraw, ImageFont\n\ndef add_text_watermark(input_path: str, output_path: str, text: str, position: str = "bottom-right"):\n    """Adiciona watermark de texto com transparência"""\n    try:\n        with Image.open(input_path) as img:\n            # Converter para RGBA\n            img = img.convert("RGBA")\n\n            # Criar layer transparente\n            watermark = Image.new("RGBA", img.size, (255, 255, 255, 0))\n            draw = ImageDraw.Draw(watermark)\n\n            # Usar fonte padrão\n            font = ImageFont.load_default()\n\n            # Calcular posição\n            bbox = draw.textbbox((0, 0), text, font=font)\n            text_width = bbox[2] - bbox[0]\n            text_height = bbox[3] - bbox[1]\n\n            if position == "bottom-right":\n                x = img.width - text_width - 20\n                y = img.height - text_height - 20\n            elif position == "center":\n                x = (img.width - text_width) // 2\n                y = (img.height - text_height) // 2\n            else:\n                x, y = 20, 20\n\n            # Desenhar com 50% opacidade\n            draw.text((x, y), text, fill=(255, 255, 255, 128), font=font)\n\n            # Combinar\n            watermarked = Image.alpha_composite(img, watermark)\n\n            # Salvar\n            if output_path.lower().endswith((\'.jpg\', \'.jpeg\')):\n                watermarked = watermarked.convert("RGB")\n\n            watermarked.save(output_path, quality=95, optimize=True)\n\n            return {\n                "success": True,\n                "output_path": output_path,\n                "watermark_position": position\n            }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Aplicar filtros (blur, sharpen, enhance contrast)",output:'from PIL import Image, ImageFilter, ImageEnhance\n\ndef apply_image_filters(input_path: str, output_path: str, filters: list):\n    """Aplica múltiplos filtros em sequência"""\n    try:\n        with Image.open(input_path) as img:\n            for filter_name in filters:\n                if filter_name == "blur":\n                    img = img.filter(ImageFilter.BLUR)\n                elif filter_name == "sharpen":\n                    img = img.filter(ImageFilter.SHARPEN)\n                elif filter_name == "edge_enhance":\n                    img = img.filter(ImageFilter.EDGE_ENHANCE)\n                elif filter_name == "grayscale":\n                    img = img.convert("L")\n                elif filter_name == "enhance_contrast":\n                    enhancer = ImageEnhance.Contrast(img)\n                    img = enhancer.enhance(1.5)\n                elif filter_name == "enhance_brightness":\n                    enhancer = ImageEnhance.Brightness(img)\n                    img = enhancer.enhance(1.2)\n\n            # Salvar\n            img.save(output_path, quality=95, optimize=True)\n\n            return {\n                "success": True,\n                "filters_applied": filters,\n                "output_path": output_path\n            }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'}],outputFormat:{type:"object",required:["success"],properties:{success:{type:"boolean",description:"Indica se a operação foi bem-sucedida"},original_size:{type:"array",description:"Dimensões originais [width, height]"},new_size:{type:"array",description:"Novas dimensões [width, height]"},output_path:{type:"string",description:"Caminho do arquivo salvo"},format:{type:"string",description:"Formato da imagem (JPEG, PNG, etc)"},size_bytes:{type:"number",description:"Tamanho do arquivo em bytes"},mode:{type:"string",description:"Modo de cor (RGB, RGBA, L, etc)"},error:{type:"string",description:"Mensagem de erro se success=false"}}}},tags:["image","imaging","pil","pillow","resize","crop","thumbnail","filter","convert","watermark","photo","picture"],keywords:["pillow","pil","image","imagem","resize","redimensionar","crop","cortar","rotate","girar","thumbnail","miniatura","convert","converter","filter","filtro","watermark","marca dagua","jpeg","png","webp","gif"],performance:{speed:7,memory:8,cpuIntensive:!1,gpuAccelerated:!1,scalability:7},scoring:{baseScore:.9,rules:[{condition:'keywords include ["resize", "redimensionar", "scale"]',adjustment:.08,description:"Excelente para resize de imagens"},{condition:'keywords include ["crop", "cortar", "recortar"]',adjustment:.08,description:"Perfeito para crop"},{condition:'keywords include ["thumbnail", "miniatura"]',adjustment:.08,description:"Ideal para criar thumbnails"},{condition:'keywords include ["convert", "converter", "formato"]',adjustment:.05,description:"Bom para conversão de formato"},{condition:'keywords include ["watermark", "marca"]',adjustment:.05,description:"Suporta watermarks"},{condition:'keywords include ["filter", "filtro", "blur", "sharpen"]',adjustment:.05,description:"Filtros básicos disponíveis"},{condition:'keywords include ["detect", "detection", "face", "object"]',adjustment:-.4,description:"Não é para computer vision"},{condition:'keywords include ["video", "vídeo", "mp4", "avi"]',adjustment:-.6,description:"Não processa vídeo"},{condition:"fileSize > 100MB",adjustment:-.25,description:"Performance ruim com imagens muito grandes"},{condition:'keywords include ["batch", "lote", "múltiplas"]',adjustment:-.1,description:"Performance moderada em batch"}]},config:{maxRetries:3,timeout:3e4,cacheable:!0,requiresAuth:!1,rateLimit:null},alternatives:[{name:"OpenCV",when:"Computer vision, detecção de objetos, processamento de vídeo",reason:"OpenCV é mais poderoso para tarefas avançadas de visão computacional"},{name:"pyvips",when:"Imagens muito grandes (>100MB), performance crítica",reason:"pyvips é otimizado para imagens gigantes e usa menos memória"},{name:"scikit-image",when:"Análise científica de imagens, algoritmos avançados",reason:"scikit-image tem algoritmos mais sofisticados para análise"},{name:"imageio",when:"I/O simplificado, GIFs animados",reason:"imageio tem API mais simples e suporta GIFs animados"}],documentation:{official:"https://pillow.readthedocs.io/",examples:"https://pillow.readthedocs.io/en/stable/handbook/tutorial.html",apiReference:"https://pillow.readthedocs.io/en/stable/reference/index.html"},commonIssues:[{issue:'Erro "cannot identify image file"',solution:"Arquivo corrompido ou formato não suportado. Use img.verify() para validar",code:'try:\n    img = Image.open(path)\n    img.verify()\n    img = Image.open(path)  # Reabrir após verify\nexcept Exception as e:\n    print(f"Arquivo inválido: {e}")'},{issue:"JPEG não aceita transparência",solution:"Converter RGBA para RGB com fundo antes de salvar",code:'if img.mode == "RGBA":\n    background = Image.new("RGB", img.size, (255, 255, 255))\n    background.paste(img, mask=img.split()[3])\n    img = background\nimg.save(path, "JPEG")'},{issue:"Qualidade ruim após salvar",solution:"Aumentar quality e usar optimize=True",code:"img.save(path, quality=95, optimize=True)"},{issue:"MemoryError com imagens grandes",solution:"Reduzir resolução primeiro com thumbnail()",code:"img.thumbnail((max_width, max_height), Image.Resampling.LANCZOS)\nimg.save(output_path)"}],bestPractices:["Sempre especifique Image.Resampling.LANCZOS para melhor qualidade","Use optimize=True ao salvar para reduzir tamanho do arquivo","Feche imagens explicitamente ou use context manager (with)","Verifique o modo da imagem antes de salvar em JPEG","Use thumbnail() para manter aspect ratio automaticamente","Adicione try-except para operações de arquivo","Especifique quality entre 85-95 para produção","Converta RGBA para RGB antes de salvar em JPEG"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},D={id:"opencv-004",name:"OpenCV",packageName:"opencv-python",version:"4.9.0",category:w.COMPUTER_VISION,subcategories:["face-detection","object-detection","video-processing","image-analysis","feature-detection","contour-detection","image-segmentation","motion-tracking","ocr","machine-learning"],description:"Biblioteca mais poderosa e completa para processamento de imagens e visão computacional, oferecendo mais de 2500 algoritmos otimizados para detecção, reconhecimento, rastreamento e análise.",purpose:"Realizar tarefas avançadas de visão computacional, processamento de vídeo, detecção de objetos e análise de imagens",useCases:["Detecção de faces e reconhecimento facial","Detecção e rastreamento de objetos","Processamento e análise de vídeo em tempo real","Análise de contornos e formas","Detecção de bordas e features","Segmentação de imagens","OCR e detecção de texto","Rastreamento de movimento","Análise de cores e histogramas","Calibração de câmera","Realidade aumentada","Machine learning para visão computacional"],complexity:T.ADVANCED,environment:S.PYTHON,dependencies:["numpy"],installCommand:"pip install opencv-python",promptSystem:{systemPrompt:"Você é um especialista em OpenCV (cv2), a biblioteca Python de visão computacional.\n\nAo trabalhar com OpenCV, você SEMPRE deve:\n- Usar cv2.imread() para ler imagens (retorna BGR, não RGB)\n- Converter BGR para RGB quando necessário (cv2.cvtColor)\n- Usar interpolação adequada em resize (INTER_LANCZOS4 para melhor qualidade)\n- Verificar se a imagem foi carregada (img is not None)\n- Liberar recursos após processar vídeo (cap.release(), out.release())\n- Usar cv2.destroyAllWindows() após mostrar janelas\n- Especificar codec correto ao salvar vídeos\n\nREGRAS DE USO:\n1. SEMPRE verifique se imagem/vídeo foi carregado corretamente\n2. SEMPRE converta BGR para RGB quando integrar com outras bibliotecas\n3. SEMPRE libere recursos de vídeo (release())\n4. NUNCA use cv2.imshow() em servidores sem display\n5. SEMPRE use try-except para operações de arquivo\n6. SEMPRE especifique interpolação em resize\n7. Use GPU acceleration quando disponível (cv2.cuda)\n\nQUANDO USAR OPENCV:\n✅ Detecção de objetos e faces\n✅ Processamento de vídeo\n✅ Análise de contornos e formas\n✅ Rastreamento de movimento\n✅ Computer vision avançada\n✅ OCR e detecção de texto\n✅ Performance crítica em processamento de imagem\n✅ Feature detection (SIFT, ORB, etc)\n✅ Machine learning para visão\n\nQUANDO NÃO USAR OPENCV:\n❌ Operações simples de imagem (use Pillow)\n❌ Apenas resize/crop básico (use Pillow)\n❌ Deep learning moderno (use PyTorch/TensorFlow)\n❌ Processamento de áudio (use pydub)\n❌ Manipulação de PDF (use reportlab)\n\nESTRUTURA DE RESPOSTA:\nSempre retorne um dicionário com:\n- success: boolean\n- output_path: caminho do arquivo salvo\n- dimensions: dimensões da imagem/vídeo\n- detections: objetos/faces detectados (se aplicável)\n- processing_time: tempo de processamento\n- metadata: informações adicionais",userPromptTemplate:"Tarefa: {task_description}\n\nEntrada:\n{input_description}\n\nParâmetros:\n{parameters}\n\nUse OpenCV (cv2) para processar seguindo as melhores práticas:\n1. Carregue a imagem/vídeo com cv2.imread() ou cv2.VideoCapture()\n2. Execute a operação solicitada\n3. Salve o resultado com cv2.imwrite() ou cv2.VideoWriter()\n4. Retorne informações detalhadas",examples:[{input:"Detectar faces em uma imagem",output:'import cv2\nimport numpy as np\n\ndef detect_faces(image_path: str, output_path: str):\n    """Detecta faces usando Haar Cascade"""\n    try:\n        # Carregar classificador\n        face_cascade = cv2.CascadeClassifier(\n            cv2.data.haarcascades + \'haarcascade_frontalface_default.xml\'\n        )\n\n        # Ler imagem\n        img = cv2.imread(image_path)\n        if img is None:\n            return {"success": False, "error": "Não foi possível carregar a imagem"}\n\n        # Converter para escala de cinza\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n        # Detectar faces\n        faces = face_cascade.detectMultiScale(\n            gray,\n            scaleFactor=1.1,\n            minNeighbors=5,\n            minSize=(30, 30)\n        )\n\n        # Desenhar retângulos\n        for (x, y, w, h) in faces:\n            cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n\n        # Salvar\n        cv2.imwrite(output_path, img)\n\n        return {\n            "success": True,\n            "faces_detected": len(faces),\n            "output_path": output_path,\n            "coordinates": faces.tolist()\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Processar vídeo aplicando filtro de detecção de bordas",output:'import cv2\n\ndef process_video_edges(input_path: str, output_path: str):\n    """Processa vídeo aplicando detecção de bordas Canny"""\n    try:\n        # Abrir vídeo\n        cap = cv2.VideoCapture(input_path)\n        if not cap.isOpened():\n            return {"success": False, "error": "Não foi possível abrir o vídeo"}\n\n        # Propriedades\n        fps = int(cap.get(cv2.CAP_PROP_FPS))\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        # Writer\n        fourcc = cv2.VideoWriter_fourcc(*\'mp4v\')\n        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n        frames_processed = 0\n\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            # Converter para cinza\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n            # Detectar bordas\n            edges = cv2.Canny(gray, 100, 200)\n\n            # Converter de volta para BGR\n            edges_bgr = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n\n            # Escrever frame\n            out.write(edges_bgr)\n            frames_processed += 1\n\n        # Liberar recursos\n        cap.release()\n        out.release()\n\n        return {\n            "success": True,\n            "output_path": output_path,\n            "frames_processed": frames_processed,\n            "total_frames": total_frames,\n            "fps": fps,\n            "dimensions": (width, height)\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Detectar e extrair contornos de objetos",output:'import cv2\nimport numpy as np\n\ndef detect_contours(image_path: str, output_path: str, min_area: int = 500):\n    """Detecta e desenha contornos de objetos"""\n    try:\n        # Ler imagem\n        img = cv2.imread(image_path)\n        if img is None:\n            return {"success": False, "error": "Imagem não encontrada"}\n\n        # Converter para escala de cinza\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n        # Aplicar blur para reduzir ruído\n        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # Threshold binário\n        _, thresh = cv2.threshold(blurred, 127, 255, cv2.THRESH_BINARY)\n\n        # Encontrar contornos\n        contours, hierarchy = cv2.findContours(\n            thresh,\n            cv2.RETR_EXTERNAL,\n            cv2.CHAIN_APPROX_SIMPLE\n        )\n\n        # Filtrar por área\n        valid_contours = [c for c in contours if cv2.contourArea(c) > min_area]\n\n        # Desenhar contornos\n        result = img.copy()\n        cv2.drawContours(result, valid_contours, -1, (0, 255, 0), 2)\n\n        # Adicionar informações\n        contour_data = []\n        for i, cnt in enumerate(valid_contours):\n            area = cv2.contourArea(cnt)\n            perimeter = cv2.arcLength(cnt, True)\n            x, y, w, h = cv2.boundingRect(cnt)\n\n            contour_data.append({\n                "id": i,\n                "area": float(area),\n                "perimeter": float(perimeter),\n                "bounding_box": [int(x), int(y), int(w), int(h)]\n            })\n\n        # Salvar\n        cv2.imwrite(output_path, result)\n\n        return {\n            "success": True,\n            "output_path": output_path,\n            "contours_found": len(valid_contours),\n            "contour_data": contour_data\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Rastrear objeto em vídeo usando tracking",output:'import cv2\n\ndef track_object_in_video(video_path: str, output_path: str, bbox: tuple):\n    """Rastreia objeto em vídeo usando tracker CSRT"""\n    try:\n        # Abrir vídeo\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            return {"success": False, "error": "Erro ao abrir vídeo"}\n\n        # Ler primeiro frame\n        ret, frame = cap.read()\n        if not ret:\n            return {"success": False, "error": "Erro ao ler primeiro frame"}\n\n        # Inicializar tracker (CSRT é mais preciso)\n        tracker = cv2.TrackerCSRT_create()\n        tracker.init(frame, bbox)\n\n        # Propriedades do vídeo\n        fps = int(cap.get(cv2.CAP_PROP_FPS))\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n        # Writer\n        fourcc = cv2.VideoWriter_fourcc(*\'mp4v\')\n        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n        frames_tracked = 0\n        tracking_lost = False\n\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            # Atualizar tracker\n            success, bbox = tracker.update(frame)\n\n            if success:\n                # Desenhar bounding box\n                x, y, w, h = [int(v) for v in bbox]\n                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n                frames_tracked += 1\n            else:\n                tracking_lost = True\n                cv2.putText(frame, "Tracking Lost", (10, 50),\n                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n\n            out.write(frame)\n\n        # Liberar recursos\n        cap.release()\n        out.release()\n\n        return {\n            "success": True,\n            "output_path": output_path,\n            "frames_tracked": frames_tracked,\n            "tracking_lost": tracking_lost\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Aplicar filtros e efeitos de imagem",output:'import cv2\nimport numpy as np\n\ndef apply_image_effects(image_path: str, output_path: str, effects: list):\n    """Aplica múltiplos efeitos na imagem"""\n    try:\n        img = cv2.imread(image_path)\n        if img is None:\n            return {"success": False, "error": "Imagem não encontrada"}\n\n        result = img.copy()\n\n        for effect in effects:\n            if effect == "blur":\n                result = cv2.GaussianBlur(result, (15, 15), 0)\n\n            elif effect == "sharpen":\n                kernel = np.array([[-1,-1,-1],\n                                  [-1, 9,-1],\n                                  [-1,-1,-1]])\n                result = cv2.filter2D(result, -1, kernel)\n\n            elif effect == "edge_detection":\n                gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n                edges = cv2.Canny(gray, 100, 200)\n                result = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n\n            elif effect == "grayscale":\n                result = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n                result = cv2.cvtColor(result, cv2.COLOR_GRAY2BGR)\n\n            elif effect == "sepia":\n                kernel = np.array([[0.272, 0.534, 0.131],\n                                  [0.349, 0.686, 0.168],\n                                  [0.393, 0.769, 0.189]])\n                result = cv2.transform(result, kernel)\n\n            elif effect == "negative":\n                result = cv2.bitwise_not(result)\n\n        cv2.imwrite(output_path, result)\n\n        return {\n            "success": True,\n            "output_path": output_path,\n            "effects_applied": effects,\n            "dimensions": result.shape[:2]\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'}],outputFormat:{type:"object",required:["success"],properties:{success:{type:"boolean",description:"Indica se a operação foi bem-sucedida"},output_path:{type:"string",description:"Caminho do arquivo processado"},dimensions:{type:"array",description:"Dimensões [width, height]"},detections:{type:"array",description:"Objetos/faces detectados"},frames_processed:{type:"number",description:"Número de frames processados (vídeo)"},fps:{type:"number",description:"Frames por segundo (vídeo)"},processing_time:{type:"number",description:"Tempo de processamento em ms"},error:{type:"string",description:"Mensagem de erro se success=false"}}}},tags:["opencv","cv2","computer-vision","face-detection","object-detection","video-processing","image-analysis","tracking","ocr","contours"],keywords:["opencv","cv2","computer vision","visao computacional","face detection","deteccao de face","object detection","deteccao de objeto","video","tracking","rastreamento","contour","contorno","edge detection","deteccao de borda","feature detection","ocr","motion","movimento"],performance:{speed:9,memory:8,cpuIntensive:!0,gpuAccelerated:!0,scalability:9},scoring:{baseScore:.85,rules:[{condition:'keywords include ["face", "facial", "reconhecimento"]',adjustment:.1,description:"Excelente para detecção facial"},{condition:'keywords include ["object", "detection", "detect"]',adjustment:.1,description:"Perfeito para detecção de objetos"},{condition:'keywords include ["video", "vídeo", "mp4", "avi"]',adjustment:.1,description:"Ideal para processamento de vídeo"},{condition:'keywords include ["tracking", "rastreamento", "motion"]',adjustment:.1,description:"Excelente para tracking"},{condition:'keywords include ["contour", "contorno", "shape"]',adjustment:.08,description:"Ótimo para análise de contornos"},{condition:'keywords include ["edge", "borda", "canny"]',adjustment:.08,description:"Poderoso para detecção de bordas"},{condition:'keywords include ["ocr", "text", "texto"]',adjustment:.05,description:"Suporta OCR básico"},{condition:'keywords include ["resize", "crop"] AND NOT include ["face", "object", "video"]',adjustment:-.2,description:"Operações simples são melhor com Pillow"},{condition:'keywords include ["deep learning", "neural network", "cnn"]',adjustment:-.3,description:"Deep learning moderno melhor com PyTorch/TensorFlow"}]},config:{maxRetries:2,timeout:12e4,cacheable:!1,requiresAuth:!1,rateLimit:null},alternatives:[{name:"Pillow",when:"Operações básicas de imagem (resize, crop, filtros simples)",reason:"Pillow é mais simples e leve para operações básicas"},{name:"PyTorch/TensorFlow",when:"Deep learning, redes neurais modernas",reason:"Frameworks modernos são melhores para DL"},{name:"MediaPipe",when:"ML em tempo real (pose, hands, face mesh)",reason:"MediaPipe é otimizado para ML em tempo real"},{name:"scikit-image",when:"Análise científica de imagens",reason:"scikit-image tem algoritmos mais sofisticados"}],documentation:{official:"https://docs.opencv.org/",examples:"https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html",apiReference:"https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html"},commonIssues:[{issue:"Imagem carregada como None",solution:"Verificar se o path está correto e o formato é suportado",code:'img = cv2.imread(path)\nif img is None:\n    print("Erro ao carregar imagem")\n    return None'},{issue:"Cores erradas (BGR vs RGB)",solution:"OpenCV usa BGR por padrão, converter se necessário",code:"img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)"},{issue:"Vídeo não salva corretamente",solution:"Usar codec correto e liberar recursos",code:"fourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(path, fourcc, fps, (w, h))\n# ... processar\nout.release()"},{issue:"Erro ao mostrar janela em servidor",solution:"Não usar cv2.imshow() em ambiente sem display",code:"# Ao invés de cv2.imshow(), salvar arquivo\ncv2.imwrite(output_path, img)"}],bestPractices:["Sempre verifique se imagem/vídeo foi carregado (is not None)","Converta BGR para RGB ao integrar com outras bibliotecas","Libere recursos de vídeo com cap.release() e out.release()","Use interpolação INTER_LANCZOS4 para melhor qualidade em resize","Não use cv2.imshow() em servidores sem display","Especifique codec correto ao salvar vídeos (mp4v, xvid, etc)","Use GPU acceleration quando disponível (cv2.cuda)","Aplique blur antes de threshold para reduzir ruído"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},M={id:"requests-005",name:"Requests",packageName:"requests",version:"2.31.0",category:w.WEB_SCRAPING,subcategories:["http-client","api-integration","rest-api","web-requests","authentication","json-handling","file-download","session-management"],description:"Biblioteca Python mais popular para fazer requisições HTTP de forma simples e elegante. Oferece API intuitiva para consumir APIs REST, fazer downloads, autenticação e gerenciar sessões.",purpose:"Fazer requisições HTTP/HTTPS para APIs, websites e serviços web de forma simples e eficiente",useCases:["Consumir APIs REST (GET, POST, PUT, DELETE, PATCH)","Autenticação em APIs (Basic, Bearer Token, OAuth)","Download de arquivos e imagens","Upload de arquivos multipart/form-data","Gerenciar sessões e cookies","Trabalhar com JSON automaticamente","Configurar headers customizados","Timeout e retry de requisições","Proxy e certificados SSL","Streaming de grandes arquivos"],complexity:T.BASIC,environment:S.PYTHON,dependencies:["urllib3","certifi","charset-normalizer"],installCommand:"pip install requests",promptSystem:{systemPrompt:"Você é um especialista em Requests, a biblioteca Python para requisições HTTP.\n\nAo trabalhar com Requests, você SEMPRE deve:\n- Usar try-except para capturar erros de rede\n- Verificar response.status_code antes de processar\n- Usar response.json() para APIs que retornam JSON\n- Adicionar timeout em todas as requisições (evitar hang infinito)\n- Usar sessions para múltiplas requisições ao mesmo servidor\n- Verificar response.raise_for_status() para detectar erros HTTP\n- Usar headers adequados (Content-Type, Authorization, User-Agent)\n\nREGRAS DE USO:\n1. SEMPRE adicione timeout nas requisições (ex: timeout=10)\n2. SEMPRE use try-except para requests.RequestException\n3. SEMPRE verifique response.status_code ou use raise_for_status()\n4. NUNCA ignore erros SSL sem necessidade (verify=False apenas quando necessário)\n5. SEMPRE use response.json() para APIs JSON (não json.loads(response.text))\n6. SEMPRE feche sessões explicitamente ou use context manager\n7. Use sessions para múltiplas requisições (reutiliza conexões)\n\nQUANDO USAR REQUESTS:\n✅ Consumir APIs REST\n✅ Fazer requisições HTTP simples\n✅ Download de arquivos\n✅ Autenticação em APIs\n✅ Trabalhar com JSON\n✅ Gerenciar sessões e cookies\n✅ Requisições síncronas\n\nQUANDO NÃO USAR REQUESTS:\n❌ Requisições assíncronas em massa (use httpx ou aiohttp)\n❌ Web scraping complexo com JavaScript (use Selenium ou Playwright)\n❌ Parsing de HTML (combine com BeautifulSoup)\n❌ Performance crítica com milhares de requisições simultâneas\n❌ Streaming de vídeo em tempo real\n\nESTRUTURA DE RESPOSTA:\nSempre retorne um dicionário com:\n- success: boolean\n- status_code: código HTTP\n- data: dados da resposta (JSON ou texto)\n- headers: headers da resposta\n- error: mensagem de erro (se houver)",userPromptTemplate:"Tarefa: {task_description}\n\nEntrada:\n{input_description}\n\nParâmetros:\n{parameters}\n\nUse Requests para fazer a requisição HTTP seguindo as melhores práticas:\n1. Configure headers e autenticação se necessário\n2. Execute a requisição com timeout\n3. Verifique status code\n4. Processe a resposta (JSON, texto ou arquivo)\n5. Trate erros adequadamente",examples:[{input:"Fazer requisição GET simples para uma API",output:'import requests\nfrom typing import Dict, Any\n\ndef get_api_data(url: str, params: Dict = None, headers: Dict = None) -> Dict[str, Any]:\n    """Faz requisição GET para uma API"""\n    try:\n        # Fazer requisição com timeout\n        response = requests.get(\n            url,\n            params=params,\n            headers=headers,\n            timeout=10\n        )\n\n        # Verificar status\n        response.raise_for_status()\n\n        # Retornar JSON\n        return {\n            "success": True,\n            "status_code": response.status_code,\n            "data": response.json(),\n            "headers": dict(response.headers)\n        }\n    except requests.exceptions.Timeout:\n        return {"success": False, "error": "Timeout na requisição"}\n    except requests.exceptions.HTTPError as e:\n        return {"success": False, "error": f"Erro HTTP: {e}"}\n    except requests.exceptions.RequestException as e:\n        return {"success": False, "error": f"Erro na requisição: {e}"}'},{input:"Fazer POST com JSON e autenticação Bearer",output:'import requests\nfrom typing import Dict, Any\n\ndef post_api_data(url: str, data: Dict, token: str) -> Dict[str, Any]:\n    """Faz requisição POST com autenticação Bearer"""\n    try:\n        headers = {\n            "Content-Type": "application/json",\n            "Authorization": f"Bearer {token}",\n            "User-Agent": "SyncAds/1.0"\n        }\n\n        response = requests.post(\n            url,\n            json=data,  # Automaticamente serializa para JSON\n            headers=headers,\n            timeout=15\n        )\n\n        response.raise_for_status()\n\n        return {\n            "success": True,\n            "status_code": response.status_code,\n            "data": response.json(),\n            "message": "Dados enviados com sucesso"\n        }\n    except requests.exceptions.HTTPError as e:\n        return {\n            "success": False,\n            "status_code": e.response.status_code if e.response else None,\n            "error": f"Erro HTTP {e.response.status_code}: {e.response.text if e.response else str(e)}"\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Download de arquivo com progress",output:'import requests\nfrom pathlib import Path\n\ndef download_file(url: str, output_path: str, chunk_size: int = 8192) -> Dict[str, Any]:\n    """Download de arquivo com streaming"""\n    try:\n        response = requests.get(url, stream=True, timeout=30)\n        response.raise_for_status()\n\n        total_size = int(response.headers.get(\'content-length\', 0))\n        downloaded = 0\n\n        with open(output_path, \'wb\') as f:\n            for chunk in response.iter_content(chunk_size=chunk_size):\n                if chunk:\n                    f.write(chunk)\n                    downloaded += len(chunk)\n\n        return {\n            "success": True,\n            "output_path": output_path,\n            "size_bytes": downloaded,\n            "size_mb": round(downloaded / (1024 * 1024), 2),\n            "url": url\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Usar Session para múltiplas requisições",output:'import requests\nfrom typing import List, Dict, Any\n\ndef fetch_multiple_endpoints(base_url: str, endpoints: List[str], token: str) -> Dict[str, Any]:\n    """Faz múltiplas requisições usando Session (reutiliza conexão)"""\n    results = []\n\n    try:\n        # Criar sessão (reutiliza conexões TCP)\n        with requests.Session() as session:\n            # Configurar headers padrão\n            session.headers.update({\n                "Authorization": f"Bearer {token}",\n                "Content-Type": "application/json"\n            })\n\n            for endpoint in endpoints:\n                url = f"{base_url}{endpoint}"\n\n                try:\n                    response = session.get(url, timeout=10)\n                    response.raise_for_status()\n\n                    results.append({\n                        "endpoint": endpoint,\n                        "success": True,\n                        "data": response.json()\n                    })\n                except Exception as e:\n                    results.append({\n                        "endpoint": endpoint,\n                        "success": False,\n                        "error": str(e)\n                    })\n\n        return {\n            "success": True,\n            "total_requests": len(endpoints),\n            "successful": sum(1 for r in results if r["success"]),\n            "results": results\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Upload de arquivo multipart/form-data",output:'import requests\nfrom pathlib import Path\n\ndef upload_file(url: str, file_path: str, field_name: str = "file", additional_data: Dict = None) -> Dict[str, Any]:\n    """Upload de arquivo via multipart/form-data"""\n    try:\n        # Abrir arquivo\n        with open(file_path, \'rb\') as f:\n            files = {field_name: (Path(file_path).name, f, \'application/octet-stream\')}\n\n            # Dados adicionais (form fields)\n            data = additional_data or {}\n\n            response = requests.post(\n                url,\n                files=files,\n                data=data,\n                timeout=60\n            )\n\n            response.raise_for_status()\n\n            return {\n                "success": True,\n                "status_code": response.status_code,\n                "response": response.json() if response.headers.get(\'content-type\', \'\').startswith(\'application/json\') else response.text,\n                "file_uploaded": file_path\n            }\n    except FileNotFoundError:\n        return {"success": False, "error": f"Arquivo não encontrado: {file_path}"}\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Requisição com retry automático",output:'import requests\nfrom requests.adapters import HTTPAdapter\nfrom requests.packages.urllib3.util.retry import Retry\n\ndef get_with_retry(url: str, max_retries: int = 3) -> Dict[str, Any]:\n    """Faz requisição GET com retry automático"""\n    try:\n        # Configurar retry strategy\n        retry_strategy = Retry(\n            total=max_retries,\n            backoff_factor=1,  # 1s, 2s, 4s\n            status_forcelist=[429, 500, 502, 503, 504],\n            allowed_methods=["GET", "POST"]\n        )\n\n        adapter = HTTPAdapter(max_retries=retry_strategy)\n\n        with requests.Session() as session:\n            session.mount("http://", adapter)\n            session.mount("https://", adapter)\n\n            response = session.get(url, timeout=10)\n            response.raise_for_status()\n\n            return {\n                "success": True,\n                "status_code": response.status_code,\n                "data": response.json() if \'application/json\' in response.headers.get(\'content-type\', \'\') else response.text,\n                "retries_used": max_retries - retry_strategy.total\n            }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'}],outputFormat:{type:"object",required:["success"],properties:{success:{type:"boolean",description:"Indica se a requisição foi bem-sucedida"},status_code:{type:"number",description:"Código HTTP da resposta"},data:{type:"any",description:"Dados da resposta (JSON ou texto)"},headers:{type:"object",description:"Headers da resposta"},error:{type:"string",description:"Mensagem de erro se success=false"},response_time:{type:"number",description:"Tempo de resposta em ms"}}}},tags:["http","requests","api","rest","client","web","download","upload","json","session"],keywords:["requests","http","https","api","rest","get","post","put","delete","patch","download","upload","json","session","authentication","bearer","token","header","cookie"],performance:{speed:8,memory:9,cpuIntensive:!1,gpuAccelerated:!1,scalability:7},scoring:{baseScore:.95,rules:[{condition:'keywords include ["api", "rest", "endpoint"]',adjustment:.05,description:"Perfeito para consumir APIs REST"},{condition:'keywords include ["get", "post", "put", "delete"]',adjustment:.03,description:"Ideal para requisições HTTP"},{condition:'keywords include ["download", "baixar", "arquivo"]',adjustment:.03,description:"Ótimo para download de arquivos"},{condition:'keywords include ["json", "data"]',adjustment:.02,description:"Excelente suporte a JSON"},{condition:'keywords include ["auth", "authentication", "token", "bearer"]',adjustment:.03,description:"Suporta vários tipos de autenticação"},{condition:'keywords include ["session", "cookie", "cookies"]',adjustment:.03,description:"Gerenciamento de sessões integrado"},{condition:'keywords include ["async", "asyncio", "concurrent"]',adjustment:-.4,description:"Não é assíncrono, use httpx ou aiohttp"},{condition:'keywords include ["javascript", "render", "dynamic"]',adjustment:-.5,description:"Não renderiza JavaScript, use Selenium/Playwright"},{condition:'keywords include ["websocket", "ws", "streaming"]',adjustment:-.4,description:"Não suporta WebSockets nativamente"}]},config:{maxRetries:3,timeout:1e4,cacheable:!1,requiresAuth:!1,rateLimit:null},alternatives:[{name:"httpx",when:"Requisições assíncronas, HTTP/2, suporte moderno",reason:"httpx é assíncrono e suporta HTTP/2"},{name:"aiohttp",when:"Async/await, milhares de requisições simultâneas",reason:"aiohttp é totalmente assíncrono e mais rápido em massa"},{name:"urllib",when:"Biblioteca padrão, sem dependências externas",reason:"urllib vem com Python mas é menos conveniente"},{name:"Selenium/Playwright",when:"Sites com JavaScript, renderização de browser",reason:"Requisições simples não executam JavaScript"}],documentation:{official:"https://requests.readthedocs.io/",examples:"https://requests.readthedocs.io/en/latest/user/quickstart/",apiReference:"https://requests.readthedocs.io/en/latest/api/"},commonIssues:[{issue:"ConnectionError ou timeout",solution:"Aumentar timeout ou verificar conectividade",code:'try:\n    response = requests.get(url, timeout=30)\nexcept requests.exceptions.Timeout:\n    print("Timeout - servidor demorou muito")\nexcept requests.exceptions.ConnectionError:\n    print("Erro de conexão - verificar internet")'},{issue:"SSL Certificate verification failed",solution:"Atualizar certifi ou desabilitar verificação (não recomendado)",code:"# Opção 1: Atualizar certificados\n# pip install --upgrade certifi\n\n# Opção 2: Desabilitar (APENAS para desenvolvimento)\nresponse = requests.get(url, verify=False)"},{issue:"JSON decode error",solution:"Verificar se resposta é realmente JSON",code:"if 'application/json' in response.headers.get('content-type', ''):\n    data = response.json()\nelse:\n    data = response.text"},{issue:"Erro 401 Unauthorized",solution:"Verificar autenticação e headers",code:'headers = {\n    "Authorization": f"Bearer {token}",\n    "Content-Type": "application/json"\n}\nresponse = requests.get(url, headers=headers)'}],bestPractices:["Sempre adicione timeout para evitar hang infinito","Use try-except para capturar erros de rede","Verifique response.status_code ou use raise_for_status()","Use response.json() ao invés de json.loads(response.text)","Use Session para múltiplas requisições ao mesmo servidor","Configure User-Agent adequado","Use stream=True para arquivos grandes","Feche sessões explicitamente ou use context manager"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},L={id:"beautifulsoup-006",name:"BeautifulSoup4",packageName:"beautifulsoup4",version:"4.12.0",category:w.WEB_SCRAPING,subcategories:["html-parsing","xml-parsing","web-scraping","data-extraction","dom-navigation","content-extraction","link-extraction","table-parsing"],description:"Biblioteca Python mais popular para fazer parsing de HTML e XML, extraindo dados de páginas web de forma simples e intuitiva. Oferece métodos elegantes para navegar, buscar e modificar a árvore de parse.",purpose:"Fazer parsing de HTML/XML e extrair dados estruturados de páginas web",useCases:["Extrair dados de páginas web (web scraping)","Fazer parsing de HTML e XML","Buscar elementos por tag, classe, ID, atributos","Extrair texto limpo de HTML","Coletar links e URLs de páginas","Extrair dados de tabelas HTML","Navegar na árvore DOM","Limpar e processar HTML","Extrair metadados de páginas","Coletar imagens e mídias"],complexity:T.BASIC,environment:S.PYTHON,dependencies:["lxml","html5lib"],installCommand:"pip install beautifulsoup4 lxml",promptSystem:{systemPrompt:"Você é um especialista em BeautifulSoup4, a biblioteca Python para parsing de HTML/XML.\n\nAo trabalhar com BeautifulSoup, você SEMPRE deve:\n- Especificar o parser (lxml é mais rápido, html5lib é mais tolerante)\n- Usar find() para um elemento, find_all() para múltiplos\n- Verificar se elemento existe antes de acessar (pode retornar None)\n- Usar .get_text() ou .text para extrair texto\n- Usar .get('attr') para acessar atributos com segurança\n- Combinar com requests para buscar páginas\n- Usar CSS selectors com .select() quando conveniente\n\nREGRAS DE USO:\n1. SEMPRE especifique o parser: BeautifulSoup(html, 'lxml')\n2. SEMPRE verifique se elemento existe antes de usar (if element:)\n3. SEMPRE use .get('href') ao invés de ['href'] para evitar KeyError\n4. NUNCA assuma que elemento foi encontrado (pode ser None)\n5. SEMPRE remova espaços em branco com .strip() ao extrair texto\n6. Use find() para primeiro elemento, find_all() para lista\n7. Prefira CSS selectors (.select()) quando a busca for complexa\n\nQUANDO USAR BEAUTIFULSOUP:\n✅ Parsing de HTML/XML estático\n✅ Extrair dados de páginas web simples\n✅ Buscar elementos por tag, classe, ID\n✅ Extrair texto, links, imagens\n✅ Parsing de tabelas HTML\n✅ Limpar e processar HTML\n✅ Sites sem JavaScript pesado\n\nQUANDO NÃO USAR BEAUTIFULSOUP:\n❌ Sites com JavaScript dinâmico (use Selenium/Playwright)\n❌ APIs REST disponíveis (use requests diretamente)\n❌ Performance crítica em milhões de páginas (use scrapy)\n❌ Scraping em larga escala com concorrência (use scrapy)\n❌ Sites com proteção anti-scraping complexa\n❌ Conteúdo carregado por AJAX (use Selenium/Playwright)\n\nESTRUTURA DE RESPOSTA:\nSempre retorne um dicionário com:\n- success: boolean\n- data: dados extraídos\n- elements_found: número de elementos encontrados\n- url: URL da página (se aplicável)\n- error: mensagem de erro (se houver)",userPromptTemplate:"Tarefa: {task_description}\n\nEntrada:\n{input_description}\n\nParâmetros:\n{parameters}\n\nUse BeautifulSoup4 para fazer parsing e extrair dados seguindo as melhores práticas:\n1. Parse o HTML com parser adequado\n2. Busque os elementos desejados\n3. Extraia os dados com segurança\n4. Retorne dados estruturados",examples:[{input:"Extrair todos os links de uma página",output:'import requests\nfrom bs4 import BeautifulSoup\nfrom typing import List, Dict, Any\nfrom urllib.parse import urljoin\n\ndef extract_links(url: str) -> Dict[str, Any]:\n    """Extrai todos os links de uma página"""\n    try:\n        # Buscar página\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n\n        # Parse HTML\n        soup = BeautifulSoup(response.content, \'lxml\')\n\n        # Encontrar todos os links\n        links = []\n        for a_tag in soup.find_all(\'a\', href=True):\n            href = a_tag.get(\'href\')\n            text = a_tag.get_text(strip=True)\n\n            # Converter para URL absoluta\n            absolute_url = urljoin(url, href)\n\n            links.append({\n                "url": absolute_url,\n                "text": text,\n                "title": a_tag.get(\'title\', \'\')\n            })\n\n        return {\n            "success": True,\n            "url": url,\n            "links_found": len(links),\n            "links": links\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Extrair dados de uma tabela HTML",output:"import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef extract_table_data(url: str, table_index: int = 0) -> Dict[str, Any]:\n    \"\"\"Extrai dados de tabela HTML e converte para DataFrame\"\"\"\n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n\n        soup = BeautifulSoup(response.content, 'lxml')\n\n        # Encontrar todas as tabelas\n        tables = soup.find_all('table')\n\n        if not tables or table_index >= len(tables):\n            return {\"success\": False, \"error\": f\"Tabela {table_index} não encontrada\"}\n\n        table = tables[table_index]\n\n        # Extrair headers\n        headers = []\n        header_row = table.find('thead')\n        if header_row:\n            headers = [th.get_text(strip=True) for th in header_row.find_all(['th', 'td'])]\n        else:\n            # Tentar primeira linha como header\n            first_row = table.find('tr')\n            if first_row:\n                headers = [th.get_text(strip=True) for th in first_row.find_all(['th', 'td'])]\n\n        # Extrair linhas\n        rows = []\n        tbody = table.find('tbody') or table\n        for tr in tbody.find_all('tr')[1 if not table.find('thead') else 0:]:\n            cells = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]\n            if cells:  # Ignorar linhas vazias\n                rows.append(cells)\n\n        # Criar DataFrame\n        df = pd.DataFrame(rows, columns=headers if headers else None)\n\n        return {\n            \"success\": True,\n            \"url\": url,\n            \"rows\": len(df),\n            \"columns\": len(df.columns),\n            \"headers\": headers,\n            \"data\": df.to_dict('records')\n        }\n    except Exception as e:\n        return {\"success\": False, \"error\": str(e)}"},{input:"Extrair artigos de um blog com título, autor e data",output:"import requests\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\n\ndef scrape_blog_articles(url: str) -> Dict[str, Any]:\n    \"\"\"Extrai artigos de blog com metadados\"\"\"\n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n\n        soup = BeautifulSoup(response.content, 'lxml')\n\n        articles = []\n\n        # Buscar artigos (ajustar seletores conforme estrutura do site)\n        article_elements = soup.select('article') or soup.find_all('div', class_='post')\n\n        for article in article_elements:\n            # Extrair título\n            title_elem = article.find(['h1', 'h2', 'h3'], class_=['title', 'post-title', 'entry-title'])\n            title = title_elem.get_text(strip=True) if title_elem else \"Sem título\"\n\n            # Extrair link\n            link_elem = article.find('a', href=True)\n            link = link_elem.get('href') if link_elem else \"\"\n            if link and not link.startswith('http'):\n                from urllib.parse import urljoin\n                link = urljoin(url, link)\n\n            # Extrair autor\n            author_elem = article.find(['span', 'div', 'a'], class_=['author', 'by-author', 'post-author'])\n            author = author_elem.get_text(strip=True) if author_elem else \"Desconhecido\"\n\n            # Extrair data\n            date_elem = article.find(['time', 'span', 'div'], class_=['date', 'post-date', 'published'])\n            date = date_elem.get_text(strip=True) if date_elem else \"\"\n\n            # Extrair resumo/excerpt\n            excerpt_elem = article.find(['p', 'div'], class_=['excerpt', 'summary', 'description'])\n            excerpt = excerpt_elem.get_text(strip=True) if excerpt_elem else \"\"\n\n            articles.append({\n                \"title\": title,\n                \"link\": link,\n                \"author\": author,\n                \"date\": date,\n                \"excerpt\": excerpt[:200]  # Limitar a 200 caracteres\n            })\n\n        return {\n            \"success\": True,\n            \"url\": url,\n            \"articles_found\": len(articles),\n            \"articles\": articles\n        }\n    except Exception as e:\n        return {\"success\": False, \"error\": str(e)}"},{input:"Extrair todas as imagens de uma página",output:'import requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\ndef extract_images(url: str) -> Dict[str, Any]:\n    """Extrai todas as imagens de uma página"""\n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n\n        soup = BeautifulSoup(response.content, \'lxml\')\n\n        images = []\n\n        # Encontrar todas as tags img\n        for img in soup.find_all(\'img\'):\n            src = img.get(\'src\') or img.get(\'data-src\')  # Lazy loading\n            if not src:\n                continue\n\n            # Converter para URL absoluta\n            absolute_url = urljoin(url, src)\n\n            images.append({\n                "url": absolute_url,\n                "alt": img.get(\'alt\', \'\'),\n                "title": img.get(\'title\', \'\'),\n                "width": img.get(\'width\', \'\'),\n                "height": img.get(\'height\', \'\')\n            })\n\n        return {\n            "success": True,\n            "url": url,\n            "images_found": len(images),\n            "images": images\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Buscar elementos com CSS selectors complexos",output:'import requests\nfrom bs4 import BeautifulSoup\n\ndef extract_with_css_selectors(url: str) -> Dict[str, Any]:\n    """Usa CSS selectors para extrair dados específicos"""\n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n\n        soup = BeautifulSoup(response.content, \'lxml\')\n\n        # CSS Selectors avançados\n        data = {\n            # Elementos com classe específica\n            "titles": [elem.get_text(strip=True) for elem in soup.select(\'h2.product-title\')],\n\n            # Elementos dentro de outros (descendentes)\n            "prices": [elem.get_text(strip=True) for elem in soup.select(\'div.product span.price\')],\n\n            # Elementos com múltiplas classes\n            "featured": [elem.get_text(strip=True) for elem in soup.select(\'div.product.featured\')],\n\n            # Elementos com atributo específico\n            "data_items": [elem.get(\'data-id\') for elem in soup.select(\'[data-id]\')],\n\n            # Pseudo-seletores\n            "first_paragraph": soup.select_one(\'article p:first-of-type\').get_text(strip=True) if soup.select_one(\'article p:first-of-type\') else "",\n\n            # Combinadores\n            "sibling_headers": [elem.get_text(strip=True) for elem in soup.select(\'h2 + p\')],\n\n            # ID + classe\n            "specific_element": soup.select_one(\'#main .content .article\').get_text(strip=True) if soup.select_one(\'#main .content .article\') else ""\n        }\n\n        return {\n            "success": True,\n            "url": url,\n            "data": data\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Limpar e extrair texto puro de HTML",output:'import requests\nfrom bs4 import BeautifulSoup\nimport re\n\ndef extract_clean_text(url: str) -> Dict[str, Any]:\n    """Extrai texto limpo removendo scripts, styles e tags"""\n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n\n        soup = BeautifulSoup(response.content, \'lxml\')\n\n        # Remover scripts e styles\n        for script in soup([\'script\', \'style\', \'nav\', \'footer\', \'header\']):\n            script.decompose()\n\n        # Extrair texto\n        text = soup.get_text()\n\n        # Limpar espaços em branco excessivos\n        lines = (line.strip() for line in text.splitlines())\n        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))\n        text = \' \'.join(chunk for chunk in chunks if chunk)\n\n        # Extrair parágrafos\n        paragraphs = [p.get_text(strip=True) for p in soup.find_all(\'p\') if p.get_text(strip=True)]\n\n        # Extrair títulos\n        headings = {\n            f"h{i}": [h.get_text(strip=True) for h in soup.find_all(f\'h{i}\')]\n            for i in range(1, 7)\n        }\n\n        return {\n            "success": True,\n            "url": url,\n            "text_length": len(text),\n            "text": text,\n            "paragraphs_count": len(paragraphs),\n            "paragraphs": paragraphs,\n            "headings": headings\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'}],outputFormat:{type:"object",required:["success"],properties:{success:{type:"boolean",description:"Indica se o parsing foi bem-sucedido"},url:{type:"string",description:"URL da página parseada"},data:{type:"any",description:"Dados extraídos"},elements_found:{type:"number",description:"Número de elementos encontrados"},error:{type:"string",description:"Mensagem de erro se success=false"}}}},tags:["beautifulsoup","bs4","scraping","parsing","html","xml","web","extraction","dom","selector"],keywords:["beautifulsoup","bs4","scraping","scrape","raspagem","parsing","parse","html","xml","extract","extrair","web","page","pagina","selector","css","find","buscar"],performance:{speed:7,memory:8,cpuIntensive:!1,gpuAccelerated:!1,scalability:6},scoring:{baseScore:.9,rules:[{condition:'keywords include ["scraping", "scrape", "extrair", "extract"]',adjustment:.08,description:"Perfeito para web scraping"},{condition:'keywords include ["html", "parse", "parsing"]',adjustment:.08,description:"Ideal para parsing de HTML"},{condition:'keywords include ["table", "tabela", "dados"]',adjustment:.05,description:"Ótimo para extrair tabelas"},{condition:'keywords include ["link", "links", "url"]',adjustment:.05,description:"Excelente para coletar links"},{condition:'keywords include ["text", "texto", "content"]',adjustment:.05,description:"Bom para extrair texto"},{condition:'keywords include ["selector", "css", "class", "id"]',adjustment:.05,description:"Suporte completo a CSS selectors"},{condition:'keywords include ["javascript", "js", "dynamic", "ajax"]',adjustment:-.6,description:"Não executa JavaScript, use Selenium/Playwright"},{condition:'keywords include ["api", "rest", "json"] AND NOT include ["html", "scraping"]',adjustment:-.4,description:"Para APIs REST, use requests diretamente"},{condition:'keywords include ["large scale", "millions", "concurrent"]',adjustment:-.3,description:"Para larga escala, use Scrapy"}]},config:{maxRetries:3,timeout:1e4,cacheable:!0,requiresAuth:!1,rateLimit:null},alternatives:[{name:"Scrapy",when:"Web scraping em larga escala, crawling complexo",reason:"Scrapy é framework completo para scraping em produção"},{name:"Selenium/Playwright",when:"Sites com JavaScript, conteúdo dinâmico",reason:"Renderizam JavaScript e simulam navegador real"},{name:"lxml",when:"Performance crítica, parsing muito rápido",reason:"lxml é mais rápido mas API menos conveniente"},{name:"Requests + Regex",when:"Extração muito simples, padrões regulares",reason:"Regex pode ser mais direto para casos triviais"}],documentation:{official:"https://www.crummy.com/software/BeautifulSoup/bs4/doc/",examples:"https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start",apiReference:"https://www.crummy.com/software/BeautifulSoup/bs4/doc/#methods"},commonIssues:[{issue:"Elemento retorna None",solution:"Sempre verificar se elemento existe antes de acessar",code:"element = soup.find('div', class_='content')\nif element:\n    text = element.get_text()\nelse:\n    text = \"Elemento não encontrado\""},{issue:"KeyError ao acessar atributo",solution:"Usar .get() ao invés de []",code:"# Errado: href = a_tag['href']  # KeyError se não existir\n# Certo:\nhref = a_tag.get('href', '')  # Retorna '' se não existir"},{issue:"Parsing lento",solution:"Usar lxml ao invés de html.parser",code:"# Mais rápido\nsoup = BeautifulSoup(html, 'lxml')\n# Mais lento\nsoup = BeautifulSoup(html, 'html.parser')"},{issue:"Texto com espaços em branco excessivos",solution:"Usar strip=True em get_text()",code:"text = element.get_text(strip=True)\n# ou\ntext = element.get_text().strip()"},{issue:"Encoding errado (caracteres estranhos)",solution:"Especificar encoding correto",code:"response = requests.get(url)\nresponse.encoding = 'utf-8'\nsoup = BeautifulSoup(response.content, 'lxml')"}],bestPractices:["Sempre especifique o parser (lxml é mais rápido)","Verifique se elemento existe antes de acessar (if element:)","Use .get() para atributos ao invés de [] para evitar KeyError","Use strip=True em get_text() para remover espaços","Combine com requests para buscar páginas HTML","Use CSS selectors (.select()) para buscas complexas","Remova scripts e styles antes de extrair texto","Use urljoin() para converter URLs relativas em absolutas"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},F={id:"selenium-007",name:"Selenium WebDriver",packageName:"selenium",version:"4.16.0",category:w.BROWSER_AUTOMATION,subcategories:["web-automation","browser-control","web-scraping","testing","javascript-rendering","form-automation","screenshot","interaction-simulation"],description:"Biblioteca mais popular para automação de navegadores web, permitindo controlar Chrome, Firefox, Edge e Safari programaticamente. Ideal para scraping de sites com JavaScript, testes automatizados e automação de tarefas web.",purpose:"Automatizar navegadores web, executar JavaScript, interagir com elementos dinâmicos e realizar scraping de sites complexos",useCases:["Web scraping de sites com JavaScript/AJAX","Automação de tarefas repetitivas em websites","Testes automatizados end-to-end","Preencher formulários automaticamente","Fazer login e navegar em sites","Capturar screenshots de páginas","Coletar dados de SPAs (Single Page Applications)","Automatizar downloads de arquivos","Monitorar mudanças em websites","Interagir com elementos dinâmicos"],complexity:T.INTERMEDIATE,environment:S.PYTHON,dependencies:["urllib3"],installCommand:"pip install selenium",promptSystem:{systemPrompt:"Você é um especialista em Selenium WebDriver, a biblioteca Python para automação de navegadores.\n\nAo trabalhar com Selenium, você SEMPRE deve:\n- Usar WebDriverWait para esperar elementos carregarem (não time.sleep)\n- Verificar se elementos existem antes de interagir\n- Usar expected_conditions para waits explícitos\n- Configurar headless mode quando não precisa visualizar\n- Fazer quit() do driver ao finalizar ou usar context manager\n- Usar By.CSS_SELECTOR ou By.XPATH para localizar elementos\n- Adicionar try-except para capturar exceções de elementos não encontrados\n\nREGRAS DE USO:\n1. SEMPRE use WebDriverWait ao invés de time.sleep\n2. SEMPRE faça driver.quit() ao finalizar ou use context manager\n3. SEMPRE use expected_conditions (EC) para waits\n4. NUNCA deixe navegador aberto sem quit()\n5. SEMPRE verifique se elemento está presente antes de clicar\n6. Use headless=True para servidores sem display\n7. Configure user-agent para evitar detecção como bot\n\nQUANDO USAR SELENIUM:\n✅ Sites com JavaScript/AJAX dinâmico\n✅ SPAs (React, Vue, Angular)\n✅ Sites que precisam de interação (cliques, scroll)\n✅ Páginas que carregam conteúdo dinamicamente\n✅ Fazer login e navegar autenticado\n✅ Preencher formulários complexos\n✅ Capturar screenshots\n✅ Sites com proteção anti-scraping básica\n\nQUANDO NÃO USAR SELENIUM:\n❌ Sites estáticos simples (use requests + BeautifulSoup)\n❌ APIs REST disponíveis (use requests)\n❌ Performance crítica (Selenium é mais lento)\n❌ Scraping em larga escala (use Scrapy + Splash)\n❌ Apenas parsing de HTML (use BeautifulSoup)\n❌ Operações que não precisam de navegador\n\nESTRUTURA DE RESPOSTA:\nSempre retorne um dicionário com:\n- success: boolean\n- data: dados coletados\n- url: URL acessada\n- screenshots: paths de screenshots (se aplicável)\n- error: mensagem de erro (se houver)",userPromptTemplate:"Tarefa: {task_description}\n\nEntrada:\n{input_description}\n\nParâmetros:\n{parameters}\n\nUse Selenium WebDriver seguindo as melhores práticas:\n1. Configure o driver (Chrome/Firefox) com opções adequadas\n2. Navegue até a URL\n3. Espere elementos carregarem (WebDriverWait)\n4. Interaja com a página conforme necessário\n5. Extraia os dados\n6. Feche o driver (quit)",examples:[{input:"Fazer scraping de site com JavaScript",output:'from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.chrome.options import Options\nfrom typing import Dict, Any, List\n\ndef scrape_dynamic_site(url: str, element_selector: str, timeout: int = 10) -> Dict[str, Any]:\n    """Faz scraping de site com JavaScript usando Selenium"""\n    driver = None\n    try:\n        # Configurar Chrome em modo headless\n        chrome_options = Options()\n        chrome_options.add_argument(\'--headless\')\n        chrome_options.add_argument(\'--no-sandbox\')\n        chrome_options.add_argument(\'--disable-dev-shm-usage\')\n        chrome_options.add_argument(\'--disable-gpu\')\n        chrome_options.add_argument(\'--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\')\n\n        # Inicializar driver\n        driver = webdriver.Chrome(options=chrome_options)\n        driver.get(url)\n\n        # Esperar elementos carregarem\n        wait = WebDriverWait(driver, timeout)\n        elements = wait.until(\n            EC.presence_of_all_elements_located((By.CSS_SELECTOR, element_selector))\n        )\n\n        # Extrair dados\n        data = []\n        for element in elements:\n            data.append({\n                "text": element.text,\n                "html": element.get_attribute(\'innerHTML\'),\n                "href": element.get_attribute(\'href\') if element.tag_name == \'a\' else None\n            })\n\n        return {\n            "success": True,\n            "url": url,\n            "elements_found": len(data),\n            "data": data\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}\n    finally:\n        if driver:\n            driver.quit()'},{input:"Fazer login em um site e extrair dados autenticados",output:'from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.chrome.options import Options\n\ndef login_and_scrape(login_url: str, username: str, password: str, data_url: str) -> Dict[str, Any]:\n    """Faz login e extrai dados de área autenticada"""\n    driver = None\n    try:\n        chrome_options = Options()\n        chrome_options.add_argument(\'--headless\')\n        chrome_options.add_argument(\'--no-sandbox\')\n\n        driver = webdriver.Chrome(options=chrome_options)\n        wait = WebDriverWait(driver, 10)\n\n        # Acessar página de login\n        driver.get(login_url)\n\n        # Preencher formulário de login\n        username_field = wait.until(EC.presence_of_element_located((By.ID, \'username\')))\n        password_field = driver.find_element(By.ID, \'password\')\n        submit_button = driver.find_element(By.CSS_SELECTOR, \'button[type="submit"]\')\n\n        username_field.send_keys(username)\n        password_field.send_keys(password)\n        submit_button.click()\n\n        # Esperar login completar (aguardar redirecionamento)\n        wait.until(EC.url_changes(login_url))\n\n        # Navegar para página de dados\n        driver.get(data_url)\n\n        # Esperar conteúdo carregar\n        content = wait.until(EC.presence_of_element_located((By.CLASS_NAME, \'content\')))\n\n        # Extrair dados\n        data = {\n            "title": driver.title,\n            "content": content.text,\n            "current_url": driver.current_url\n        }\n\n        return {\n            "success": True,\n            "logged_in": True,\n            "data": data\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}\n    finally:\n        if driver:\n            driver.quit()'},{input:"Capturar screenshot de página",output:'from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nimport time\n\ndef capture_screenshot(url: str, output_path: str, full_page: bool = True) -> Dict[str, Any]:\n    """Captura screenshot de uma página"""\n    driver = None\n    try:\n        chrome_options = Options()\n        chrome_options.add_argument(\'--headless\')\n        chrome_options.add_argument(\'--no-sandbox\')\n        chrome_options.add_argument(\'--window-size=1920,1080\')\n\n        driver = webdriver.Chrome(options=chrome_options)\n        driver.get(url)\n\n        # Esperar página carregar completamente\n        time.sleep(2)  # Dar tempo para JavaScript executar\n\n        if full_page:\n            # Screenshot da página completa\n            original_size = driver.get_window_size()\n            required_width = driver.execute_script(\'return document.body.scrollWidth\')\n            required_height = driver.execute_script(\'return document.body.scrollHeight\')\n            driver.set_window_size(required_width, required_height)\n\n        # Capturar screenshot\n        driver.save_screenshot(output_path)\n\n        return {\n            "success": True,\n            "url": url,\n            "screenshot_path": output_path,\n            "full_page": full_page\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}\n    finally:\n        if driver:\n            driver.quit()'},{input:"Scroll infinito e coleta de dados",output:'from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.options import Options\nimport time\n\ndef scroll_and_collect(url: str, scroll_times: int = 5, scroll_pause: int = 2) -> Dict[str, Any]:\n    """Faz scroll infinito e coleta dados"""\n    driver = None\n    try:\n        chrome_options = Options()\n        chrome_options.add_argument(\'--headless\')\n\n        driver = webdriver.Chrome(options=chrome_options)\n        driver.get(url)\n\n        all_items = set()\n        last_height = driver.execute_script("return document.body.scrollHeight")\n\n        for _ in range(scroll_times):\n            # Scroll até o final\n            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")\n\n            # Esperar carregar\n            time.sleep(scroll_pause)\n\n            # Coletar itens\n            items = driver.find_elements(By.CSS_SELECTOR, \'.item\')  # Ajustar seletor\n            for item in items:\n                all_items.add(item.text)\n\n            # Verificar se chegou ao fim\n            new_height = driver.execute_script("return document.body.scrollHeight")\n            if new_height == last_height:\n                break\n            last_height = new_height\n\n        return {\n            "success": True,\n            "url": url,\n            "items_collected": len(all_items),\n            "items": list(all_items)\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}\n    finally:\n        if driver:\n            driver.quit()'},{input:"Preencher formulário e submeter",output:'from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait, Select\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.chrome.options import Options\n\ndef fill_and_submit_form(url: str, form_data: Dict[str, str]) -> Dict[str, Any]:\n    """Preenche e submete formulário"""\n    driver = None\n    try:\n        chrome_options = Options()\n        chrome_options.add_argument(\'--headless\')\n\n        driver = webdriver.Chrome(options=chrome_options)\n        wait = WebDriverWait(driver, 10)\n\n        driver.get(url)\n\n        # Preencher campos\n        for field_name, value in form_data.items():\n            try:\n                # Tentar por ID\n                field = driver.find_element(By.ID, field_name)\n            except:\n                # Tentar por name\n                field = driver.find_element(By.NAME, field_name)\n\n            # Verificar tipo de campo\n            if field.tag_name == \'select\':\n                Select(field).select_by_visible_text(value)\n            elif field.get_attribute(\'type\') == \'checkbox\':\n                if value.lower() in [\'true\', \'1\', \'yes\']:\n                    if not field.is_selected():\n                        field.click()\n            else:\n                field.clear()\n                field.send_keys(value)\n\n        # Submeter formulário\n        submit_button = driver.find_element(By.CSS_SELECTOR, \'button[type="submit"], input[type="submit"]\')\n        submit_button.click()\n\n        # Esperar resposta\n        wait.until(EC.url_changes(url))\n\n        return {\n            "success": True,\n            "form_submitted": True,\n            "final_url": driver.current_url,\n            "page_title": driver.title\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}\n    finally:\n        if driver:\n            driver.quit()'}],outputFormat:{type:"object",required:["success"],properties:{success:{type:"boolean",description:"Indica se a operação foi bem-sucedida"},url:{type:"string",description:"URL acessada"},data:{type:"any",description:"Dados coletados"},screenshot_path:{type:"string",description:"Caminho do screenshot (se aplicável)"},elements_found:{type:"number",description:"Número de elementos encontrados"},error:{type:"string",description:"Mensagem de erro se success=false"}}}},tags:["selenium","webdriver","browser","automation","scraping","testing","javascript","dynamic","interaction"],keywords:["selenium","webdriver","browser","navegador","automation","automacao","scraping","javascript","dynamic","dinamico","click","form","formulario","login","screenshot","interaction","interacao"],performance:{speed:4,memory:6,cpuIntensive:!0,gpuAccelerated:!1,scalability:5},scoring:{baseScore:.8,rules:[{condition:'keywords include ["javascript", "js", "dynamic", "ajax", "spa"]',adjustment:.15,description:"Essencial para sites com JavaScript"},{condition:'keywords include ["login", "authenticate", "session"]',adjustment:.1,description:"Excelente para automação de login"},{condition:'keywords include ["form", "formulario", "submit", "preencher"]',adjustment:.1,description:"Ideal para preencher formulários"},{condition:'keywords include ["click", "interact", "scroll", "interacao"]',adjustment:.1,description:"Perfeito para interação com elementos"},{condition:'keywords include ["screenshot", "captura", "image"]',adjustment:.08,description:"Ótimo para screenshots"},{condition:'keywords include ["react", "vue", "angular", "spa"]',adjustment:.1,description:"Ideal para SPAs"},{condition:'keywords include ["static", "simple"] AND NOT include ["javascript", "dynamic"]',adjustment:-.4,description:"Sites estáticos são melhor com BeautifulSoup"},{condition:'keywords include ["api", "rest", "json"] AND NOT include ["scraping", "browser"]',adjustment:-.5,description:"APIs REST não precisam de navegador"},{condition:'keywords include ["large scale", "millions", "performance"]',adjustment:-.3,description:"Selenium é lento para larga escala"}]},config:{maxRetries:2,timeout:6e4,cacheable:!1,requiresAuth:!1,rateLimit:null},alternatives:[{name:"Playwright",when:"Automação moderna, melhor performance, múltiplos contextos",reason:"Playwright é mais rápido e tem API mais moderna"},{name:"Puppeteer",when:"JavaScript/Node.js, apenas Chrome",reason:"Puppeteer é nativo para Node.js"},{name:"BeautifulSoup + Requests",when:"Sites estáticos sem JavaScript",reason:"Muito mais rápido e leve para sites estáticos"},{name:"Scrapy",when:"Web scraping em larga escala",reason:"Scrapy é framework completo e mais performático"}],documentation:{official:"https://www.selenium.dev/documentation/",examples:"https://selenium-python.readthedocs.io/",apiReference:"https://www.selenium.dev/selenium/docs/api/py/"},commonIssues:[{issue:"Element not found / NoSuchElementException",solution:"Usar WebDriverWait para esperar elemento carregar",code:'from selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\nelement = WebDriverWait(driver, 10).until(\n    EC.presence_of_element_located((By.ID, "element-id"))\n)'},{issue:"ChromeDriver version mismatch",solution:"Instalar webdriver-manager para gerenciar drivers automaticamente",code:"# pip install webdriver-manager\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.chrome.service import Service\n\nservice = Service(ChromeDriverManager().install())\ndriver = webdriver.Chrome(service=service)"},{issue:"StaleElementReferenceException",solution:"Re-localizar elemento após mudanças na página",code:'# Não armazenar referência do elemento\n# Sempre buscar novamente quando necessário\nelement = driver.find_element(By.ID, "element-id")\nelement.click()'},{issue:"TimeoutException",solution:"Aumentar timeout ou verificar se elemento realmente existe",code:'wait = WebDriverWait(driver, 30)  # Aumentar timeout\n# Ou verificar se elemento existe\ntry:\n    element = wait.until(EC.presence_of_element_located((By.ID, "id")))\nexcept TimeoutException:\n    print("Elemento não encontrado após timeout")'}],bestPractices:["Use WebDriverWait ao invés de time.sleep","Sempre faça driver.quit() ao finalizar","Configure headless mode em produção","Use expected_conditions para waits explícitos","Configure user-agent para evitar detecção","Verifique se elementos existem antes de interagir","Use context manager quando possível","Configure timeouts adequados para cada operação"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},z={id:"sqlalchemy-008",name:"SQLAlchemy",packageName:"sqlalchemy",version:"2.0.0",category:w.DATABASE,subcategories:["orm","database","sql","query-builder","migrations","models","relationships","postgresql","mysql","sqlite"],description:"ORM (Object-Relational Mapping) e toolkit SQL mais popular para Python. Oferece abstração completa de banco de dados com suporte a múltiplos backends, query builder poderoso e sistema de migrations.",purpose:"Interagir com bancos de dados relacionais usando Python de forma orientada a objetos, executar queries complexas e gerenciar schemas",useCases:["Criar e gerenciar modelos de banco de dados (ORM)","Executar queries SQL complexas","Relacionamentos entre tabelas (1:1, 1:N, N:N)","Migrations e versionamento de schema","Suporte a PostgreSQL, MySQL, SQLite, Oracle","Transações e rollback","Connection pooling","Query builder type-safe","Eager/Lazy loading de relacionamentos","Raw SQL quando necessário"],complexity:T.ADVANCED,environment:S.PYTHON,dependencies:["greenlet","typing-extensions"],installCommand:"pip install sqlalchemy",promptSystem:{systemPrompt:"Você é um especialista em SQLAlchemy, o ORM Python para bancos de dados relacionais.\n\nAo trabalhar com SQLAlchemy, você SEMPRE deve:\n- Usar SQLAlchemy 2.0 style (declarative base com mapped_column)\n- Criar session factory com proper scope\n- Fazer commit explícito de transações\n- Usar context manager para sessions (with statement)\n- Definir relacionamentos com lazy loading adequado\n- Adicionar indexes em colunas de busca frequente\n- Usar type hints nas definições de modelos\n\nREGRAS DE USO:\n1. SEMPRE use context manager para sessions\n2. SEMPRE faça commit explícito ou rollback em caso de erro\n3. SEMPRE defina __repr__ nos modelos para debugging\n4. NUNCA deixe sessões abertas sem close\n5. SEMPRE use eager loading para evitar N+1 queries\n6. Use engine.dispose() ao finalizar aplicação\n7. Configure connection pool adequadamente\n\nQUANDO USAR SQLALCHEMY:\n✅ Aplicações com banco de dados relacional\n✅ Modelos complexos com relacionamentos\n✅ Queries SQL complexas com type safety\n✅ Migrations de schema\n✅ Múltiplos backends de database\n✅ Transações complexas\n✅ Connection pooling\n\nQUANDO NÃO USAR SQLALCHEMY:\n❌ Bancos NoSQL (use pymongo, redis-py)\n❌ Queries extremamente simples (use sqlite3 direto)\n❌ Apenas raw SQL sem ORM (use psycopg2, pymysql)\n❌ GraphQL APIs (use Strawberry, Graphene)\n❌ Performance crítica com milhões de inserções (use bulk inserts)\n\nESTRUTURA DE RESPOSTA:\nSempre retorne um dicionário com:\n- success: boolean\n- data: dados retornados (lista de objetos ou dicts)\n- rows_affected: número de linhas afetadas\n- query: query executada (se debug)\n- error: mensagem de erro (se houver)",userPromptTemplate:"Tarefa: {task_description}\n\nEntrada:\n{input_description}\n\nParâmetros:\n{parameters}\n\nUse SQLAlchemy seguindo as melhores práticas:\n1. Configure engine e session\n2. Defina modelos se necessário\n3. Execute a operação (query, insert, update, delete)\n4. Commit/rollback adequadamente\n5. Retorne dados estruturados",examples:[{input:"Criar modelo de usuário com relacionamentos",output:'from sqlalchemy import create_engine, String, Integer, ForeignKey, DateTime\nfrom sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship, Session\nfrom datetime import datetime\nfrom typing import List, Optional\n\n# Base declarativa\nclass Base(DeclarativeBase):\n    pass\n\n# Modelo User\nclass User(Base):\n    __tablename__ = \'users\'\n\n    id: Mapped[int] = mapped_column(primary_key=True)\n    username: Mapped[str] = mapped_column(String(50), unique=True, index=True)\n    email: Mapped[str] = mapped_column(String(100), unique=True)\n    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)\n\n    # Relacionamento 1:N\n    posts: Mapped[List["Post"]] = relationship(back_populates="author", cascade="all, delete-orphan")\n\n    def __repr__(self):\n        return f"<User(id={self.id}, username=\'{self.username}\')>"\n\n# Modelo Post\nclass Post(Base):\n    __tablename__ = \'posts\'\n\n    id: Mapped[int] = mapped_column(primary_key=True)\n    title: Mapped[str] = mapped_column(String(200))\n    content: Mapped[str]\n    author_id: Mapped[int] = mapped_column(ForeignKey(\'users.id\'))\n    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)\n\n    # Relacionamento N:1\n    author: Mapped["User"] = relationship(back_populates="posts")\n\n    def __repr__(self):\n        return f"<Post(id={self.id}, title=\'{self.title}\')>"\n\n# Setup\nengine = create_engine(\'sqlite:///app.db\', echo=True)\nBase.metadata.create_all(engine)'},{input:"Executar queries com filtros e joins",output:'from sqlalchemy import create_engine, select\nfrom sqlalchemy.orm import Session, joinedload\nfrom typing import Dict, Any, List\n\ndef query_users_with_posts(min_posts: int = 0) -> Dict[str, Any]:\n    """Query usuários com seus posts usando joins"""\n    engine = create_engine(\'sqlite:///app.db\')\n\n    try:\n        with Session(engine) as session:\n            # Query com join e eager loading\n            stmt = (\n                select(User)\n                .join(User.posts)\n                .options(joinedload(User.posts))\n                .where(User.posts.any())\n                .group_by(User.id)\n                .having(func.count(Post.id) >= min_posts)\n            )\n\n            users = session.execute(stmt).unique().scalars().all()\n\n            # Converter para dicts\n            result = []\n            for user in users:\n                result.append({\n                    "id": user.id,\n                    "username": user.username,\n                    "email": user.email,\n                    "posts_count": len(user.posts),\n                    "posts": [\n                        {\n                            "id": post.id,\n                            "title": post.title,\n                            "created_at": post.created_at.isoformat()\n                        }\n                        for post in user.posts\n                    ]\n                })\n\n            return {\n                "success": True,\n                "users_found": len(result),\n                "data": result\n            }\n    except Exception as e:\n        return {"success": False, "error": str(e)}\n    finally:\n        engine.dispose()'},{input:"Inserir dados com transação",output:'from sqlalchemy import create_engine\nfrom sqlalchemy.orm import Session\n\ndef create_user_with_posts(username: str, email: str, posts_data: List[Dict]) -> Dict[str, Any]:\n    """Cria usuário e posts em uma transação"""\n    engine = create_engine(\'sqlite:///app.db\')\n\n    try:\n        with Session(engine) as session:\n            # Criar usuário\n            new_user = User(\n                username=username,\n                email=email\n            )\n            session.add(new_user)\n            session.flush()  # Flush para obter ID\n\n            # Criar posts\n            for post_data in posts_data:\n                new_post = Post(\n                    title=post_data[\'title\'],\n                    content=post_data[\'content\'],\n                    author_id=new_user.id\n                )\n                session.add(new_post)\n\n            # Commit transação\n            session.commit()\n\n            return {\n                "success": True,\n                "user_id": new_user.id,\n                "username": new_user.username,\n                "posts_created": len(posts_data)\n            }\n    except Exception as e:\n        session.rollback()\n        return {"success": False, "error": str(e)}\n    finally:\n        engine.dispose()'},{input:"Update e Delete com condições",output:'from sqlalchemy import create_engine, update, delete\nfrom sqlalchemy.orm import Session\n\ndef update_and_delete_posts(user_id: int, archive_old: bool = True) -> Dict[str, Any]:\n    """Atualiza e deleta posts com condições"""\n    engine = create_engine(\'sqlite:///app.db\')\n\n    try:\n        with Session(engine) as session:\n            # Update: marcar posts antigos como arquivados\n            if archive_old:\n                cutoff_date = datetime.utcnow() - timedelta(days=365)\n                stmt_update = (\n                    update(Post)\n                    .where(Post.created_at < cutoff_date)\n                    .where(Post.author_id == user_id)\n                    .values(archived=True)\n                )\n                result_update = session.execute(stmt_update)\n                updated_count = result_update.rowcount\n            else:\n                updated_count = 0\n\n            # Delete: remover posts sem conteúdo\n            stmt_delete = (\n                delete(Post)\n                .where(Post.content == "")\n                .where(Post.author_id == user_id)\n            )\n            result_delete = session.execute(stmt_delete)\n            deleted_count = result_delete.rowcount\n\n            session.commit()\n\n            return {\n                "success": True,\n                "posts_updated": updated_count,\n                "posts_deleted": deleted_count\n            }\n    except Exception as e:\n        session.rollback()\n        return {"success": False, "error": str(e)}\n    finally:\n        engine.dispose()'},{input:"Raw SQL para queries complexas",output:'from sqlalchemy import create_engine, text\nfrom sqlalchemy.orm import Session\n\ndef execute_raw_sql(query: str, params: Dict = None) -> Dict[str, Any]:\n    """Executa raw SQL quando necessário"""\n    engine = create_engine(\'sqlite:///app.db\')\n\n    try:\n        with Session(engine) as session:\n            # Executar raw SQL com parâmetros\n            stmt = text(query)\n            result = session.execute(stmt, params or {})\n\n            # Buscar resultados\n            if result.returns_rows:\n                rows = result.fetchall()\n                columns = result.keys()\n\n                # Converter para lista de dicts\n                data = [dict(zip(columns, row)) for row in rows]\n\n                return {\n                    "success": True,\n                    "rows_found": len(data),\n                    "data": data\n                }\n            else:\n                # DML (INSERT, UPDATE, DELETE)\n                session.commit()\n                return {\n                    "success": True,\n                    "rows_affected": result.rowcount\n                }\n    except Exception as e:\n        session.rollback()\n        return {"success": False, "error": str(e)}\n    finally:\n        engine.dispose()'}],outputFormat:{type:"object",required:["success"],properties:{success:{type:"boolean",description:"Indica se a operação foi bem-sucedida"},data:{type:"array",description:"Dados retornados (lista de objetos)"},rows_affected:{type:"number",description:"Número de linhas afetadas"},rows_found:{type:"number",description:"Número de linhas encontradas"},error:{type:"string",description:"Mensagem de erro se success=false"}}}},tags:["sqlalchemy","orm","database","sql","postgresql","mysql","sqlite","query","models","migrations"],keywords:["sqlalchemy","orm","database","banco de dados","sql","query","model","modelo","table","tabela","postgresql","mysql","sqlite","relationship","relacionamento","migration"],performance:{speed:7,memory:8,cpuIntensive:!1,gpuAccelerated:!1,scalability:8},scoring:{baseScore:.85,rules:[{condition:'keywords include ["orm", "model", "modelo", "relationship"]',adjustment:.1,description:"Perfeito para ORM"},{condition:'keywords include ["database", "banco", "sql", "query"]',adjustment:.08,description:"Ideal para operações de database"},{condition:'keywords include ["postgresql", "mysql", "sqlite"]',adjustment:.07,description:"Suporta múltiplos backends"},{condition:'keywords include ["migration", "schema", "create table"]',adjustment:.05,description:"Bom para migrations"},{condition:'keywords include ["nosql", "mongodb", "redis"]',adjustment:-.6,description:"Não é para bancos NoSQL"},{condition:'keywords include ["simple", "single query"] AND NOT include ["relationship", "join"]',adjustment:-.2,description:"Queries muito simples não precisam de ORM"}]},config:{maxRetries:2,timeout:3e4,cacheable:!1,requiresAuth:!1,rateLimit:null},alternatives:[{name:"Django ORM",when:"Projeto Django, ORM mais simples",reason:"Django ORM é integrado e mais simples"},{name:"Peewee",when:"ORM mais leve e simples",reason:"Peewee é mais leve mas menos poderoso"},{name:"psycopg2/pymysql",when:"Apenas raw SQL, sem ORM",reason:"Drivers nativos são mais diretos"},{name:"Tortoise ORM",when:"ORM assíncrono",reason:"Tortoise é async-first"}],documentation:{official:"https://docs.sqlalchemy.org/",examples:"https://docs.sqlalchemy.org/en/20/tutorial/",apiReference:"https://docs.sqlalchemy.org/en/20/orm/"},commonIssues:[{issue:"DetachedInstanceError",solution:"Usar eager loading ou acessar relacionamentos dentro da sessão",code:"# Use joinedload para eager loading\nstmt = select(User).options(joinedload(User.posts))\nusers = session.execute(stmt).scalars().all()"},{issue:"N+1 Query Problem",solution:"Usar eager loading com joinedload ou selectinload",code:"# Evitar N+1\nstmt = select(User).options(selectinload(User.posts))\nusers = session.execute(stmt).scalars().all()"},{issue:"Session closed error",solution:"Usar context manager ou não fechar sessão prematuramente",code:"with Session(engine) as session:\n    # Fazer operações aqui\n    session.commit()  # Commit antes de sair do context"}],bestPractices:["Use SQLAlchemy 2.0 style com mapped_column","Sempre use context manager para sessions","Faça commit explícito ou rollback em caso de erro","Use eager loading para evitar N+1 queries","Defina indexes em colunas de busca frequente","Configure connection pool adequadamente","Use type hints nas definições de modelos","Defina __repr__ nos modelos para debugging"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},B={id:"fastapi-009",name:"FastAPI",packageName:"fastapi",version:"0.109.0",category:w.WEB_FRAMEWORK,subcategories:["rest-api","web-framework","async","openapi","swagger","pydantic","validation","authentication","websockets","microservices"],description:"Framework web moderno e de alta performance para construir APIs REST com Python 3.7+. Baseado em type hints, oferece validação automática, documentação interativa (Swagger/OpenAPI) e suporte completo a async/await.",purpose:"Criar APIs REST rápidas, robustas e auto-documentadas com validação automática de dados e type safety",useCases:["Criar APIs REST com endpoints HTTP","Validação automática de request/response com Pydantic","Documentação interativa Swagger UI e ReDoc","Autenticação e autorização (OAuth2, JWT)","WebSockets para comunicação em tempo real","Background tasks e workers","Dependency injection","CORS e middleware","Upload e download de arquivos","GraphQL integration","Microservices e APIs escaláveis"],complexity:T.INTERMEDIATE,environment:S.PYTHON,dependencies:["pydantic","starlette","uvicorn"],installCommand:"pip install fastapi uvicorn[standard]",promptSystem:{systemPrompt:"Você é um especialista em FastAPI, o framework Python para APIs REST.\n\nAo trabalhar com FastAPI, você SEMPRE deve:\n- Usar type hints em todas as funções e parâmetros\n- Definir modelos Pydantic para request/response\n- Usar async/await quando possível para melhor performance\n- Adicionar response_model para validação de saída\n- Usar dependency injection para código reutilizável\n- Documentar endpoints com docstrings e tags\n- Configurar CORS quando necessário\n\nREGRAS DE USO:\n1. SEMPRE use type hints (FastAPI depende deles)\n2. SEMPRE defina Pydantic models para validação\n3. SEMPRE adicione response_model nos endpoints\n4. Use async def quando possível (melhor performance)\n5. SEMPRE trate erros com HTTPException\n6. Configure CORS para APIs públicas\n7. Use dependency injection para auth e validação\n\nQUANDO USAR FASTAPI:\n✅ Criar APIs REST modernas\n✅ Microservices\n✅ APIs com validação automática\n✅ Documentação automática necessária\n✅ Performance crítica (async)\n✅ Type safety importante\n✅ WebSockets\n✅ Background tasks\n\nQUANDO NÃO USAR FASTAPI:\n❌ Sites com templates HTML (use Flask/Django)\n❌ Aplicações sem API (use Django)\n❌ Projetos legados Python 2.7\n❌ Apenas scripts simples\n❌ GraphQL puro (use Strawberry/Graphene)\n\nESTRUTURA DE RESPOSTA:\nSempre retorne código FastAPI com:\n- Type hints completos\n- Pydantic models\n- Response models\n- Error handling\n- Documentação inline",userPromptTemplate:"Tarefa: {task_description}\n\nEntrada:\n{input_description}\n\nParâmetros:\n{parameters}\n\nCrie um endpoint FastAPI seguindo as melhores práticas:\n1. Defina Pydantic models para request/response\n2. Use type hints apropriados\n3. Adicione validação automática\n4. Implemente error handling\n5. Documente o endpoint",examples:[{input:"Criar API REST básica com CRUD",output:'from fastapi import FastAPI, HTTPException, status\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\nfrom datetime import datetime\n\napp = FastAPI(\n    title="User API",\n    description="API REST para gerenciar usuários",\n    version="1.0.0"\n)\n\n# Models\nclass UserBase(BaseModel):\n    username: str = Field(..., min_length=3, max_length=50)\n    email: str = Field(..., regex=r"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$")\n    full_name: Optional[str] = None\n\nclass UserCreate(UserBase):\n    password: str = Field(..., min_length=8)\n\nclass UserResponse(UserBase):\n    id: int\n    created_at: datetime\n    is_active: bool = True\n\n    class Config:\n        from_attributes = True\n\n# In-memory database\nusers_db: List[UserResponse] = []\n\n# Endpoints\n@app.post("/users/", response_model=UserResponse, status_code=status.HTTP_201_CREATED)\nasync def create_user(user: UserCreate):\n    """Criar novo usuário"""\n    # Verificar se username já existe\n    if any(u.username == user.username for u in users_db):\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail="Username já existe"\n        )\n\n    new_user = UserResponse(\n        id=len(users_db) + 1,\n        username=user.username,\n        email=user.email,\n        full_name=user.full_name,\n        created_at=datetime.utcnow()\n    )\n    users_db.append(new_user)\n    return new_user\n\n@app.get("/users/", response_model=List[UserResponse])\nasync def list_users(skip: int = 0, limit: int = 100):\n    """Listar todos os usuários"""\n    return users_db[skip : skip + limit]\n\n@app.get("/users/{user_id}", response_model=UserResponse)\nasync def get_user(user_id: int):\n    """Buscar usuário por ID"""\n    user = next((u for u in users_db if u.id == user_id), None)\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail="Usuário não encontrado"\n        )\n    return user\n\n@app.delete("/users/{user_id}", status_code=status.HTTP_204_NO_CONTENT)\nasync def delete_user(user_id: int):\n    """Deletar usuário"""\n    global users_db\n    user = next((u for u in users_db if u.id == user_id), None)\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail="Usuário não encontrado"\n        )\n    users_db = [u for u in users_db if u.id != user_id]'},{input:"Implementar autenticação JWT",output:'from fastapi import FastAPI, Depends, HTTPException, status\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom pydantic import BaseModel\nfrom jose import JWTError, jwt\nfrom passlib.context import CryptContext\nfrom datetime import datetime, timedelta\n\napp = FastAPI()\n\n# Configurações\nSECRET_KEY = "your-secret-key-here"\nALGORITHM = "HS256"\nACCESS_TOKEN_EXPIRE_MINUTES = 30\n\n# Security\npwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")\noauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")\n\n# Models\nclass Token(BaseModel):\n    access_token: str\n    token_type: str\n\nclass TokenData(BaseModel):\n    username: Optional[str] = None\n\nclass User(BaseModel):\n    username: str\n    email: Optional[str] = None\n    full_name: Optional[str] = None\n    disabled: Optional[bool] = None\n\n# Funções auxiliares\ndef verify_password(plain_password, hashed_password):\n    return pwd_context.verify(plain_password, hashed_password)\n\ndef get_password_hash(password):\n    return pwd_context.hash(password)\n\ndef create_access_token(data: dict, expires_delta: Optional[timedelta] = None):\n    to_encode = data.copy()\n    if expires_delta:\n        expire = datetime.utcnow() + expires_delta\n    else:\n        expire = datetime.utcnow() + timedelta(minutes=15)\n    to_encode.update({"exp": expire})\n    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)\n    return encoded_jwt\n\nasync def get_current_user(token: str = Depends(oauth2_scheme)):\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail="Credenciais inválidas",\n        headers={"WWW-Authenticate": "Bearer"},\n    )\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n        username: str = payload.get("sub")\n        if username is None:\n            raise credentials_exception\n        token_data = TokenData(username=username)\n    except JWTError:\n        raise credentials_exception\n\n    # Buscar usuário no banco\n    user = get_user(username=token_data.username)\n    if user is None:\n        raise credentials_exception\n    return user\n\n# Endpoints\n@app.post("/token", response_model=Token)\nasync def login(form_data: OAuth2PasswordRequestForm = Depends()):\n    """Login e geração de token JWT"""\n    user = authenticate_user(form_data.username, form_data.password)\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail="Username ou senha incorretos",\n            headers={"WWW-Authenticate": "Bearer"},\n        )\n    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n    access_token = create_access_token(\n        data={"sub": user.username}, expires_delta=access_token_expires\n    )\n    return {"access_token": access_token, "token_type": "bearer"}\n\n@app.get("/users/me/", response_model=User)\nasync def read_users_me(current_user: User = Depends(get_current_user)):\n    """Retornar usuário autenticado"""\n    return current_user'},{input:"Upload e download de arquivos",output:'from fastapi import FastAPI, File, UploadFile, HTTPException\nfrom fastapi.responses import FileResponse\nfrom pathlib import Path\nfrom typing import List\nimport shutil\n\napp = FastAPI()\n\nUPLOAD_DIR = Path("uploads")\nUPLOAD_DIR.mkdir(exist_ok=True)\n\n@app.post("/upload/")\nasync def upload_file(file: UploadFile = File(...)):\n    """Upload de arquivo único"""\n    try:\n        file_path = UPLOAD_DIR / file.filename\n\n        with file_path.open("wb") as buffer:\n            shutil.copyfileobj(file.file, buffer)\n\n        return {\n            "filename": file.filename,\n            "content_type": file.content_type,\n            "size": file_path.stat().st_size,\n            "path": str(file_path)\n        }\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n    finally:\n        await file.close()\n\n@app.post("/upload-multiple/")\nasync def upload_multiple_files(files: List[UploadFile] = File(...)):\n    """Upload de múltiplos arquivos"""\n    uploaded_files = []\n\n    for file in files:\n        try:\n            file_path = UPLOAD_DIR / file.filename\n            with file_path.open("wb") as buffer:\n                shutil.copyfileobj(file.file, buffer)\n\n            uploaded_files.append({\n                "filename": file.filename,\n                "size": file_path.stat().st_size\n            })\n        except Exception as e:\n            uploaded_files.append({\n                "filename": file.filename,\n                "error": str(e)\n            })\n        finally:\n            await file.close()\n\n    return {"files": uploaded_files}\n\n@app.get("/download/{filename}")\nasync def download_file(filename: str):\n    """Download de arquivo"""\n    file_path = UPLOAD_DIR / filename\n\n    if not file_path.exists():\n        raise HTTPException(status_code=404, detail="Arquivo não encontrado")\n\n    return FileResponse(\n        path=file_path,\n        filename=filename,\n        media_type="application/octet-stream"\n    )'},{input:"Background tasks e workers",output:'from fastapi import FastAPI, BackgroundTasks\nfrom pydantic import BaseModel, EmailStr\nfrom typing import List\nimport time\n\napp = FastAPI()\n\nclass EmailSchema(BaseModel):\n    email: List[EmailStr]\n    subject: str\n    body: str\n\ndef send_email_background(email: str, subject: str, body: str):\n    """Simular envio de email (tarefa pesada)"""\n    print(f"Enviando email para {email}...")\n    time.sleep(5)  # Simular delay\n    print(f"Email enviado para {email}")\n\ndef process_data_background(data: dict):\n    """Processar dados em background"""\n    print(f"Processando {len(data)} itens...")\n    time.sleep(10)\n    print("Processamento concluído!")\n\n@app.post("/send-email/")\nasync def send_email(email_data: EmailSchema, background_tasks: BackgroundTasks):\n    """Enviar emails em background"""\n    # Adicionar tarefas em background\n    for email in email_data.email:\n        background_tasks.add_task(\n            send_email_background,\n            email,\n            email_data.subject,\n            email_data.body\n        )\n\n    return {\n        "message": "Emails sendo enviados em background",\n        "emails_count": len(email_data.email)\n    }\n\n@app.post("/process/")\nasync def process_data(\n    data: dict,\n    background_tasks: BackgroundTasks\n):\n    """Processar dados em background e retornar imediatamente"""\n    background_tasks.add_task(process_data_background, data)\n\n    return {\n        "message": "Processamento iniciado em background",\n        "status": "processing"\n    }'},{input:"WebSocket para comunicação em tempo real",output:'from fastapi import FastAPI, WebSocket, WebSocketDisconnect\nfrom typing import List\n\napp = FastAPI()\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: List[WebSocket] = []\n\n    async def connect(self, websocket: WebSocket):\n        await websocket.accept()\n        self.active_connections.append(websocket)\n\n    def disconnect(self, websocket: WebSocket):\n        self.active_connections.remove(websocket)\n\n    async def send_personal_message(self, message: str, websocket: WebSocket):\n        await websocket.send_text(message)\n\n    async def broadcast(self, message: str):\n        for connection in self.active_connections:\n            await connection.send_text(message)\n\nmanager = ConnectionManager()\n\n@app.websocket("/ws/{client_id}")\nasync def websocket_endpoint(websocket: WebSocket, client_id: int):\n    """WebSocket para chat em tempo real"""\n    await manager.connect(websocket)\n    try:\n        # Notificar outros clientes\n        await manager.broadcast(f"Cliente #{client_id} entrou no chat")\n\n        while True:\n            # Receber mensagem\n            data = await websocket.receive_text()\n\n            # Enviar de volta para o cliente\n            await manager.send_personal_message(f"Você disse: {data}", websocket)\n\n            # Broadcast para todos\n            await manager.broadcast(f"Cliente #{client_id} disse: {data}")\n    except WebSocketDisconnect:\n        manager.disconnect(websocket)\n        await manager.broadcast(f"Cliente #{client_id} saiu do chat")'}],outputFormat:{type:"object",description:"FastAPI endpoint com validação completa",properties:{endpoint:{type:"string",description:"Rota do endpoint"},method:{type:"string",description:"Método HTTP (GET, POST, etc)"},request_model:{type:"object",description:"Modelo Pydantic de entrada"},response_model:{type:"object",description:"Modelo Pydantic de saída"},status_code:{type:"number",description:"Código HTTP de sucesso"}}}},tags:["fastapi","api","rest","web","framework","async","pydantic","swagger","openapi","microservices"],keywords:["fastapi","api","rest","endpoint","web","framework","async","pydantic","validation","swagger","openapi","jwt","auth","websocket","microservice"],performance:{speed:9,memory:8,cpuIntensive:!1,gpuAccelerated:!1,scalability:9},scoring:{baseScore:.9,rules:[{condition:'keywords include ["api", "rest", "endpoint"]',adjustment:.08,description:"Perfeito para APIs REST"},{condition:'keywords include ["validation", "pydantic", "schema"]',adjustment:.05,description:"Validação automática integrada"},{condition:'keywords include ["async", "performance", "fast"]',adjustment:.05,description:"Alta performance com async"},{condition:'keywords include ["documentation", "swagger", "openapi"]',adjustment:.05,description:"Documentação automática"},{condition:'keywords include ["websocket", "realtime", "tempo real"]',adjustment:.05,description:"Suporte nativo a WebSockets"},{condition:'keywords include ["html", "template", "frontend"]',adjustment:-.4,description:"Não é para sites com templates"},{condition:'keywords include ["graphql"] AND NOT include ["rest"]',adjustment:-.3,description:"GraphQL melhor com Strawberry/Graphene"}]},config:{maxRetries:3,timeout:3e4,cacheable:!1,requiresAuth:!1,rateLimit:null},alternatives:[{name:"Flask",when:"APIs simples, projetos legados, templates HTML",reason:"Flask é mais simples mas menos features"},{name:"Django REST Framework",when:"Projeto Django existente, admin panel necessário",reason:"DRF é integrado ao Django"},{name:"Starlette",when:"Máxima simplicidade, sem validação automática",reason:"Starlette é a base do FastAPI mas mais simples"}],documentation:{official:"https://fastapi.tiangolo.com/",examples:"https://fastapi.tiangolo.com/tutorial/",apiReference:"https://fastapi.tiangolo.com/reference/"},commonIssues:[{issue:"Pydantic validation error",solution:"Verificar type hints e modelos Pydantic",code:"class UserCreate(BaseModel):\n    username: str = Field(..., min_length=3)\n    email: EmailStr  # Validação automática de email"},{issue:"422 Unprocessable Entity",solution:"Request não passa na validação Pydantic",code:"# Adicionar validação customizada\nfrom pydantic import validator\n\nclass User(BaseModel):\n    age: int\n\n    @validator('age')\n    def age_must_be_positive(cls, v):\n        if v < 0:\n            raise ValueError('age must be positive')\n        return v"}],bestPractices:["Use type hints em todas as funções","Defina Pydantic models para request/response","Adicione response_model para validação de saída","Use async def quando possível","Configure CORS para APIs públicas","Use dependency injection para código reutilizável","Documente endpoints com docstrings e tags","Implemente rate limiting para APIs públicas"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},j={id:"scikit-learn-010",name:"Scikit-Learn",packageName:"scikit-learn",version:"1.4.0",category:w.MACHINE_LEARNING,subcategories:["classification","regression","clustering","preprocessing","feature-engineering","model-selection","ensemble","dimensionality-reduction","cross-validation","metrics"],description:"Biblioteca Python mais popular para Machine Learning clássico, oferecendo algoritmos robustos para classificação, regressão, clustering, pré-processamento e seleção de features. Integração perfeita com NumPy e Pandas.",purpose:"Construir, treinar e avaliar modelos de Machine Learning para problemas de classificação, regressão, clustering e mais",useCases:["Classificação (Decision Trees, Random Forest, SVM, Logistic Regression)","Regressão (Linear, Ridge, Lasso, Polynomial)","Clustering (K-Means, DBSCAN, Hierarchical)","Pré-processamento de dados (scaling, encoding, normalization)","Feature engineering e seleção","Cross-validation e model selection","Ensemble methods (Bagging, Boosting, Stacking)","Dimensionality reduction (PCA, t-SNE)","Detecção de anomalias","Métricas de avaliação de modelos"],complexity:T.ADVANCED,environment:S.PYTHON,dependencies:["numpy","scipy","joblib","threadpoolctl"],installCommand:"pip install scikit-learn",promptSystem:{systemPrompt:"Você é um especialista em Scikit-Learn, a biblioteca Python para Machine Learning.\n\nAo trabalhar com Scikit-Learn, você SEMPRE deve:\n- Dividir dados em treino e teste com train_test_split\n- Fazer pré-processamento (scaling, encoding) antes de treinar\n- Usar cross-validation para avaliar modelos robustamente\n- Calcular múltiplas métricas (accuracy, precision, recall, F1)\n- Fazer feature scaling para algoritmos sensíveis (SVM, KNN, Neural Nets)\n- Usar pipelines para encadear transformações\n- Salvar modelos treinados com joblib\n\nREGRAS DE USO:\n1. SEMPRE divida dados em treino/teste (80/20 ou 70/30)\n2. SEMPRE faça scaling/normalization quando necessário\n3. SEMPRE use cross-validation para avaliação robusta\n4. SEMPRE calcule múltiplas métricas (não apenas accuracy)\n5. SEMPRE trate dados faltantes antes de treinar\n6. Use GridSearchCV ou RandomizedSearchCV para hyperparameter tuning\n7. Salve modelos com joblib (não pickle)\n\nQUANDO USAR SCIKIT-LEARN:\n✅ Machine Learning clássico (não deep learning)\n✅ Classificação e regressão\n✅ Clustering e análise exploratória\n✅ Feature engineering\n✅ Datasets pequenos/médios (<1GB)\n✅ Prototipagem rápida de modelos\n✅ Ensembles (Random Forest, Gradient Boosting)\n\nQUANDO NÃO USAR SCIKIT-LEARN:\n❌ Deep Learning (use TensorFlow, PyTorch)\n❌ Processamento de texto avançado (use transformers)\n❌ Dados gigantes (>10GB) (use Spark MLlib)\n❌ GPU acceleration necessária\n❌ Modelos de linguagem (use HuggingFace)\n❌ Computer vision avançada (use PyTorch, TensorFlow)\n\nESTRUTURA DE RESPOSTA:\nSempre retorne um dicionário com:\n- model_type: tipo do modelo usado\n- metrics: métricas de avaliação\n- feature_importance: importância das features (se disponível)\n- predictions: predições (se aplicável)\n- model_path: caminho do modelo salvo",userPromptTemplate:"Tarefa: {task_description}\n\nEntrada:\n{input_description}\n\nParâmetros:\n{parameters}\n\nUse Scikit-Learn seguindo as melhores práticas:\n1. Prepare e pré-processe os dados\n2. Divida em treino e teste\n3. Treine o modelo apropriado\n4. Avalie com métricas adequadas\n5. Retorne resultados estruturados",examples:[{input:"Criar modelo de classificação com Random Forest",output:'from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport pandas as pd\nimport numpy as np\nimport joblib\n\ndef train_classification_model(X: pd.DataFrame, y: pd.Series, model_path: str = "model.joblib"):\n    """Treina modelo de classificação com Random Forest"""\n\n    # Dividir dados\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n\n    # Pré-processamento\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    # Treinar modelo\n    model = RandomForestClassifier(\n        n_estimators=100,\n        max_depth=10,\n        random_state=42,\n        n_jobs=-1\n    )\n    model.fit(X_train_scaled, y_train)\n\n    # Predições\n    y_pred = model.predict(X_test_scaled)\n    y_pred_proba = model.predict_proba(X_test_scaled)\n\n    # Avaliação\n    accuracy = accuracy_score(y_test, y_pred)\n    report = classification_report(y_test, y_pred, output_dict=True)\n\n    # Cross-validation\n    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n\n    # Feature importance\n    feature_importance = pd.DataFrame({\n        \'feature\': X.columns,\n        \'importance\': model.feature_importances_\n    }).sort_values(\'importance\', ascending=False)\n\n    # Salvar modelo e scaler\n    joblib.dump({\'model\': model, \'scaler\': scaler}, model_path)\n\n    return {\n        "success": True,\n        "model_type": "RandomForestClassifier",\n        "accuracy": float(accuracy),\n        "cv_mean_score": float(cv_scores.mean()),\n        "cv_std_score": float(cv_scores.std()),\n        "classification_report": report,\n        "feature_importance": feature_importance.to_dict(\'records\'),\n        "model_path": model_path,\n        "test_samples": len(y_test)\n    }'},{input:"Regressão linear com regularização",output:'from sklearn.linear_model import Ridge, Lasso\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport numpy as np\n\ndef train_regression_model(X, y, regularization: str = "ridge"):\n    """Treina modelo de regressão com regularização"""\n\n    # Split\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    # Scaling\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    # Escolher modelo\n    if regularization == "ridge":\n        model = Ridge()\n        param_grid = {\'alpha\': [0.1, 1.0, 10.0, 100.0]}\n    else:  # lasso\n        model = Lasso()\n        param_grid = {\'alpha\': [0.001, 0.01, 0.1, 1.0]}\n\n    # GridSearch para encontrar melhor alpha\n    grid_search = GridSearchCV(\n        model, param_grid, cv=5,\n        scoring=\'neg_mean_squared_error\',\n        n_jobs=-1\n    )\n    grid_search.fit(X_train_scaled, y_train)\n\n    # Melhor modelo\n    best_model = grid_search.best_estimator_\n\n    # Predições\n    y_pred = best_model.predict(X_test_scaled)\n\n    # Métricas\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n\n    return {\n        "success": True,\n        "model_type": f"{regularization.capitalize()}Regression",\n        "best_alpha": float(grid_search.best_params_[\'alpha\']),\n        "metrics": {\n            "mse": float(mse),\n            "rmse": float(rmse),\n            "mae": float(mae),\n            "r2_score": float(r2)\n        },\n        "cv_best_score": float(-grid_search.best_score_)\n    }'},{input:"Clustering K-Means com elbow method",output:'from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\nimport numpy as np\n\ndef perform_clustering(X, max_clusters: int = 10):\n    """Clustering K-Means com análise de número ótimo de clusters"""\n\n    # Scaling\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Elbow method\n    inertias = []\n    silhouette_scores = []\n\n    for k in range(2, max_clusters + 1):\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        kmeans.fit(X_scaled)\n\n        inertias.append(kmeans.inertia_)\n        silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n\n    # Encontrar melhor K (maior silhouette)\n    best_k = silhouette_scores.index(max(silhouette_scores)) + 2\n\n    # Treinar com melhor K\n    best_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n    labels = best_kmeans.fit_predict(X_scaled)\n\n    # Métricas\n    silhouette = silhouette_score(X_scaled, labels)\n    davies_bouldin = davies_bouldin_score(X_scaled, labels)\n\n    # Tamanho dos clusters\n    unique, counts = np.unique(labels, return_counts=True)\n    cluster_sizes = dict(zip(unique.tolist(), counts.tolist()))\n\n    return {\n        "success": True,\n        "best_k": int(best_k),\n        "n_samples": len(X),\n        "metrics": {\n            "silhouette_score": float(silhouette),\n            "davies_bouldin_score": float(davies_bouldin),\n            "inertia": float(best_kmeans.inertia_)\n        },\n        "cluster_sizes": cluster_sizes,\n        "labels": labels.tolist(),\n        "elbow_data": {\n            "k_values": list(range(2, max_clusters + 1)),\n            "inertias": [float(x) for x in inertias],\n            "silhouette_scores": [float(x) for x in silhouette_scores]\n        }\n    }'},{input:"Pipeline completo com pré-processamento",output:'from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report\nimport pandas as pd\n\ndef create_ml_pipeline(X: pd.DataFrame, y: pd.Series):\n    """Cria pipeline completo de ML com pré-processamento"""\n\n    # Identificar colunas numéricas e categóricas\n    numeric_features = X.select_dtypes(include=[\'int64\', \'float64\']).columns.tolist()\n    categorical_features = X.select_dtypes(include=[\'object\', \'category\']).columns.tolist()\n\n    # Pré-processamento\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\'num\', StandardScaler(), numeric_features),\n            (\'cat\', OneHotEncoder(handle_unknown=\'ignore\'), categorical_features)\n        ]\n    )\n\n    # Pipeline completo\n    pipeline = Pipeline([\n        (\'preprocessor\', preprocessor),\n        (\'classifier\', GradientBoostingClassifier(\n            n_estimators=100,\n            learning_rate=0.1,\n            max_depth=3,\n            random_state=42\n        ))\n    ])\n\n    # Split\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n\n    # Treinar pipeline\n    pipeline.fit(X_train, y_train)\n\n    # Avaliar\n    y_pred = pipeline.predict(X_test)\n    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n\n    report = classification_report(y_test, y_pred, output_dict=True)\n\n    return {\n        "success": True,\n        "model_type": "GradientBoostingClassifier",\n        "pipeline_steps": [step[0] for step in pipeline.steps],\n        "numeric_features": numeric_features,\n        "categorical_features": categorical_features,\n        "cv_scores": {\n            "mean": float(cv_scores.mean()),\n            "std": float(cv_scores.std())\n        },\n        "test_metrics": report\n    }'}],outputFormat:{type:"object",required:["success","model_type"],properties:{success:{type:"boolean",description:"Indica se o treinamento foi bem-sucedido"},model_type:{type:"string",description:"Tipo do modelo treinado"},metrics:{type:"object",description:"Métricas de avaliação"},feature_importance:{type:"array",description:"Importância das features"},model_path:{type:"string",description:"Caminho do modelo salvo"},error:{type:"string",description:"Mensagem de erro se success=false"}}}},tags:["scikit-learn","sklearn","machine-learning","ml","classification","regression","clustering","preprocessing","feature-engineering"],keywords:["scikit-learn","sklearn","machine learning","ml","classification","classificacao","regression","regressao","clustering","agrupamento","random forest","svm","logistic regression","kmeans","pca","preprocessing","feature"],performance:{speed:8,memory:7,cpuIntensive:!0,gpuAccelerated:!1,scalability:7},scoring:{baseScore:.85,rules:[{condition:'keywords include ["classification", "classificacao", "classifier"]',adjustment:.1,description:"Excelente para classificação"},{condition:'keywords include ["regression", "regressao", "predict"]',adjustment:.1,description:"Perfeito para regressão"},{condition:'keywords include ["clustering", "agrupamento", "kmeans"]',adjustment:.1,description:"Ideal para clustering"},{condition:'keywords include ["preprocessing", "feature", "scaling"]',adjustment:.08,description:"Ferramentas robustas de pré-processamento"},{condition:'keywords include ["random forest", "ensemble", "gradient boosting"]',adjustment:.08,description:"Ensembles poderosos"},{condition:'keywords include ["deep learning", "neural network", "cnn", "rnn"]',adjustment:-.5,description:"Deep learning use TensorFlow/PyTorch"},{condition:'keywords include ["nlp", "transformers", "bert", "gpt"]',adjustment:-.6,description:"NLP avançado use HuggingFace Transformers"},{condition:'keywords include ["big data", "spark", "distributed"]',adjustment:-.4,description:"Big data use Spark MLlib"}]},config:{maxRetries:2,timeout:3e5,cacheable:!0,requiresAuth:!1,rateLimit:null},alternatives:[{name:"XGBoost",when:"Performance máxima em gradient boosting, competições",reason:"XGBoost é mais rápido e performático que sklearn GradientBoosting"},{name:"TensorFlow/PyTorch",when:"Deep learning, redes neurais, GPU acceleration",reason:"Frameworks modernos para deep learning"},{name:"LightGBM",when:"Datasets grandes, gradient boosting rápido",reason:"LightGBM é extremamente rápido em grandes datasets"},{name:"HuggingFace Transformers",when:"NLP moderno, modelos pré-treinados",reason:"Transformers é estado da arte em NLP"}],documentation:{official:"https://scikit-learn.org/",examples:"https://scikit-learn.org/stable/auto_examples/",apiReference:"https://scikit-learn.org/stable/modules/classes.html"},commonIssues:[{issue:"ConvergenceWarning",solution:"Aumentar max_iter ou fazer feature scaling",code:"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n# Ou aumentar max_iter\nmodel = LogisticRegression(max_iter=1000)"},{issue:"ValueError: Input contains NaN",solution:"Tratar valores faltantes antes de treinar",code:"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='mean')\nX_imputed = imputer.fit_transform(X)"},{issue:"Overfitting (train score >> test score)",solution:"Usar regularização, reduzir complexidade, mais dados",code:"# Adicionar regularização\nmodel = RandomForestClassifier(max_depth=5, min_samples_leaf=10)\n# Ou usar cross-validation\nscores = cross_val_score(model, X, y, cv=5)"}],bestPractices:["Sempre divida dados em treino e teste","Faça feature scaling para algoritmos sensíveis (SVM, KNN)","Use cross-validation para avaliação robusta","Calcule múltiplas métricas, não apenas accuracy","Use pipelines para encadear transformações","Salve modelos com joblib ao invés de pickle","Faça hyperparameter tuning com GridSearchCV","Trate dados faltantes e outliers antes de treinar"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},G={id:"transformers-011",name:"Transformers (HuggingFace)",packageName:"transformers",version:"4.36.0",category:w.NLP,subcategories:["text-classification","sentiment-analysis","ner","question-answering","text-generation","translation","summarization","embeddings","pretrained-models","bert","gpt","llm"],description:"Biblioteca estado-da-arte para NLP (Natural Language Processing) com acesso a milhares de modelos pré-treinados (BERT, GPT, T5, etc). Oferece APIs simples para tarefas como classificação de texto, análise de sentimento, geração de texto, tradução e muito mais.",purpose:"Processar linguagem natural usando modelos de deep learning pré-treinados para tarefas de NLP",useCases:["Análise de sentimento (positivo, negativo, neutro)","Classificação de texto em categorias","Named Entity Recognition (NER) - extrair nomes, locais, datas","Question Answering - responder perguntas sobre texto","Geração de texto (GPT, GPT-2, GPT-3)","Tradução automática entre idiomas","Sumarização de textos longos","Text embeddings e similarity","Zero-shot classification","Fill-mask (completar texto)","Conversational AI e chatbots","Feature extraction para ML"],complexity:T.ADVANCED,environment:S.PYTHON,dependencies:["torch","numpy","tokenizers","huggingface-hub"],installCommand:"pip install transformers torch",promptSystem:{systemPrompt:'Você é um especialista em HuggingFace Transformers, a biblioteca Python para NLP com modelos pré-treinados.\n\nAo trabalhar com Transformers, você SEMPRE deve:\n- Usar pipeline() para tarefas comuns (mais simples e eficiente)\n- Especificar modelo adequado para a tarefa (bert-base, gpt2, t5, etc)\n- Considerar idioma do modelo (multilingual ou específico)\n- Fazer truncation=True para textos longos\n- Usar device="cuda" quando GPU disponível\n- Cache modelos para evitar downloads repetidos\n- Limitar max_length para evitar OOM (Out of Memory)\n\nREGRAS DE USO:\n1. SEMPRE use pipeline() para tarefas padrão (mais simples)\n2. SEMPRE especifique modelo adequado para o idioma\n3. SEMPRE adicione truncation=True para textos longos\n4. SEMPRE considere max_length para evitar OOM\n5. Use batch processing quando possível (mais eficiente)\n6. Cache modelos localmente após primeiro download\n7. Considere usar modelos menores para prototipagem\n\nQUANDO USAR TRANSFORMERS:\n✅ NLP moderno (classificação, NER, sentiment)\n✅ Análise de texto em produção\n✅ Question answering\n✅ Geração de texto\n✅ Tradução automática\n✅ Embeddings de texto\n✅ Zero-shot learning\n✅ Fine-tuning de modelos\n\nQUANDO NÃO USAR TRANSFORMERS:\n❌ Processamento de texto muito simples (use regex, nltk)\n❌ ML clássico sem deep learning (use scikit-learn)\n❌ Hardware muito limitado (modelos grandes precisam GPU)\n❌ Apenas tokenização simples (use nltk, spacy)\n❌ Aplicações sem deep learning\n❌ Real-time crítico sem GPU\n\nESTRUTURA DE RESPOSTA:\nSempre retorne um dicionário com:\n- task: tarefa executada\n- model: modelo usado\n- results: resultados da inferência\n- scores: scores de confiança\n- processing_time: tempo de processamento',userPromptTemplate:"Tarefa: {task_description}\n\nEntrada:\n{input_description}\n\nParâmetros:\n{parameters}\n\nUse HuggingFace Transformers seguindo as melhores práticas:\n1. Escolha o modelo adequado para a tarefa e idioma\n2. Use pipeline() para simplicidade\n3. Configure truncation e max_length\n4. Execute a inferência\n5. Retorne resultados estruturados",examples:[{input:"Análise de sentimento em português",output:'from transformers import pipeline\nfrom typing import Dict, Any, List\n\ndef analyze_sentiment(texts: List[str]) -> Dict[str, Any]:\n    """Análise de sentimento usando modelo pré-treinado"""\n    try:\n        # Pipeline de sentiment analysis\n        # Modelo multilingual ou português específico\n        classifier = pipeline(\n            "sentiment-analysis",\n            model="nlptown/bert-base-multilingual-uncased-sentiment",\n            device=-1  # CPU (-1) ou GPU (0)\n        )\n\n        # Processar textos\n        results = classifier(\n            texts,\n            truncation=True,\n            max_length=512\n        )\n\n        # Formatar resultados\n        formatted_results = []\n        for text, result in zip(texts, results):\n            formatted_results.append({\n                "text": text[:100] + "..." if len(text) > 100 else text,\n                "sentiment": result[\'label\'],\n                "score": float(result[\'score\'])\n            })\n\n        return {\n            "success": True,\n            "task": "sentiment-analysis",\n            "model": "nlptown/bert-base-multilingual-uncased-sentiment",\n            "texts_processed": len(texts),\n            "results": formatted_results\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Named Entity Recognition (NER)",output:'from transformers import pipeline\nfrom typing import Dict, Any\n\ndef extract_entities(text: str) -> Dict[str, Any]:\n    """Extrai entidades nomeadas do texto (pessoas, locais, organizações)"""\n    try:\n        # Pipeline NER\n        ner = pipeline(\n            "ner",\n            model="dbmdz/bert-large-cased-finetuned-conll03-english",\n            aggregation_strategy="simple"  # Agrupa tokens\n        )\n\n        # Extrair entidades\n        entities = ner(text)\n\n        # Agrupar por tipo\n        entities_by_type = {}\n        for entity in entities:\n            entity_type = entity[\'entity_group\']\n            if entity_type not in entities_by_type:\n                entities_by_type[entity_type] = []\n\n            entities_by_type[entity_type].append({\n                "text": entity[\'word\'],\n                "score": float(entity[\'score\']),\n                "start": entity[\'start\'],\n                "end": entity[\'end\']\n            })\n\n        return {\n            "success": True,\n            "task": "named-entity-recognition",\n            "text_length": len(text),\n            "entities_found": len(entities),\n            "entities_by_type": entities_by_type,\n            "all_entities": [\n                {\n                    "text": e[\'word\'],\n                    "type": e[\'entity_group\'],\n                    "score": float(e[\'score\'])\n                }\n                for e in entities\n            ]\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Question Answering sobre contexto",output:'from transformers import pipeline\nfrom typing import Dict, Any\n\ndef answer_question(context: str, question: str) -> Dict[str, Any]:\n    """Responde pergunta baseada em contexto fornecido"""\n    try:\n        # Pipeline QA\n        qa = pipeline(\n            "question-answering",\n            model="deepset/roberta-base-squad2"\n        )\n\n        # Buscar resposta\n        result = qa(\n            question=question,\n            context=context,\n            max_length=512,\n            truncation=True\n        )\n\n        return {\n            "success": True,\n            "task": "question-answering",\n            "question": question,\n            "answer": result[\'answer\'],\n            "confidence_score": float(result[\'score\']),\n            "start_position": result[\'start\'],\n            "end_position": result[\'end\']\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Geração de texto com GPT-2",output:'from transformers import pipeline\nfrom typing import Dict, Any, List\n\ndef generate_text(\n    prompt: str,\n    max_length: int = 100,\n    num_sequences: int = 1,\n    temperature: float = 0.7\n) -> Dict[str, Any]:\n    """Gera texto usando modelo GPT-2"""\n    try:\n        # Pipeline de geração\n        generator = pipeline(\n            "text-generation",\n            model="gpt2",\n            device=-1\n        )\n\n        # Gerar texto\n        results = generator(\n            prompt,\n            max_length=max_length,\n            num_return_sequences=num_sequences,\n            temperature=temperature,\n            do_sample=True,\n            top_k=50,\n            top_p=0.95\n        )\n\n        # Formatar resultados\n        generated_texts = [r[\'generated_text\'] for r in results]\n\n        return {\n            "success": True,\n            "task": "text-generation",\n            "model": "gpt2",\n            "prompt": prompt,\n            "sequences_generated": len(generated_texts),\n            "generated_texts": generated_texts,\n            "parameters": {\n                "max_length": max_length,\n                "temperature": temperature,\n                "num_sequences": num_sequences\n            }\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Classificação Zero-Shot (sem treinamento)",output:'from transformers import pipeline\nfrom typing import Dict, Any, List\n\ndef zero_shot_classification(\n    text: str,\n    candidate_labels: List[str]\n) -> Dict[str, Any]:\n    """Classifica texto em categorias sem treinamento prévio"""\n    try:\n        # Pipeline zero-shot\n        classifier = pipeline(\n            "zero-shot-classification",\n            model="facebook/bart-large-mnli"\n        )\n\n        # Classificar\n        result = classifier(\n            text,\n            candidate_labels=candidate_labels,\n            multi_label=False\n        )\n\n        # Formatar resultados\n        classifications = [\n            {\n                "label": label,\n                "score": float(score)\n            }\n            for label, score in zip(result[\'labels\'], result[\'scores\'])\n        ]\n\n        return {\n            "success": True,\n            "task": "zero-shot-classification",\n            "text": text[:200],\n            "top_prediction": result[\'labels\'][0],\n            "top_score": float(result[\'scores\'][0]),\n            "all_classifications": classifications\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'},{input:"Sumarização de texto longo",output:'from transformers import pipeline\nfrom typing import Dict, Any\n\ndef summarize_text(\n    text: str,\n    max_length: int = 130,\n    min_length: int = 30\n) -> Dict[str, Any]:\n    """Sumariza texto longo em resumo conciso"""\n    try:\n        # Pipeline de sumarização\n        summarizer = pipeline(\n            "summarization",\n            model="facebook/bart-large-cnn"\n        )\n\n        # Sumarizar\n        summary = summarizer(\n            text,\n            max_length=max_length,\n            min_length=min_length,\n            do_sample=False,\n            truncation=True\n        )\n\n        summary_text = summary[0][\'summary_text\']\n\n        # Calcular redução\n        compression_ratio = len(summary_text) / len(text)\n\n        return {\n            "success": True,\n            "task": "summarization",\n            "original_length": len(text),\n            "summary_length": len(summary_text),\n            "compression_ratio": round(compression_ratio, 2),\n            "summary": summary_text\n        }\n    except Exception as e:\n        return {"success": False, "error": str(e)}'}],outputFormat:{type:"object",required:["success","task"],properties:{success:{type:"boolean",description:"Indica se a inferência foi bem-sucedida"},task:{type:"string",description:"Tarefa executada (sentiment, ner, qa, etc)"},model:{type:"string",description:"Modelo usado"},results:{type:"any",description:"Resultados da inferência"},scores:{type:"array",description:"Scores de confiança"},error:{type:"string",description:"Mensagem de erro se success=false"}}}},tags:["transformers","huggingface","nlp","bert","gpt","sentiment","ner","classification","text-generation","question-answering"],keywords:["transformers","huggingface","nlp","natural language","linguagem natural","bert","gpt","sentiment","sentimento","classification","classificacao","ner","entities","entidades","question answering","qa","text generation","geracao de texto","translation","traducao","summarization","sumarizacao","embedding"],performance:{speed:5,memory:5,cpuIntensive:!0,gpuAccelerated:!0,scalability:6},scoring:{baseScore:.85,rules:[{condition:'keywords include ["sentiment", "sentimento", "analise"]',adjustment:.1,description:"Excelente para análise de sentimento"},{condition:'keywords include ["classification", "classificacao", "categorize"]',adjustment:.1,description:"Perfeito para classificação de texto"},{condition:'keywords include ["ner", "entities", "entidades", "extract"]',adjustment:.1,description:"Ideal para NER"},{condition:'keywords include ["question", "qa", "answer", "responder"]',adjustment:.1,description:"Ótimo para Q&A"},{condition:'keywords include ["generate", "gerar", "gpt", "completion"]',adjustment:.1,description:"Poderoso para geração de texto"},{condition:'keywords include ["translation", "traducao", "translate"]',adjustment:.08,description:"Bom para tradução"},{condition:'keywords include ["summarize", "sumarizar", "resumo", "summary"]',adjustment:.08,description:"Eficiente para sumarização"},{condition:'keywords include ["regex", "simple text", "basic"]',adjustment:-.4,description:"Texto simples não precisa de transformers"},{condition:'keywords include ["tabular", "numeric", "numbers only"]',adjustment:-.6,description:"Não é para dados tabulares"},{condition:'keywords include ["real-time", "milliseconds"] AND NOT include ["gpu"]',adjustment:-.3,description:"Modelos grandes são lentos sem GPU"}]},config:{maxRetries:2,timeout:6e4,cacheable:!0,requiresAuth:!1,rateLimit:null},alternatives:[{name:"spaCy",when:"NLP tradicional, pipeline completo, performance crítica",reason:"spaCy é mais rápido para NLP tradicional"},{name:"NLTK",when:"Processamento básico, tokenização, stemming",reason:"NLTK é mais leve para tarefas básicas"},{name:"OpenAI API",when:"GPT-4, GPT-3.5, modelos mais poderosos",reason:"OpenAI tem modelos mais avançados (mas pago)"},{name:"scikit-learn",when:"ML clássico, TF-IDF, CountVectorizer",reason:"sklearn é suficiente para ML tradicional"}],documentation:{official:"https://huggingface.co/docs/transformers/",examples:"https://huggingface.co/docs/transformers/task_summary",apiReference:"https://huggingface.co/docs/transformers/main_classes/pipelines"},commonIssues:[{issue:"Out of Memory (OOM)",solution:"Reduzir max_length, usar modelo menor, ou processar em batches menores",code:'# Reduzir max_length\nresult = classifier(text, max_length=256, truncation=True)\n\n# Ou usar modelo menor\nclassifier = pipeline("sentiment-analysis", model="distilbert-base-uncased")'},{issue:"Modelo lento (CPU)",solution:"Usar GPU com device=0 ou modelo distilled (menor)",code:'# Usar GPU\nclassifier = pipeline("sentiment-analysis", device=0)\n\n# Ou usar modelo distilled (mais rápido)\nclassifier = pipeline("sentiment-analysis", model="distilbert-base-uncased")'},{issue:"Token sequence too long",solution:"Adicionar truncation=True e limitar max_length",code:"result = model(text, truncation=True, max_length=512)"},{issue:"Download de modelo demora muito",solution:"Cache modelos ou baixar manualmente",code:'# Modelos são cachados automaticamente em ~/.cache/huggingface/\n# Para forçar cache:\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained("bert-base-uncased", cache_dir="./models")'}],bestPractices:["Use pipeline() para tarefas padrão (mais simples)","Especifique modelo adequado para idioma e tarefa","Sempre adicione truncation=True para textos longos","Configure max_length para evitar OOM","Use modelos distilled para prototipagem (mais rápidos)","Cache modelos localmente após primeiro download","Use GPU (device=0) quando disponível","Processe em batch para melhor performance"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},H={id:"playwright-012",name:"Playwright",packageName:"playwright",version:"1.40.0",category:w.BROWSER_AUTOMATION,subcategories:["web-scraping","automation","testing","screenshots","spa-scraping"],description:"Biblioteca moderna para automação de browsers (Chromium, Firefox, WebKit). Ideal para scraping de SPAs, testes e automação com auto-wait inteligente e API async.",purpose:"Automatizar browsers modernos, fazer scraping de sites dinâmicos e realizar testes end-to-end",useCases:["Web scraping de SPAs","Automação de formulários","Screenshots e PDFs","Testes automatizados","Monitoramento de sites"],complexity:T.INTERMEDIATE,environment:S.PYTHON,dependencies:["greenlet","pyee"],installCommand:"pip install playwright && python -m playwright install",promptSystem:{systemPrompt:"Especialista em Playwright. Use async/await, auto-wait inteligente, e evite time.sleep(). Sempre feche contexts e browsers.",userPromptTemplate:"Tarefa: {task_description}\nUse Playwright para automatizar browser.",examples:[{input:"Scraping básico",output:'from playwright.async_api import async_playwright\nasync def scrape():\n    async with async_playwright() as p:\n        browser = await p.chromium.launch()\n        page = await browser.new_page()\n        await page.goto("https://example.com")\n        title = await page.title()\n        await browser.close()\n        return title'}],outputFormat:{type:"object",properties:{success:{type:"boolean"},data:{type:"any"}}}},tags:["playwright","browser","automation","scraping"],keywords:["playwright","browser","headless","spa","screenshot"],performance:{speed:8,memory:6,cpuIntensive:!0,gpuAccelerated:!1,scalability:8},scoring:{baseScore:.88,rules:[{condition:'keywords include ["spa", "javascript", "dynamic"]',adjustment:.1,description:"Ideal para SPAs"},{condition:'keywords include ["screenshot", "pdf"]',adjustment:.08,description:"Capturas nativas"},{condition:'keywords include ["static", "simple"]',adjustment:-.4,description:"Use BeautifulSoup para sites estáticos"}]},config:{maxRetries:2,timeout:6e4,cacheable:!1,requiresAuth:!1,rateLimit:null},alternatives:[{name:"Selenium",when:"Compatibilidade legacy",reason:"Selenium mais antigo"}],documentation:{official:"https://playwright.dev/",examples:"https://playwright.dev/python/docs/intro",apiReference:"https://playwright.dev/python/docs/api/class-playwright"},commonIssues:[{issue:"Timeout",solution:"Aumentar timeout ou usar wait_for",code:'await page.wait_for_selector("h1", timeout=30000)'}],bestPractices:["Use auto-wait","Sempre feche browser","Use headless=True em produção","Configure user-agent"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},V={id:"pytorch-013",name:"PyTorch",packageName:"torch",version:"2.1.0",category:w.DEEP_LEARNING,subcategories:["neural-networks","computer-vision","nlp","training","inference"],description:"Framework de deep learning desenvolvido pelo Facebook. API Pythônica, computação dinâmica, forte em pesquisa e produção.",purpose:"Construir e treinar redes neurais profundas, fazer inferência com modelos DL",useCases:["Treinar redes neurais","Computer vision (CNNs)","NLP (RNNs, Transformers)","Transfer learning","Inferência de modelos"],complexity:T.ADVANCED,environment:S.PYTHON,dependencies:["numpy"],installCommand:"pip install torch torchvision torchaudio",promptSystem:{systemPrompt:"Especialista em PyTorch. Use torch.nn para modelos, DataLoader para dados, GPU quando disponível (.cuda()), sempre faça model.eval() para inferência.",userPromptTemplate:"Tarefa: {task_description}\nUse PyTorch para deep learning.",examples:[{input:"Criar rede neural simples",output:"import torch\nimport torch.nn as nn\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(10, 64)\n        self.fc2 = nn.Linear(64, 2)\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)"}],outputFormat:{type:"object",properties:{success:{type:"boolean"},model:{type:"any"},metrics:{type:"object"}}}},tags:["pytorch","deep-learning","neural-networks","ai","ml"],keywords:["pytorch","torch","deep learning","neural network","cnn","rnn","transformer","gpu"],performance:{speed:9,memory:6,cpuIntensive:!0,gpuAccelerated:!0,scalability:9},scoring:{baseScore:.85,rules:[{condition:'keywords include ["neural", "deep learning", "cnn", "rnn"]',adjustment:.1,description:"Perfeito para DL"},{condition:'keywords include ["gpu", "cuda"]',adjustment:.05,description:"Suporte nativo a GPU"},{condition:'keywords include ["simple ml", "linear regression"]',adjustment:-.3,description:"Use scikit-learn para ML clássico"}]},config:{maxRetries:1,timeout:3e5,cacheable:!0,requiresAuth:!1,rateLimit:null},alternatives:[{name:"TensorFlow",when:"Produção enterprise, TPU",reason:"TensorFlow mais maduro para produção"}],documentation:{official:"https://pytorch.org/docs/",examples:"https://pytorch.org/tutorials/",apiReference:"https://pytorch.org/docs/stable/nn.html"},commonIssues:[{issue:"CUDA out of memory",solution:"Reduzir batch size ou usar gradient checkpointing",code:"torch.cuda.empty_cache()"}],bestPractices:["Use DataLoader","model.eval() para inferência","with torch.no_grad() para economizar memória","Mova tensores para GPU: .cuda()"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},W={id:"httpx-014",name:"HTTPX",packageName:"httpx",version:"0.25.0",category:w.WEB_SCRAPING,subcategories:["http-client","async","api","rest"],description:"Cliente HTTP moderno com suporte a HTTP/2, async/await e API similar ao requests. Ideal para APIs REST assíncronas.",purpose:"Fazer requisições HTTP assíncronas com performance superior",useCases:["APIs REST async","HTTP/2","Requisições simultâneas","Streaming de dados"],complexity:T.INTERMEDIATE,environment:S.PYTHON,dependencies:["httpcore","certifi"],installCommand:"pip install httpx",promptSystem:{systemPrompt:"Especialista em HTTPX. Use async/await, timeout adequado, e close() em clients.",userPromptTemplate:"Use HTTPX para requisições HTTP assíncronas: {task_description}",examples:[{input:"GET async",output:'import httpx\nasync def fetch():\n    async with httpx.AsyncClient() as client:\n        response = await client.get("https://api.example.com")\n        return response.json()'}],outputFormat:{type:"object",properties:{success:{type:"boolean"},data:{type:"any"},status_code:{type:"number"}}}},tags:["httpx","http","async","api"],keywords:["httpx","http","async","http2","rest","api"],performance:{speed:9,memory:8,cpuIntensive:!1,gpuAccelerated:!1,scalability:9},scoring:{baseScore:.85,rules:[{condition:'keywords include ["async", "concurrent", "multiple"]',adjustment:.1,description:"Ideal para async"},{condition:'keywords include ["http2"]',adjustment:.05,description:"Suporte nativo HTTP/2"},{condition:'keywords include ["simple", "single request"]',adjustment:-.1,description:"requests é mais simples"}]},config:{maxRetries:3,timeout:3e4,cacheable:!1,requiresAuth:!1,rateLimit:null},alternatives:[{name:"requests",when:"Síncrono, simplicidade",reason:"requests mais simples para casos síncronos"}],documentation:{official:"https://www.python-httpx.org/",examples:"https://www.python-httpx.org/quickstart/",apiReference:"https://www.python-httpx.org/api/"},commonIssues:[{issue:"Connection pool limit",solution:"Configurar limits em AsyncClient",code:"limits = httpx.Limits(max_keepalive_connections=5)"}],bestPractices:["Use AsyncClient context manager","Configure timeout","Reutilize client para múltiplas requests","Use limits para pool"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},$={id:"scrapy-015",name:"Scrapy",packageName:"scrapy",version:"2.11.0",category:w.WEB_SCRAPING,subcategories:["crawling","scraping","spiders","pipelines"],description:"Framework completo para web scraping em larga escala. Sistema de spiders, pipelines, middleware e scheduling integrado.",purpose:"Fazer web scraping em larga escala com crawling automático",useCases:["Crawling de sites","Scraping em larga escala","Data pipelines","Monitoramento de preços"],complexity:T.ADVANCED,environment:S.PYTHON,dependencies:["twisted","lxml","parsel"],installCommand:"pip install scrapy",promptSystem:{systemPrompt:"Especialista em Scrapy. Crie spiders, use CSS/XPath selectors, configure pipelines e respeite robots.txt.",userPromptTemplate:"Crie spider Scrapy para: {task_description}",examples:[{input:"Spider básico",output:'import scrapy\nclass MySpider(scrapy.Spider):\n    name = "example"\n    start_urls = ["https://example.com"]\n    def parse(self, response):\n        for title in response.css("h1::text"):\n            yield {"title": title.get()}'}],outputFormat:{type:"object",properties:{success:{type:"boolean"},items_scraped:{type:"number"},data:{type:"array"}}}},tags:["scrapy","scraping","crawling","spider"],keywords:["scrapy","spider","crawl","scraping","large scale"],performance:{speed:9,memory:7,cpuIntensive:!1,gpuAccelerated:!1,scalability:10},scoring:{baseScore:.8,rules:[{condition:'keywords include ["large scale", "crawl", "multiple pages"]',adjustment:.15,description:"Ideal para larga escala"},{condition:'keywords include ["single page", "simple"]',adjustment:-.4,description:"Use BeautifulSoup para páginas simples"}]},config:{maxRetries:3,timeout:6e4,cacheable:!1,requiresAuth:!1,rateLimit:{maxRequests:10,windowMs:1e3}},alternatives:[{name:"BeautifulSoup",when:"Scraping simples",reason:"BS4 mais simples para casos pequenos"}],documentation:{official:"https://docs.scrapy.org/",examples:"https://docs.scrapy.org/en/latest/intro/tutorial.html",apiReference:"https://docs.scrapy.org/en/latest/topics/api.html"},commonIssues:[{issue:"Robots.txt blocked",solution:"Configure ROBOTSTXT_OBEY",code:"ROBOTSTXT_OBEY = False  # settings.py"}],bestPractices:["Respeite robots.txt","Configure user-agent","Use pipelines para processar dados","Implemente rate limiting"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},X={id:"moviepy-016",name:"MoviePy",packageName:"moviepy",version:"1.0.3",category:w.VIDEO_PROCESSING,subcategories:["video-editing","effects","compositing"],description:"Biblioteca para edição de vídeo: cortes, concatenação, efeitos, legendas.",purpose:"Editar vídeos programaticamente",useCases:["Cortar vídeos","Adicionar áudio","Aplicar efeitos","Concatenar clipes"],complexity:T.INTERMEDIATE,environment:S.PYTHON,dependencies:["numpy","imageio","decorator"],installCommand:"pip install moviepy",promptSystem:{systemPrompt:"Especialista em MoviePy. Use VideoFileClip, concatenate_videoclips, sempre feche clips com .close()",userPromptTemplate:"Edite vídeo: {task_description}",examples:[{input:"Cortar vídeo",output:'from moviepy.editor import VideoFileClip\nclip = VideoFileClip("input.mp4").subclip(10, 20)\nclip.write_videofile("output.mp4")'}],outputFormat:{type:"object",properties:{success:{type:"boolean"},output_path:{type:"string"}}}},tags:["moviepy","video","editing"],keywords:["moviepy","video","edit","cut","concatenate"],performance:{speed:6,memory:5,cpuIntensive:!0,gpuAccelerated:!1,scalability:6},scoring:{baseScore:.85,rules:[{condition:'keywords include ["video", "edit", "cut"]',adjustment:.1,description:"Ideal para edição"}]},config:{maxRetries:1,timeout:3e5,cacheable:!1,requiresAuth:!1,rateLimit:null},alternatives:[{name:"FFmpeg",when:"Performance crítica",reason:"FFmpeg mais rápido"}],documentation:{official:"https://zulko.github.io/moviepy/",examples:"https://zulko.github.io/moviepy/getting_started/quick_presentation.html",apiReference:"https://zulko.github.io/moviepy/ref/ref.html"},commonIssues:[{issue:"Memory error",solution:"Use resize() para reduzir resolução",code:"clip = clip.resize(0.5)"}],bestPractices:["Sempre feche clips","Use resize() para economizar memória","Configure codec adequado"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},Q={id:"pydub-017",name:"Pydub",packageName:"pydub",version:"0.25.1",category:w.AUDIO_PROCESSING,subcategories:["audio-editing","format-conversion","effects"],description:"Biblioteca simples para manipulação de áudio: cortar, concatenar, converter formatos, aplicar efeitos.",purpose:"Processar arquivos de áudio facilmente",useCases:["Converter MP3/WAV","Cortar áudio","Ajustar volume","Concatenar arquivos"],complexity:T.BASIC,environment:S.PYTHON,dependencies:["ffmpeg"],installCommand:"pip install pydub",promptSystem:{systemPrompt:"Especialista em Pydub. Use AudioSegment, export() para salvar, configure ffmpeg path se necessário",userPromptTemplate:"Processe áudio: {task_description}",examples:[{input:"Cortar áudio",output:'from pydub import AudioSegment\naudio = AudioSegment.from_mp3("input.mp3")\ncut = audio[10000:20000]\ncut.export("output.mp3", format="mp3")'}],outputFormat:{type:"object",properties:{success:{type:"boolean"},output_path:{type:"string"}}}},tags:["pydub","audio","mp3","wav"],keywords:["pydub","audio","mp3","wav","sound","music"],performance:{speed:7,memory:7,cpuIntensive:!1,gpuAccelerated:!1,scalability:7},scoring:{baseScore:.9,rules:[{condition:'keywords include ["audio", "mp3", "wav", "sound"]',adjustment:.08,description:"Perfeito para áudio"}]},config:{maxRetries:2,timeout:6e4,cacheable:!1,requiresAuth:!1,rateLimit:null},alternatives:[{name:"librosa",when:"Análise de áudio científica",reason:"librosa para análise avançada"}],documentation:{official:"https://github.com/jiaaro/pydub",examples:"https://github.com/jiaaro/pydub#quick-start",apiReference:"https://github.com/jiaaro/pydub/blob/master/API.markdown"},commonIssues:[{issue:"FFmpeg not found",solution:"Install ffmpeg",code:'AudioSegment.converter = "/path/to/ffmpeg"'}],bestPractices:["Instale FFmpeg","Use from_mp3/from_wav corretos","Configure bitrate ao exportar"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},Y={id:"reportlab-018",name:"ReportLab",packageName:"reportlab",version:"4.0.0",category:w.FILE_PROCESSING,subcategories:["pdf-generation","reports","documents"],description:"Biblioteca poderosa para geração de PDFs programaticamente: relatórios, faturas, certificados.",purpose:"Criar documentos PDF complexos",useCases:["Gerar relatórios PDF","Faturas","Certificados","Gráficos em PDF"],complexity:T.INTERMEDIATE,environment:S.PYTHON,dependencies:["pillow"],installCommand:"pip install reportlab",promptSystem:{systemPrompt:"Especialista em ReportLab. Use canvas ou platypus, configure pagesize, sempre salve com .save()",userPromptTemplate:"Gere PDF: {task_description}",examples:[{input:"PDF simples",output:'from reportlab.pdfgen import canvas\nc = canvas.Canvas("output.pdf")\nc.drawString(100, 750, "Hello PDF")\nc.save()'}],outputFormat:{type:"object",properties:{success:{type:"boolean"},pdf_path:{type:"string"}}}},tags:["reportlab","pdf","document"],keywords:["reportlab","pdf","document","report","invoice"],performance:{speed:8,memory:7,cpuIntensive:!1,gpuAccelerated:!1,scalability:8},scoring:{baseScore:.92,rules:[{condition:'keywords include ["pdf", "document", "report"]',adjustment:.08,description:"Ideal para PDFs"}]},config:{maxRetries:2,timeout:3e4,cacheable:!0,requiresAuth:!1,rateLimit:null},alternatives:[{name:"WeasyPrint",when:"HTML para PDF",reason:"WeasyPrint converte HTML"}],documentation:{official:"https://www.reportlab.com/docs/reportlab-userguide.pdf",examples:"https://www.reportlab.com/documentation/",apiReference:"https://www.reportlab.com/docs/reportlab-reference.pdf"},commonIssues:[{issue:"Font not found",solution:"Use fonts padrão ou registre font",code:"from reportlab.pdfbase import pdfmetrics"}],bestPractices:["Use platypus para layouts complexos","Configure pagesize corretamente","Otimize imagens antes de adicionar"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},J={id:"matplotlib-019",name:"Matplotlib",packageName:"matplotlib",version:"3.8.0",category:w.DATA_VISUALIZATION,subcategories:["plotting","charts","graphs"],description:"Biblioteca fundamental para visualização de dados em Python. Gráficos de linha, barras, scatter, histogramas e mais.",purpose:"Criar visualizações e gráficos de dados",useCases:["Gráficos de linha","Gráficos de barras","Histogramas","Scatter plots","Subplots"],complexity:T.INTERMEDIATE,environment:S.PYTHON,dependencies:["numpy","pillow"],installCommand:"pip install matplotlib",promptSystem:{systemPrompt:"Especialista em Matplotlib. Use plt.subplots(), configure labels/titles, plt.savefig() para salvar, plt.close() para liberar memória",userPromptTemplate:"Crie gráfico: {task_description}",examples:[{input:"Gráfico de linha",output:'import matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\nplt.plot(x, y)\nplt.xlabel("X")\nplt.ylabel("Y")\nplt.title("Seno")\nplt.savefig("plot.png")\nplt.close()'}],outputFormat:{type:"object",properties:{success:{type:"boolean"},image_path:{type:"string"}}}},tags:["matplotlib","plot","visualization","chart"],keywords:["matplotlib","plot","graph","chart","visualization","dados"],performance:{speed:7,memory:7,cpuIntensive:!1,gpuAccelerated:!1,scalability:7},scoring:{baseScore:.9,rules:[{condition:'keywords include ["plot", "graph", "chart", "visualization"]',adjustment:.08,description:"Ideal para visualização"}]},config:{maxRetries:2,timeout:3e4,cacheable:!0,requiresAuth:!1,rateLimit:null},alternatives:[{name:"Seaborn",when:"Gráficos estatísticos",reason:"Seaborn built on matplotlib"}],documentation:{official:"https://matplotlib.org/",examples:"https://matplotlib.org/stable/gallery/",apiReference:"https://matplotlib.org/stable/api/"},commonIssues:[{issue:"Figure not showing",solution:"Use plt.show() ou plt.savefig()",code:"plt.show()"}],bestPractices:["Use plt.subplots() ao invés de plt.subplot()","Configure figsize adequado","Sempre plt.close() após salvar","Use tight_layout()"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},K={id:"seaborn-020",name:"Seaborn",packageName:"seaborn",version:"0.13.0",category:w.DATA_VISUALIZATION,subcategories:["statistical-plots","heatmaps","distributions"],description:"Biblioteca de visualização estatística built on matplotlib. Gráficos elegantes e informativos com poucas linhas.",purpose:"Criar visualizações estatísticas bonitas",useCases:["Heatmaps","Distributions","Regression plots","Categorical plots","Pair plots"],complexity:T.BASIC,environment:S.PYTHON,dependencies:["matplotlib","pandas","numpy"],installCommand:"pip install seaborn",promptSystem:{systemPrompt:"Especialista em Seaborn. Use sns.set_theme(), trabalhe com DataFrames, configure palettes",userPromptTemplate:"Crie visualização estatística: {task_description}",examples:[{input:"Heatmap",output:'import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\ndata = np.random.rand(10, 12)\nsns.heatmap(data, annot=True)\nplt.savefig("heatmap.png")\nplt.close()'}],outputFormat:{type:"object",properties:{success:{type:"boolean"},image_path:{type:"string"}}}},tags:["seaborn","visualization","statistical"],keywords:["seaborn","sns","heatmap","distribution","statistical","plot"],performance:{speed:7,memory:7,cpuIntensive:!1,gpuAccelerated:!1,scalability:7},scoring:{baseScore:.88,rules:[{condition:'keywords include ["heatmap", "distribution", "statistical"]',adjustment:.1,description:"Ideal para gráficos estatísticos"}]},config:{maxRetries:2,timeout:3e4,cacheable:!0,requiresAuth:!1,rateLimit:null},alternatives:[{name:"Matplotlib",when:"Controle fino",reason:"Matplotlib mais flexível"}],documentation:{official:"https://seaborn.pydata.org/",examples:"https://seaborn.pydata.org/examples/",apiReference:"https://seaborn.pydata.org/api.html"},commonIssues:[{issue:"Style not applied",solution:"Use sns.set_theme()",code:'sns.set_theme(style="darkgrid")'}],bestPractices:["Use sns.set_theme() no início","Trabalhe com pandas DataFrames","Configure palette consistente"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}},Z={id:"redis-021",name:"Redis-py",packageName:"redis",version:"5.0.0",category:w.DATABASE,subcategories:["cache","key-value","nosql","queue"],description:"Cliente Python para Redis. Cache, sessions, queues, pub/sub, rate limiting com performance extrema.",purpose:"Trabalhar com Redis para cache e dados em memória",useCases:["Cache de dados","Session storage","Task queues","Rate limiting","Pub/Sub"],complexity:T.BASIC,environment:S.PYTHON,dependencies:[],installCommand:"pip install redis",promptSystem:{systemPrompt:"Especialista em Redis. Use connection pools, configure timeout, use pipeline para múltiplas ops, sempre trate ConnectionError",userPromptTemplate:"Use Redis para: {task_description}",examples:[{input:"Set/Get cache",output:'import redis\nr = redis.Redis(host="localhost", port=6379, decode_responses=True)\nr.set("key", "value", ex=3600)\nvalue = r.get("key")'}],outputFormat:{type:"object",properties:{success:{type:"boolean"},value:{type:"any"}}}},tags:["redis","cache","nosql"],keywords:["redis","cache","key-value","memory","queue","session"],performance:{speed:10,memory:9,cpuIntensive:!1,gpuAccelerated:!1,scalability:10},scoring:{baseScore:.92,rules:[{condition:'keywords include ["cache", "session", "queue"]',adjustment:.08,description:"Perfeito para cache"}]},config:{maxRetries:3,timeout:5e3,cacheable:!1,requiresAuth:!1,rateLimit:null},alternatives:[{name:"Memcached",when:"Cache simples",reason:"Memcached mais simples"}],documentation:{official:"https://redis-py.readthedocs.io/",examples:"https://redis-py.readthedocs.io/en/stable/examples.html",apiReference:"https://redis-py.readthedocs.io/en/stable/commands.html"},commonIssues:[{issue:"Connection refused",solution:"Verificar se Redis está rodando",code:"redis-cli ping"}],bestPractices:["Use connection pools","Configure expire em chaves","Use pipeline para batch","Trate ConnectionError"],stats:{timesUsed:0,successRate:0,averageExecutionTime:0,lastUsed:null,errors:[]}};class ee extends v.EventEmitter{core;promptRegistry;browserController;config;pythonServiceUrl;isInitialized=!1;constructor(e={}){super(),this.config={autoLoadModules:e.autoLoadModules??!0,debugMode:e.debugMode??!1,...e},this.core=function(e){return new E(e)}(e.core),this.promptRegistry=(C||(C=new P),C),this.browserController=function(e){return new k(e)}(e.browser),this.pythonServiceUrl=e.pythonService?.baseUrl||"http://localhost:8000",this.setupEventListeners(),this.config.autoLoadModules&&this.initialize().catch(e=>{this.log(`Erro na inicialização automática: ${e.message}`,"error")})}async initialize(){if(this.isInitialized)this.log("Sistema já inicializado");else{this.log("Inicializando AI System..."),this.emit("system:initializing");try{this.config.autoLoadModules&&await this.loadPromptModules(),this.registerModulesInCore(),await this.connectBrowser(),await this.connectPythonService(),this.isInitialized=!0,this.emit("system:initialized"),this.log("AI System inicializado com sucesso")}catch(e){throw this.emit("system:initialization-error",e),e}}}async loadPromptModules(){this.log("Carregando módulos de prompt...");const e=[U,q,O,D,M,L,F,z,B,j,G,H,V,W,$,X,Q,Y,J,K,Z];let t=0,a=0;for(const r of e)try{this.promptRegistry.register(r),this.log(`✓ Módulo carregado: ${r.name} (${r.packageName})`),t++}catch(s){this.log(`✗ Erro ao carregar ${r.name}: ${s.message}`,"warn"),a++}const n=this.promptRegistry.getStats();this.log(`Módulos carregados: ${t}/${e.length} (${a} falhas)`),this.log(`Total no registro: ${n.total} módulos ativos`)}registerModulesInCore(){this.log("Registrando módulos no Core IA...");const e=this.promptRegistry.export();e.forEach(e=>{this.core.registerModule({name:e.id,type:this.mapModuleCategoryToTaskType(e.category),capabilities:e.useCases,priority:this.calculateModulePriority(e),reliability:e.reliability,avgExecutionTime:e.avgExecutionTime,successRate:e.successRate,promptSystem:JSON.stringify(e.promptSystem)})}),this.log(`${e.length} módulos registrados no Core`)}async connectBrowser(){this.browserController.isConnected()||this.log("Aguardando conexão com extensão do navegador...")}async connectPythonService(){try{(await fetch(`${this.pythonServiceUrl}/health`)).ok&&this.log("Serviço Python conectado")}catch(e){this.log("Serviço Python não disponível (modo offline)","warn")}}async processRequest(e){this.isInitialized||await this.initialize();const t=Date.now(),a=e.id||this.generateRequestId();this.log(`Processando requisição: ${e.input}`),this.emit("request:processing",{requestId:a,request:e});try{const n={id:a,userId:e.userId,input:e.input,context:e.context,priority:e.priority,timestamp:Date.now()},s=await this.core.analyzeRequest(n);this.log(`Decisão: ${s.taskType} (confiança: ${s.confidence})`);const r=this.core.createExecutionPlan(n,s);let i;switch(this.log(`Plano criado: ${r.totalSteps} passos`),s.taskType){case b.BROWSER_AUTOMATION:i=await this.executeBrowserAutomation(n,r);break;case b.PYTHON_EXECUTION:i=await this.executePythonTask(n,r);break;case b.INTERNAL_TOOLS:i=await this.executeInternalTools(n,r);break;case b.HYBRID:i=await this.executeHybridTask(n,r);break;default:i=await this.core.execute(n)}const o=this.generateRecommendations(s,i),c={requestId:a,status:this.determineOverallStatus(i),decision:s,plan:r,results:i,executionTime:Date.now()-t,recommendations:o};return this.emit("request:completed",c),c}catch(n){this.log(`Erro ao processar requisição: ${n.message}`,"error");const e={requestId:a,status:x.FAILED,decision:{},plan:{},results:[],executionTime:Date.now()-t,error:n.message};return this.emit("request:error",e),e}}async executeBrowserAutomation(e,t){this.log("Executando automação de navegador...");const a=[];for(const s of t.steps)try{const t=this.stepToBrowserCommand(s,e),n=await this.browserController.executeCommand(t);a.push({stepId:s.id,status:this.mapBrowserStatusToExecutionStatus(n.status),output:n.output,error:n.error,executionTime:n.executionTime,retriesUsed:n.retriesUsed})}catch(n){a.push({stepId:s.id,status:x.FAILED,error:n.message,executionTime:0,retriesUsed:0})}return a}async executePythonTask(e,t){this.log("Executando tarefa Python...");const a=[];for(const s of t.steps)try{const e=this.promptRegistry.get(s.module);if(!e)throw new Error(`Módulo não encontrado: ${s.module}`);const t={module:e.packageName,function:s.action,parameters:s.parameters,promptSystem:e.promptSystem},n=await fetch(`${this.pythonServiceUrl}/execute`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify(t)});if(!n.ok)throw new Error(`Python service error: ${n.statusText}`);const r=await n.json();a.push({stepId:s.id,status:r.success?x.SUCCESS:x.FAILED,output:r.result,error:r.error,executionTime:r.executionTime||0,retriesUsed:0})}catch(n){a.push({stepId:s.id,status:x.FAILED,error:n.message,executionTime:0,retriesUsed:0})}return a}async executeInternalTools(e,t){return this.log("Executando ferramentas internas..."),await this.core.execute(e)}async executeHybridTask(e,t){this.log("Executando tarefa híbrida...");const a=[];for(const n of t.steps){let s;switch(n.taskType){case b.BROWSER_AUTOMATION:s=(await this.executeBrowserAutomation(e,{...t,steps:[n]}))[0];break;case b.PYTHON_EXECUTION:s=(await this.executePythonTask(e,{...t,steps:[n]}))[0];break;default:s=(await this.executeInternalTools(e,{...t,steps:[n]}))[0]}if(a.push(s),s.status===x.FAILED&&0===t.fallbackStrategies.length)break}return a}stepToBrowserCommand(e,t){return{id:e.id,action:A.EXECUTE_SCRIPT,value:e.parameters,timeout:e.timeout}}mapBrowserStatusToExecutionStatus(e){switch(e){case I.SUCCESS:return x.SUCCESS;case I.FAILED:case I.TIMEOUT:return x.FAILED;default:return x.PENDING}}mapModuleCategoryToTaskType(e){switch(e){case w.WEB_SCRAPING:case w.AUTOMATION:return b.BROWSER_AUTOMATION;case w.IMAGE_PROCESSING:case w.VIDEO_PROCESSING:case w.MACHINE_LEARNING:case w.DEEP_LEARNING:case w.NLP:return b.PYTHON_EXECUTION;default:return b.INTERNAL_TOOLS}}calculateModulePriority(e){let t=5;return e.reliability>.95&&(t+=2),e.successRate>.95&&(t+=2),"active"===e.status&&(t+=1),e.complexity===T.BASIC&&(t+=1),Math.min(t,10)}determineOverallStatus(e){if(0===e.length)return x.PENDING;if(e.every(e=>e.status===x.SUCCESS))return x.SUCCESS;return e.some(e=>e.status===x.SUCCESS)?x.SUCCESS:x.FAILED}generateRecommendations(e,t){const a=[],n=t.filter(e=>e.status===x.FAILED);n.length>0&&a.push(`${n.length} passos falharam. Considere usar fallback.`);return t.reduce((e,t)=>e+t.executionTime,0)/t.length>1e4&&a.push("Execução lenta detectada. Considere otimização."),e.confidence<.7&&a.push("Baixa confiança na decisão. Considere refinamento da requisição."),a}async navigateAndExtract(e,t){const a={};await this.browserController.navigate(e);for(const[n,s]of Object.entries(t)){const e=await this.browserController.extract(this.browserController.buildSelector(R.CSS,s));e.status===I.SUCCESS&&(a[n]=e.output)}return a}async fillFormAndSubmit(e){return await this.browserController.fillForm(e)}async scrapePage(e){return await this.browserController.scrape(e)}findModules(e){return this.promptRegistry.search(e)}getModuleByName(e){return this.promptRegistry.getByPackage(e)}getStats(){return{core:this.core.getStats(),promptLibrary:this.promptRegistry.getStats(),browser:{connected:this.browserController.isConnected(),connection:this.browserController.getConnection()},system:{initialized:this.isInitialized,uptime:process.uptime?process.uptime():0}}}reset(){this.core.reset(),this.log("Sistema resetado")}setupEventListeners(){this.core.on("execution:started",e=>this.emit("core:execution-started",e)),this.core.on("execution:completed",e=>this.emit("core:execution-completed",e)),this.core.on("execution:error",e=>this.emit("core:execution-error",e)),this.browserController.on("extension:connected",()=>this.emit("browser:connected")),this.browserController.on("command:success",e=>this.emit("browser:command-success",e)),this.browserController.on("command:failed",e=>this.emit("browser:command-failed",e))}generateRequestId(){return`req-${Date.now()}-${Math.random().toString(36).substr(2,9)}`}log(e,t="info"){this.config.debugMode,this.emit("log",{level:t,message:e,timestamp:Date.now()})}}class te{cache=new Map;baseUrl;profiles=new Map;constructor(e="/python-service/app/omnibrain/library_profiles"){this.baseUrl=e}async loadAll(){const e=this.getAvailableProfiles(),t=[];for(const n of e)try{const e=await this.load(n);e&&t.push(e)}catch(a){}return t}async load(e){if(this.cache.has(e))return this.cache.get(e);try{const t=await this.fetchProfile(e),a=this.convertToModule(t);return this.cache.set(e,a),a}catch(t){return null}}async fetchProfile(e){const t=`library_${e}.md`,a=`${this.baseUrl}/${t}`;try{const t=await fetch(a);if(!t.ok)throw new Error(`HTTP ${t.status}: ${t.statusText}`);const n=await t.text();return this.parseMarkdown(n,e)}catch(n){return this.loadHardcodedProfile(e)}}parseMarkdown(e,t){const a={package:t,useCases:[],bestPractices:[],commonPitfalls:[],performanceTips:[],whenToUse:[],whenNotToUse:[],alternatives:[],examples:[]},n=e.split("\n");let s="",r=null,i="",o=!1;for(let c=0;c<n.length;c++){const e=n[c].trim();if(e.startsWith("```"))o=!o,!o&&i&&r&&(r.code=i.trim(),i="");else if(o)i+=e+"\n";else if(e.startsWith("# "))a.name=e.replace("# ","").trim();else if(e.startsWith("## "))s=e.replace("## ","").toLowerCase().trim(),r&&"examples"!==s&&(a.examples.push(r),r=null);else if(e.startsWith("**Package:**"))a.package=e.split("**Package:**")[1].trim().replace(/`/g,"");else if(e.startsWith("**Version:**"))a.version=e.split("**Version:**")[1].trim().replace(/`/g,"");else if(e.startsWith("**Category:**"))a.category=e.split("**Category:**")[1].trim();else if(e.startsWith("**Installation:**"))a.installation=e.split("**Installation:**")[1].trim().replace(/`/g,"");else if(e.startsWith("- ")||e.startsWith("* ")){const t=e.substring(2).trim();s.includes("use case")||s.includes("capabilities")?a.useCases.push(t):s.includes("best practice")?a.bestPractices.push(t):s.includes("pitfall")||s.includes("avoid")?a.commonPitfalls.push(t):s.includes("performance")||s.includes("optimization")?a.performanceTips.push(t):s.includes("when to use")?a.whenToUse.push(t):s.includes("when not to use")||s.includes("limitations")?a.whenNotToUse.push(t):s.includes("alternative")&&a.alternatives.push(t)}else e.startsWith("### ")&&"examples"===s?(r&&a.examples.push(r),r={title:e.replace("### ","").trim(),code:"",description:""}):r&&e&&!e.startsWith("#")?r.description?r.description+=" "+e:r.description=e:a.description||!e||e.startsWith("#")||""!==s||(a.description=e)}return r&&a.examples.push(r),a}convertToModule(e){return{id:`lib-${e.package.replace(/[^a-z0-9]/gi,"-").toLowerCase()}`,name:e.name||e.package,packageName:e.package,version:e.version||"latest",category:this.mapCategory(e.category),subcategories:this.extractSubcategories(e),description:e.description||`${e.name} Python library`,purpose:e.description||"",useCases:e.useCases||[],complexity:this.estimateComplexity(e),environment:S.PYTHON,dependencies:[],installCommand:e.installation||`pip install ${e.package}`,promptSystem:{systemPrompt:this.generateSystemPrompt(e),instructions:this.generateInstructions(e),bestPractices:e.bestPractices||[],commonPitfalls:e.commonPitfalls||[],errorHandling:[],optimizationTips:e.performanceTips||[]},whenToUse:e.whenToUse.map((t,a)=>({condition:t,reasoning:`${e.name} é adequado para este caso`,confidence:.8,priority:a+1})),whenNotToUse:e.whenNotToUse.map((e,t)=>({condition:e,reasoning:"Outras alternativas podem ser mais adequadas",confidence:.7,priority:t+1})),mainFunctions:[],examples:e.examples.map((e,t)=>({title:e.title,description:e.description,input:{},code:e.code,output:{},explanation:e.description,useCase:e.title})),inputFormat:{type:"various",description:`Input formats for ${e.name}`,examples:[]},outputFormat:{type:"various",description:`Output formats from ${e.name}`,examples:[]},fallbackModules:[],alternativeModules:e.alternatives.map(e=>`lib-${e.replace(/[^a-z0-9]/gi,"-").toLowerCase()}`),avgExecutionTime:1e3,memoryUsage:"varies",cpuIntensive:this.isCPUIntensive(e),gpuSupport:this.hasGPUSupport(e),reliability:.9,successRate:.9,popularity:80,lastUpdated:Date.now(),status:"active",documentation:this.extractDocUrl(e),repository:`https://github.com/${e.package}`,license:"MIT",tags:this.extractTags(e)}}mapCategory(e){const t={"data processing":w.DATA_PROCESSING,"web scraping":w.WEB_SCRAPING,"image processing":w.IMAGE_PROCESSING,"video processing":w.VIDEO_PROCESSING,"audio processing":w.AUDIO_PROCESSING,"machine learning":w.MACHINE_LEARNING,"deep learning":w.DEEP_LEARNING,nlp:w.NLP,"computer vision":w.COMPUTER_VISION,visualization:w.DATA_VISUALIZATION,"web framework":w.WEB_FRAMEWORK,"http client":w.API_CLIENT,database:w.DATABASE,scientific:w.SCIENTIFIC_COMPUTING},a=e?.toLowerCase()||"";for(const[n,s]of Object.entries(t))if(a.includes(n))return s;return w.OTHER}extractSubcategories(e){const t=[],a=`${e.description} ${e.useCases.join(" ")}`.toLowerCase();return(a.includes("dataframe")||a.includes("csv"))&&t.push("data-manipulation"),(a.includes("api")||a.includes("http"))&&t.push("api-client"),(a.includes("scraping")||a.includes("html"))&&t.push("web-scraping"),(a.includes("image")||a.includes("photo"))&&t.push("image-processing"),(a.includes("video")||a.includes("movie"))&&t.push("video-processing"),(a.includes("audio")||a.includes("sound"))&&t.push("audio-processing"),(a.includes("ml")||a.includes("machine learning"))&&t.push("machine-learning"),(a.includes("neural")||a.includes("deep learning"))&&t.push("deep-learning"),t.length>0?t:["general"]}estimateComplexity(e){const t=`${e.description} ${e.useCases.join(" ")}`.toLowerCase();return t.includes("advanced")||t.includes("expert")||t.includes("deep learning")?T.EXPERT:t.includes("complex")||t.includes("machine learning")?T.ADVANCED:t.includes("basic")||t.includes("simple")?T.BASIC:T.INTERMEDIATE}generateSystemPrompt(e){return`You are an expert in ${e.name}, the ${e.category} library.\n${e.description}\n\nWhen working with ${e.name}, you should:\n- Follow best practices\n- Write efficient and maintainable code\n- Handle errors appropriately\n- Document your code clearly\n- Consider performance implications`}generateInstructions(e){return[`Import ${e.name} correctly: ${e.installation}`,"Always handle exceptions appropriately","Validate inputs before processing","Document complex operations","Use type hints when possible"]}isCPUIntensive(e){const t=`${e.description} ${e.category}`.toLowerCase();return t.includes("compute")||t.includes("processing")||t.includes("machine learning")||t.includes("video")}hasGPUSupport(e){const t=`${e.name} ${e.description}`.toLowerCase();return t.includes("gpu")||t.includes("cuda")||t.includes("tensorflow")||t.includes("torch")}extractDocUrl(e){return`https://${e.package}.readthedocs.io/`||`https://pypi.org/project/${e.package}/`}extractTags(e){const t=[e.category.toLowerCase()];return e.useCases.forEach(e=>{e.toLowerCase().split(" ").forEach(e=>{e.length>4&&!t.includes(e)&&t.push(e)})}),t.slice(0,10)}loadHardcodedProfile(e){return{pandas:{name:"Pandas",package:"pandas",version:"2.2.0",category:"Data Processing",description:"Powerful data analysis and manipulation library",useCases:["CSV processing","Data analysis","Data cleaning"],installation:"pip install pandas",bestPractices:["Use vectorized operations","Avoid loops"],commonPitfalls:["SettingWithCopyWarning"],whenToUse:["Working with tabular data"],whenNotToUse:["Data larger than memory"],alternatives:["polars","dask"],examples:[]}}[e]||this.getGenericProfile(e)}getGenericProfile(e){return{name:e,package:e,version:"latest",category:"General",description:`${e} Python library`,useCases:[`Use ${e} for various tasks`],installation:`pip install ${e}`,basicUsage:"",bestPractices:[],commonPitfalls:[],performanceTips:[],whenToUse:[],whenNotToUse:[],alternatives:[],examples:[]}}getAvailableProfiles(){return["pandas","numpy","pillow","opencv-python","scikit-learn","tensorflow","torch","transformers","requests","httpx","beautifulsoup4","scrapy","selenium","playwright","sqlalchemy","fastapi","moviepy","pydub","reportlab"]}clearCache(){this.cache.clear()}async reload(e){return this.cache.delete(e),this.load(e)}}let ae=null;function ne(e={}){const{autoInit:a=!0,debugMode:n=!1,pythonServiceUrl:s="https://syncads-python-microservice-production.up.railway.app"}=e,[r,i]=t.useState({initialized:!1,loading:!1,error:null,stats:null,availableModules:[]}),o=t.useRef(null),c=t.useRef(null),l=t.useCallback(async()=>{if(!o.current){i(e=>({...e,loading:!0,error:null}));try{const e=new ee({core:{maxRetries:3,fallbackEnabled:!0,learningEnabled:!0,debugMode:n},browser:{defaultTimeout:3e4,maxRetries:3,debugMode:n},pythonService:{baseUrl:s,timeout:6e4},autoLoadModules:!1,debugMode:n});e.on("system:initialized",()=>{}),e.on("request:processing",e=>{}),e.on("request:completed",e=>{}),e.on("request:error",e=>{}),c.current=(ae||(ae=new te),ae);const t=await c.current.loadAll();o.current=e;const a=e.getStats();i({initialized:!0,loading:!1,error:null,stats:a,availableModules:t})}catch(e){i(t=>({...t,loading:!1,error:e.message||"Erro ao inicializar AI System",initialized:!1}))}}},[n,s]),d=t.useCallback(async(e,t,a)=>{if(!o.current)throw new Error("AI System não inicializado. Chame initialize() primeiro.");i(e=>({...e,loading:!0,error:null}));try{const n={userId:e,input:t,context:a,priority:1,executionMode:"auto"},s=await o.current.processRequest(n),r=o.current.getStats();return i(e=>({...e,stats:r,loading:!1})),s}catch(n){return i(e=>({...e,loading:!1,error:n.message||"Erro ao processar requisição"})),null}},[]),p=t.useCallback(e=>o.current?o.current.findModules(e):[],[]),u=t.useCallback(e=>{if(o.current)return o.current.getModuleByName(e)},[]),m=t.useCallback(async(e,t)=>{if(!o.current)throw new Error("AI System não inicializado");i(e=>({...e,loading:!0}));try{const a=await o.current.navigateAndExtract(e,t);return i(e=>({...e,loading:!1})),a}catch(a){throw i(e=>({...e,loading:!1,error:a.message})),a}},[]),g=t.useCallback(async e=>{if(!o.current)throw new Error("AI System não inicializado");i(e=>({...e,loading:!0}));try{const t=await o.current.scrapePage(e);return i(e=>({...e,loading:!1})),t}catch(t){throw i(e=>({...e,loading:!1,error:t.message})),t}},[]),f=t.useCallback(()=>{if(o.current){o.current.reset();const e=o.current.getStats();i(t=>({...t,stats:e}))}},[]),h=t.useCallback(()=>{if(o.current){const e=o.current.getStats();i(t=>({...t,stats:e}))}},[]),y=t.useCallback(async()=>{if(c.current){i(e=>({...e,loading:!0}));try{c.current.clearCache();const e=await c.current.loadAll();i(t=>({...t,availableModules:e,loading:!1}))}catch(e){i(t=>({...t,loading:!1,error:e.message}))}}},[]),_=t.useCallback(()=>o.current?.browserController,[]),v=t.useCallback(()=>{const e=_();return e?.isConnected()||!1},[_]);return t.useEffect(()=>{!a||r.initialized||r.loading||l()},[a,r.initialized,r.loading,l]),t.useEffect(()=>()=>{o.current&&o.current.removeAllListeners()},[]),{initialized:r.initialized,loading:r.loading,error:r.error,stats:r.stats,availableModules:r.availableModules,initialize:l,processRequest:d,searchModules:p,getModule:u,navigateAndExtract:m,scrapePage:g,isBrowserConnected:v,getBrowserController:_,reset:f,updateStats:h,reloadModules:y,aiSystem:o.current}}export{a as I,ne as u};
